<html>
      <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <title id="title">
          com/sparkutils/quality/impl/aggregates/ExpressionAggregates.scala.html
        </title>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/css/theme.default.min.css" type="text/css"/><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/js/jquery.tablesorter.min.js"></script><link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css" type="text/css"/><script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script><script type="text/javascript">
        $(document).ready(function() {$(".tablesorter").tablesorter();});
      </script>
        <style>
          table.codegrid { font-family: monospace; font-size: 12px; width: auto!important; }table.statementlist { width: auto!important; font-size: 13px; } table.codegrid td { padding: 0!important; border: 0!important } table td.linenumber { width: 40px!important; } 
        </style>
      </head>
      <body style="font-family: monospace;">
        <ul class="nav nav-tabs">
          <li>
            <a href="#codegrid" data-toggle="tab">Codegrid</a>
          </li>
          <li>
            <a href="#statementlist" data-toggle="tab">Statement List</a>
          </li>
        </ul>
        <div class="tab-content">
          <div class="tab-pane active" id="codegrid">
            <pre style='font-size: 12pt; font-family: courier, monospace;'>1 <span style=''>package com.sparkutils.quality.impl.aggregates
</span>2 <span style=''>
</span>3 <span style=''>import com.sparkutils.quality.QualityException.qualityException
</span>4 <span style=''>import eu.timepit.refined.boolean.False
</span>5 <span style=''>import org.apache.spark.sql.QualitySparkUtils
</span>6 <span style=''>import org.apache.spark.sql.ShimUtils.cast
</span>7 <span style=''>import org.apache.spark.sql.catalyst.dsl.expressions._
</span>8 <span style=''>import org.apache.spark.sql.catalyst.expressions.aggregate.DeclarativeAggregate
</span>9 <span style=''>import org.apache.spark.sql.catalyst.expressions.{AttributeReference, Cast, Expression, If, LambdaFunction, Literal, NamedLambdaVariable}
</span>10 <span style=''>import org.apache.spark.sql.qualityFunctions._
</span>11 <span style=''>import org.apache.spark.sql.types.{DataType, DecimalType, MapType}
</span>12 <span style=''>import org.apache.spark.sql.ShimUtils.{cast =&gt; castf}
</span>13 <span style=''>
</span>14 <span style=''>object AggregateExpressions {
</span>15 <span style=''>
</span>16 <span style=''>  def transformSumType(function: Expression, param: NamedLambdaVariable, newDT: DataType) =
</span>17 <span style=''>    </span><span style='background: #AEF1AE'>function.transform {
</span>18 <span style=''></span><span style='background: #AEF1AE'>      case n: NamedLambdaVariable if n.exprId == param.exprId =&gt;
</span>19 <span style=''></span><span style='background: #AEF1AE'>        n.copy(dataType = newDT)
</span>20 <span style=''></span><span style='background: #AEF1AE'>      // spark for decimals already gets the type wrong for 0.7.1 syntax type and double wraps it (only checking single breaks deprecated syntax), so
</span>21 <span style=''></span><span style='background: #AEF1AE'>      // ugly workarounds for types not matching.. see above comment
</span>22 <span style=''></span><span style='background: #AEF1AE'>      case cast: Cast if cast.child.isInstanceOf[Cast] &amp;&amp; cast.child.asInstanceOf[Cast].child.isInstanceOf[NamedLambdaVariable] =&gt;
</span>23 <span style=''></span><span style='background: #AEF1AE'>        val nvl = cast.child.asInstanceOf[Cast].child.asInstanceOf[NamedLambdaVariable]
</span>24 <span style=''></span><span style='background: #AEF1AE'>        if (nvl.exprId == param.exprId)
</span>25 <span style=''></span><span style='background: #AEF1AE'>          </span><span style='background: #F0ADAD'>castf(castf(nvl.copy(dataType = newDT), newDT), cast.dataType)</span><span style='background: #AEF1AE'>
</span>26 <span style=''></span><span style='background: #AEF1AE'>        else
</span>27 <span style=''></span><span style='background: #AEF1AE'>          cast
</span>28 <span style=''></span><span style='background: #AEF1AE'>      // for dbr &gt; 11.2, the cast is on the variable not the expression
</span>29 <span style=''></span><span style='background: #AEF1AE'>      case cast: Cast if cast.child.isInstanceOf[NamedLambdaVariable] =&gt;
</span>30 <span style=''></span><span style='background: #AEF1AE'>        val nvl = cast.child.asInstanceOf[NamedLambdaVariable]
</span>31 <span style=''></span><span style='background: #AEF1AE'>        if (nvl.exprId == param.exprId)
</span>32 <span style=''></span><span style='background: #AEF1AE'>          castf( child = cast.child.asInstanceOf[NamedLambdaVariable].
</span>33 <span style=''></span><span style='background: #AEF1AE'>            copy(dataType = newDT), newDT)
</span>34 <span style=''></span><span style='background: #AEF1AE'>        else
</span>35 <span style=''></span><span style='background: #AEF1AE'>          cast
</span>36 <span style=''></span><span style='background: #AEF1AE'>
</span>37 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>38 <span style=''>
</span>39 <span style=''>  /**
</span>40 <span style=''>   * Should only be created from within the RuleRunnerObject functions, not doing so will lead to unresolved functions and lambda variables - use sql strings only to construct.
</span>41 <span style=''>   *
</span>42 <span style=''>   * @param sumType when non-null it triggers a rewrite of both sum and evaluate expressions to set their parameter types to sumType, otherwise it takes those specified in the expressions already
</span>43 <span style=''>   * @param ifExpr count if true/1 etc.
</span>44 <span style=''>   * @param sum called lambda expression when ifExpr is true, with the current sum value and returns the new sum value (e.g. current -&gt; current + col2 would total all col2's)
</span>45 <span style=''>   * @param evaluate combines the count and overall result of combinations in lambda to provide the overall results (e.g. (current, count) -&gt; current / count)
</span>46 <span style=''>   * @param zero     lookup function to get the zero value for the sum type
</span>47 <span style=''>   * @param notYetResolved additional casts are needed for the dsl due to decimal precision handling.
</span>48 <span style=''>   *                 adding them in at the dsl level, however, prevents resolving the lambdas, using Column cast doesn't work either, so they are added in the expression itself
</span>49 <span style=''>   *                 As the lambda's won't be resolved when calling this the existing matching for the expr sql variant cannot work.
</span>50 <span style=''>   */
</span>51 <span style=''>  def apply(sumType: DataType, ifExpr: Expression, sum: Expression, evaluate: Expression, zero: DataType =&gt; Option[Any], add: DataType =&gt; Option[( Expression, Expression ) =&gt; Expression], notYetResolved: Boolean = false ): Expression = {
</span>52 <span style=''>    /*
</span>53 <span style=''>     * in the case of decimal's being used the DecimalPrecision analysis can change the types such that the
</span>54 <span style=''>     * precision is ignored e.g.
</span>55 <span style=''>     * sumWith('DECIMAL(38,18)', entry -&gt; entry + field)
</span>56 <span style=''>     * may end up with 38,17 for the lambda despite 38,17 being the input as such this must be casted.
</span>57 <span style=''>     */
</span>58 <span style=''>
</span>59 <span style=''>    lazy val correctedSum: Expression =
</span>60 <span style=''>      if (notYetResolved)
</span>61 <span style=''>        sum
</span>62 <span style=''>      else
</span>63 <span style=''>        correctSum(sumType, sum)
</span>64 <span style=''>
</span>65 <span style=''>    lazy val correctedEvaluate: Expression =
</span>66 <span style=''>      if (notYetResolved)
</span>67 <span style=''>        evaluate
</span>68 <span style=''>      else
</span>69 <span style=''>        correctEvaluate(sumType, evaluate)
</span>70 <span style=''>
</span>71 <span style=''>    val (SeqArgs(sum1 +: _, _), SeqArgs(Seq(sum2, count), _), useSum, useEvaluate) =
</span>72 <span style=''>      if (sumType eq null)
</span>73 <span style=''>        (sum, evaluate, sum, evaluate)
</span>74 <span style=''>      else {
</span>75 <span style=''>        val temp = (correctedSum, correctedEvaluate, correctedSum, correctedEvaluate)
</span>76 <span style=''>        temp
</span>77 <span style=''>      }
</span>78 <span style=''>
</span>79 <span style=''>    </span><span style='background: #AEF1AE'>ExpressionAggregates(Seq(count, sum1, sum2, ifExpr, useSum, useEvaluate), zero, add, sumType).toAggregateExpression()</span><span style=''>
</span>80 <span style=''>  }
</span>81 <span style=''>
</span>82 <span style=''>  def correctEvaluate(sumType: DataType, evaluate: Expression, wrapCastOnly: Boolean = false) =
</span>83 <span style=''>    evaluate match {
</span>84 <span style=''>      case FunN(Seq(RefExpression(_, nullable, _), cref),
</span>85 <span style=''>      LambdaFunction(function, Seq(sparam: NamedLambdaVariable, cparam: NamedLambdaVariable), hidden),
</span>86 <span style=''>      name, _, _, _) =&gt;
</span>87 <span style=''>
</span>88 <span style=''>        val correctedFunction =
</span>89 <span style=''>          if (wrapCastOnly)
</span>90 <span style=''>            </span><span style='background: #AEF1AE'>function</span><span style=''>
</span>91 <span style=''>          else
</span>92 <span style=''>            </span><span style='background: #AEF1AE'>transformSumType(function, sparam, sumType)</span><span style=''>
</span>93 <span style=''>
</span>94 <span style=''>        </span><span style='background: #AEF1AE'>FunN(Seq(RefExpression(sumType, nullable), cref),
</span>95 <span style=''></span><span style='background: #AEF1AE'>          LambdaFunction(correctedFunction,
</span>96 <span style=''></span><span style='background: #AEF1AE'>            Seq(sparam.copy(dataType = sumType), cparam), hidden), name)</span><span style=''>
</span>97 <span style=''>
</span>98 <span style=''>      // when we partially apply the lambda is further down
</span>99 <span style=''>      case FunForward(Seq(RefExpression(_, nullable, paramIndex), cref) :+ (
</span>100 <span style=''>        i@FunN(funparams, LambdaFunction(function, params, hidden), _, _, _, _)
</span>101 <span style=''>        )) =&gt;
</span>102 <span style=''>        // params can be longer and not 1:1 due to placeholders
</span>103 <span style=''>        val sparam = </span><span style='background: #AEF1AE'>params(paramIndex).asInstanceOf[NamedLambdaVariable]</span><span style=''>
</span>104 <span style=''>        val correctedFunction =
</span>105 <span style=''>          if (wrapCastOnly)
</span>106 <span style=''>            </span><span style='background: #AEF1AE'>function</span><span style=''>
</span>107 <span style=''>          else
</span>108 <span style=''>            </span><span style='background: #AEF1AE'>transformSumType(function, sparam, sumType)</span><span style=''>
</span>109 <span style=''>
</span>110 <span style=''>        </span><span style='background: #AEF1AE'>FunForward(Seq(RefExpression(sumType, nullable, paramIndex), cref) :+
</span>111 <span style=''></span><span style='background: #AEF1AE'>          i.copy(function = LambdaFunction(correctedFunction,
</span>112 <span style=''></span><span style='background: #AEF1AE'>            sparam.copy(dataType = sumType) +: params.drop(1), hidden),
</span>113 <span style=''></span><span style='background: #AEF1AE'>            arguments = funparams.updated(paramIndex,
</span>114 <span style=''></span><span style='background: #AEF1AE'>              funparams(paramIndex).asInstanceOf[RefExpression].copy(dataType = sumType)
</span>115 <span style=''></span><span style='background: #AEF1AE'>            )))</span><span style=''>
</span>116 <span style=''>
</span>117 <span style=''>      case _ =&gt; evaluate // shouldn't happen but better from spark than a match error
</span>118 <span style=''>    }
</span>119 <span style=''>
</span>120 <span style=''>  def correctSum(sumType: DataType, sum: Expression, wrapCastOnly: Boolean = false) =
</span>121 <span style=''>    sum match {
</span>122 <span style=''>      // re-pack with new type
</span>123 <span style=''>
</span>124 <span style=''>      // for sumWith
</span>125 <span style=''>      case FunN(Seq(RefExpression(_, nullable, _)),
</span>126 <span style=''>      LambdaFunction(fun, Seq(param: NamedLambdaVariable), hidden),
</span>127 <span style=''>      name, _, _, _) =&gt;
</span>128 <span style=''>
</span>129 <span style=''>        val correctedFunction =
</span>130 <span style=''>          if (wrapCastOnly)
</span>131 <span style=''>            </span><span style='background: #AEF1AE'>fun</span><span style=''>
</span>132 <span style=''>          else
</span>133 <span style=''>            </span><span style='background: #AEF1AE'>transformSumType(fun, param, sumType)</span><span style=''>
</span>134 <span style=''>
</span>135 <span style=''>        </span><span style='background: #AEF1AE'>FunN(Seq(RefExpression(sumType, nullable)),
</span>136 <span style=''></span><span style='background: #AEF1AE'>          LambdaFunction(cast(correctedFunction, sumType), // see above comment for cast justification
</span>137 <span style=''></span><span style='background: #AEF1AE'>            Seq(param.copy(dataType = sumType)), hidden), name)</span><span style=''>
</span>138 <span style=''>
</span>139 <span style=''>      // for mapWith
</span>140 <span style=''>      case MapTransform(r: RefExpression, key, LambdaFunction(function, Seq(param: NamedLambdaVariable), hidden), zeroF) =&gt;
</span>141 <span style=''>
</span>142 <span style=''>        val MapType(_, valueType, _) = sumType
</span>143 <span style=''>
</span>144 <span style=''>        val correctedFunction =
</span>145 <span style=''>          if (wrapCastOnly)
</span>146 <span style=''>            </span><span style='background: #AEF1AE'>function</span><span style=''>
</span>147 <span style=''>          else
</span>148 <span style=''>            </span><span style='background: #AEF1AE'>transformSumType(function, param, valueType)</span><span style=''>
</span>149 <span style=''>
</span>150 <span style=''>        </span><span style='background: #AEF1AE'>MapTransform(RefExpression(sumType, r.nullable), key,
</span>151 <span style=''></span><span style='background: #AEF1AE'>          LambdaFunction(cast(correctedFunction, valueType), // see above comment for cast justification
</span>152 <span style=''></span><span style='background: #AEF1AE'>            Seq(param.copy(dataType = valueType)), hidden), zeroF)</span><span style=''>
</span>153 <span style=''>
</span>154 <span style=''>      // when we partially apply the lambda is further down
</span>155 <span style=''>      case FunForward(Seq(RefExpression(_, nullable, paramIndex)) :+ (
</span>156 <span style=''>        i@FunN(funparams, LambdaFunction(function, params, hidden), _, _, _, _)
</span>157 <span style=''>        )) =&gt;
</span>158 <span style=''>        // params can be longer and not 1:1 due to placeholders
</span>159 <span style=''>        val sparam = </span><span style='background: #AEF1AE'>params(paramIndex).asInstanceOf[NamedLambdaVariable]</span><span style=''>
</span>160 <span style=''>        val correctedFunction =
</span>161 <span style=''>          if (wrapCastOnly)
</span>162 <span style=''>            </span><span style='background: #AEF1AE'>function</span><span style=''>
</span>163 <span style=''>          else
</span>164 <span style=''>            </span><span style='background: #AEF1AE'>transformSumType(function, sparam, sumType)</span><span style=''>
</span>165 <span style=''>
</span>166 <span style=''>        </span><span style='background: #AEF1AE'>FunForward(Seq(RefExpression(sumType, nullable, paramIndex)) :+
</span>167 <span style=''></span><span style='background: #AEF1AE'>          i.copy(function = LambdaFunction(cast(correctedFunction, sumType), // see above comment for cast justification
</span>168 <span style=''></span><span style='background: #AEF1AE'>            sparam.copy(dataType = sumType) +: params.drop(1), hidden),
</span>169 <span style=''></span><span style='background: #AEF1AE'>            arguments = funparams.updated(paramIndex,
</span>170 <span style=''></span><span style='background: #AEF1AE'>              funparams(paramIndex).asInstanceOf[RefExpression].copy(dataType = sumType)
</span>171 <span style=''></span><span style='background: #AEF1AE'>            )))</span><span style=''>
</span>172 <span style=''>
</span>173 <span style=''>      case _ =&gt; sum // not expected but better errors will come form Spark
</span>174 <span style=''>    }
</span>175 <span style=''>}
</span>176 <span style=''>
</span>177 <span style=''>/**
</span>178 <span style=''> * Represents an aggregation expression built from a filter function, a sum lambda and an evaluate lambda which uses the
</span>179 <span style=''> * count of filter hits and the sum as parameters.
</span>180 <span style=''> * @param children
</span>181 <span style=''> */
</span>182 <span style=''>case class ExpressionAggregates(override val children: Seq[Expression], zero: DataType =&gt; Option[Any], addF: DataType =&gt; Option[( Expression, Expression ) =&gt; Expression], sumType: DataType) extends DeclarativeAggregate {
</span>183 <span style=''>  // extending higherorder fun is needed otherwise case other =&gt; other.failAnalysis( is thrown in analysis/higherOrderFunctions
</span>184 <span style=''>  lazy val Seq(countLeaf: RefExpression, sumLeaf: RefExpression, evalSumLeaf: RefExpression, ifExpr, sumWith, evaluate) =
</span>185 <span style=''>    if (children.forall(_.resolved))
</span>186 <span style=''>      rewriteChildren(children)
</span>187 <span style=''>    else
</span>188 <span style=''>      children
</span>189 <span style=''>
</span>190 <span style=''>  private def rewriteChildren(children: Seq[Expression]) = {
</span>191 <span style=''>    import AggregateExpressions.{correctEvaluate, correctSum}
</span>192 <span style=''>    val Seq(_, _, _, ifExpr, sumWith, evaluate) = children
</span>193 <span style=''>
</span>194 <span style=''>    val correctedEvaluate = </span><span style='background: #AEF1AE'>correctEvaluate(sumType, evaluate, wrapCastOnly = true)</span><span style=''>
</span>195 <span style=''>    val correctedSum = </span><span style='background: #AEF1AE'>correctSum(sumType, sumWith, wrapCastOnly = true)</span><span style=''>
</span>196 <span style=''>
</span>197 <span style=''>    val (SeqArgs(sum1 +: _, _), SeqArgs(Seq(sum2, count), _), useSum, useEvaluate) =
</span>198 <span style=''>      if (sumType eq null)
</span>199 <span style=''>        (sumWith, evaluate, sumWith, evaluate)
</span>200 <span style=''>      else {
</span>201 <span style=''>        val temp = (correctedSum, correctedEvaluate, correctedSum, correctedEvaluate)
</span>202 <span style=''>        temp
</span>203 <span style=''>      }
</span>204 <span style=''>
</span>205 <span style=''>    </span><span style='background: #AEF1AE'>Seq(count, sum1, sum2, ifExpr, useSum, useEvaluate)</span><span style=''>
</span>206 <span style=''>  }
</span>207 <span style=''>
</span>208 <span style=''>  lazy val sumRef = AttributeReference(&quot;sum&quot;, sumLeaf.dataType, true)()
</span>209 <span style=''>  lazy val countRef = AttributeReference(&quot;count&quot;, countLeaf.dataType)()
</span>210 <span style=''>
</span>211 <span style=''>  lazy val sum = RefSetterExpression(Seq(sumLeaf, sumRef))
</span>212 <span style=''>  lazy val sumEval = RefSetterExpression(Seq(evalSumLeaf, sumRef))
</span>213 <span style=''>  lazy val count = RefSetterExpression(Seq(countLeaf, countRef))
</span>214 <span style=''>
</span>215 <span style=''>  lazy val runEvaluate = RunAllReturnLast(Seq(sumEval, count, evaluate))
</span>216 <span style=''>  lazy val sumWithEvaluate = RunAllReturnLast(Seq(sum, sumWith))
</span>217 <span style=''>
</span>218 <span style=''>  lazy val add = addF(sumWith.dataType).getOrElse(qualityException(s&quot;Cannot find the monoidal add for type ${sumWith.dataType}&quot;))
</span>219 <span style=''>
</span>220 <span style=''>  override lazy val initialValues: Seq[Expression] = Seq(
</span>221 <span style=''>    Literal(0L), // count
</span>222 <span style=''>    {
</span>223 <span style=''>      val dt = sumLeaf.dataType
</span>224 <span style=''>      val zeroE = zero(dt).getOrElse(qualityException(s&quot;Could not find zero for type ${dt}&quot;))
</span>225 <span style=''>      Literal(zeroE, dt)
</span>226 <span style=''>    }
</span>227 <span style=''>  )
</span>228 <span style=''>
</span>229 <span style=''>  override lazy val updateExpressions: Seq[Expression] = Seq(
</span>230 <span style=''>    If(ifExpr, count + 1L, count),
</span>231 <span style=''>    If(ifExpr, sumWithEvaluate, sum)
</span>232 <span style=''>  )
</span>233 <span style=''>
</span>234 <span style=''>  override lazy val mergeExpressions: Seq[Expression] = Seq(
</span>235 <span style=''>    count.left + count.right,
</span>236 <span style=''>    sumLeaf.dataType match {
</span>237 <span style=''>      case _: DecimalType =&gt; cast(add(sum.left, sum.right), sumLeaf.dataType) // extra protection against SPARK-39316
</span>238 <span style=''>      case _ =&gt; add(sum.left, sum.right)
</span>239 <span style=''>    }
</span>240 <span style=''>  )
</span>241 <span style=''>
</span>242 <span style=''>  override lazy val evaluateExpression: Expression = runEvaluate
</span>243 <span style=''>
</span>244 <span style=''>  override lazy val aggBufferAttributes: Seq[AttributeReference] = Seq(countRef, sumRef)
</span>245 <span style=''>
</span>246 <span style=''>  override def dataType: DataType = </span><span style='background: #AEF1AE'>evaluate.dataType</span><span style=''>
</span>247 <span style=''>
</span>248 <span style=''>  override def nullable: Boolean = </span><span style='background: #AEF1AE'>false</span><span style=''>
</span>249 <span style=''>
</span>250 <span style=''>  implicit class RichAttribute(a: RefSetterExpression) {
</span>251 <span style=''>    /** Represents this attribute at the mutable buffer side. */
</span>252 <span style=''>    def left: RefSetterExpression = </span><span style='background: #AEF1AE'>a</span><span style=''>
</span>253 <span style=''>
</span>254 <span style=''>    /** Represents this attribute at the input buffer side (the data value is read-only). */
</span>255 <span style=''>    def right: AttributeReference = </span><span style='background: #AEF1AE'>inputAggBufferAttributes(aggBufferAttributes.indexOf(a.from))</span><span style=''>
</span>256 <span style=''>  }
</span>257 <span style=''>
</span>258 <span style=''>  protected def withNewChildrenInternal(newChildren: IndexedSeq[Expression]): Expression =
</span>259 <span style=''>    </span><span style='background: #AEF1AE'>copy(children = newChildren)</span><span style=''>
</span>260 <span style=''>}
</span></pre>
          </div>
          <div class="tab-pane" id="statementlist">
            <table cellspacing="0" cellpadding="0" class="table statementlist">
      <tr>
        <th>Line</th>
        <th>Stmt Id</th>
        <th>Pos</th>
        <th>Tree</th>
        <th>Symbol</th>
        <th>Tests</th>
        <th>Code</th>
      </tr><tr>
        <td>
          17
        </td>
        <td>
          3500
        </td>
        <td>
          810
          -
          1904
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.transform
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          function.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x1: A1, default: A1 =&gt; B1): B1 = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (n @ (_: org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable)) if n.exprId.==(param.exprId) =&gt; {
        &lt;artifact&gt; val x$1: org.apache.spark.sql.types.DataType = newDT;
        &lt;artifact&gt; val x$2: String = n.copy$default$1;
        &lt;artifact&gt; val x$3: Boolean = n.copy$default$3;
        &lt;artifact&gt; val x$4: org.apache.spark.sql.catalyst.expressions.ExprId = n.copy$default$4;
        &lt;artifact&gt; val x$5: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = n.copy$default$5;
        n.copy(x$2, x$1, x$3, x$4, x$5)
      }
      case (cast @ (_: org.apache.spark.sql.catalyst.expressions.Cast)) if cast.child.isInstanceOf[org.apache.spark.sql.catalyst.expressions.Cast].&amp;&amp;(cast.child.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Cast].child.isInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable]) =&gt; {
        val nvl: org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable = cast.child.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Cast].child.asInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable];
        if (nvl.exprId.==(param.exprId))
          org.apache.spark.sql.ShimUtils.cast(org.apache.spark.sql.ShimUtils.cast({
            &lt;artifact&gt; val x$6: org.apache.spark.sql.types.DataType = newDT;
            &lt;artifact&gt; val x$7: String = nvl.copy$default$1;
            &lt;artifact&gt; val x$8: Boolean = nvl.copy$default$3;
            &lt;artifact&gt; val x$9: org.apache.spark.sql.catalyst.expressions.ExprId = nvl.copy$default$4;
            &lt;artifact&gt; val x$10: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = nvl.copy$default$5;
            nvl.copy(x$7, x$6, x$8, x$9, x$10)
          }, newDT), cast.dataType)
        else
          cast
      }
      case (cast @ (_: org.apache.spark.sql.catalyst.expressions.Cast)) if cast.child.isInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable] =&gt; {
        val nvl: org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable = cast.child.asInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable];
        if (nvl.exprId.==(param.exprId))
          org.apache.spark.sql.ShimUtils.cast({
            &lt;artifact&gt; val qual$1: org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable = cast.child.asInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable];
            &lt;artifact&gt; val x$11: org.apache.spark.sql.types.DataType = newDT;
            &lt;artifact&gt; val x$12: String = qual$1.copy$default$1;
            &lt;artifact&gt; val x$13: Boolean = qual$1.copy$default$3;
            &lt;artifact&gt; val x$14: org.apache.spark.sql.catalyst.expressions.ExprId = qual$1.copy$default$4;
            &lt;artifact&gt; val x$15: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = qual$1.copy$default$5;
            qual$1.copy(x$12, x$11, x$13, x$14, x$15)
          }, newDT)
        else
          cast
      }
      case (defaultCase$ @ _) =&gt; default.apply(x1)
    };
    final def isDefinedAt(x1: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (n @ (_: org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable)) if n.exprId.==(param.exprId) =&gt; true
      case (cast @ (_: org.apache.spark.sql.catalyst.expressions.Cast)) if cast.child.isInstanceOf[org.apache.spark.sql.catalyst.expressions.Cast].&amp;&amp;(cast.child.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Cast].child.isInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable]) =&gt; true
      case (cast @ (_: org.apache.spark.sql.catalyst.expressions.Cast)) if cast.child.isInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable] =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          17
        </td>
        <td>
          3499
        </td>
        <td>
          829
          -
          829
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          18
        </td>
        <td>
          3465
        </td>
        <td>
          868
          -
          892
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          n.exprId.==(param.exprId)
        </td>
      </tr><tr>
        <td>
          18
        </td>
        <td>
          3464
        </td>
        <td>
          880
          -
          892
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.exprId
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          param.exprId
        </td>
      </tr><tr>
        <td>
          19
        </td>
        <td>
          3468
        </td>
        <td>
          906
          -
          906
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          n.copy$default$4
        </td>
      </tr><tr>
        <td>
          19
        </td>
        <td>
          3470
        </td>
        <td>
          904
          -
          928
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          n.copy(x$2, x$1, x$3, x$4, x$5)
        </td>
      </tr><tr>
        <td>
          19
        </td>
        <td>
          3467
        </td>
        <td>
          906
          -
          906
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          n.copy$default$3
        </td>
      </tr><tr>
        <td>
          19
        </td>
        <td>
          3466
        </td>
        <td>
          906
          -
          906
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          n.copy$default$1
        </td>
      </tr><tr>
        <td>
          19
        </td>
        <td>
          3469
        </td>
        <td>
          906
          -
          906
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          n.copy$default$5
        </td>
      </tr><tr>
        <td>
          22
        </td>
        <td>
          3471
        </td>
        <td>
          1206
          -
          1275
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.isInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          cast.child.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Cast].child.isInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable]
        </td>
      </tr><tr>
        <td>
          22
        </td>
        <td>
          3472
        </td>
        <td>
          1173
          -
          1275
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          cast.child.isInstanceOf[org.apache.spark.sql.catalyst.expressions.Cast].&amp;&amp;(cast.child.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Cast].child.isInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable])
        </td>
      </tr><tr>
        <td>
          23
        </td>
        <td>
          3473
        </td>
        <td>
          1297
          -
          1366
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.asInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          cast.child.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Cast].child.asInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable]
        </td>
      </tr><tr>
        <td>
          24
        </td>
        <td>
          3474
        </td>
        <td>
          1393
          -
          1405
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.exprId
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          param.exprId
        </td>
      </tr><tr>
        <td>
          24
        </td>
        <td>
          3475
        </td>
        <td>
          1379
          -
          1405
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          nvl.exprId.==(param.exprId)
        </td>
      </tr><tr>
        <td>
          25
        </td>
        <td>
          3480
        </td>
        <td>
          1429
          -
          1455
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          nvl.copy(x$7, x$6, x$8, x$9, x$10)
        </td>
      </tr><tr>
        <td>
          25
        </td>
        <td>
          3483
        </td>
        <td>
          1417
          -
          1479
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.cast
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.ShimUtils.cast(org.apache.spark.sql.ShimUtils.cast({
  &lt;artifact&gt; val x$6: org.apache.spark.sql.types.DataType = newDT;
  &lt;artifact&gt; val x$7: String = nvl.copy$default$1;
  &lt;artifact&gt; val x$8: Boolean = nvl.copy$default$3;
  &lt;artifact&gt; val x$9: org.apache.spark.sql.catalyst.expressions.ExprId = nvl.copy$default$4;
  &lt;artifact&gt; val x$10: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = nvl.copy$default$5;
  nvl.copy(x$7, x$6, x$8, x$9, x$10)
}, newDT), cast.dataType)
        </td>
      </tr><tr>
        <td>
          25
        </td>
        <td>
          3477
        </td>
        <td>
          1433
          -
          1433
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          nvl.copy$default$3
        </td>
      </tr><tr>
        <td>
          25
        </td>
        <td>
          3479
        </td>
        <td>
          1433
          -
          1433
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          nvl.copy$default$5
        </td>
      </tr><tr>
        <td>
          25
        </td>
        <td>
          3482
        </td>
        <td>
          1465
          -
          1478
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Cast.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          cast.dataType
        </td>
      </tr><tr>
        <td>
          25
        </td>
        <td>
          3476
        </td>
        <td>
          1433
          -
          1433
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          nvl.copy$default$1
        </td>
      </tr><tr>
        <td>
          25
        </td>
        <td>
          3484
        </td>
        <td>
          1417
          -
          1479
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.cast
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.cast(org.apache.spark.sql.ShimUtils.cast({
  &lt;artifact&gt; val x$6: org.apache.spark.sql.types.DataType = newDT;
  &lt;artifact&gt; val x$7: String = nvl.copy$default$1;
  &lt;artifact&gt; val x$8: Boolean = nvl.copy$default$3;
  &lt;artifact&gt; val x$9: org.apache.spark.sql.catalyst.expressions.ExprId = nvl.copy$default$4;
  &lt;artifact&gt; val x$10: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = nvl.copy$default$5;
  nvl.copy(x$7, x$6, x$8, x$9, x$10)
}, newDT), cast.dataType)
        </td>
      </tr><tr>
        <td>
          25
        </td>
        <td>
          3478
        </td>
        <td>
          1433
          -
          1433
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          nvl.copy$default$4
        </td>
      </tr><tr>
        <td>
          25
        </td>
        <td>
          3481
        </td>
        <td>
          1423
          -
          1463
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.cast
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.cast({
  &lt;artifact&gt; val x$6: org.apache.spark.sql.types.DataType = newDT;
  &lt;artifact&gt; val x$7: String = nvl.copy$default$1;
  &lt;artifact&gt; val x$8: Boolean = nvl.copy$default$3;
  &lt;artifact&gt; val x$9: org.apache.spark.sql.catalyst.expressions.ExprId = nvl.copy$default$4;
  &lt;artifact&gt; val x$10: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = nvl.copy$default$5;
  nvl.copy(x$7, x$6, x$8, x$9, x$10)
}, newDT)
        </td>
      </tr><tr>
        <td>
          27
        </td>
        <td>
          3485
        </td>
        <td>
          1503
          -
          1507
        </td>
        <td>
          Ident
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.$anonfun.cast
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          cast
        </td>
      </tr><tr>
        <td>
          29
        </td>
        <td>
          3486
        </td>
        <td>
          1605
          -
          1649
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.isInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          cast.child.isInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable]
        </td>
      </tr><tr>
        <td>
          30
        </td>
        <td>
          3487
        </td>
        <td>
          1671
          -
          1715
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.asInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          cast.child.asInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable]
        </td>
      </tr><tr>
        <td>
          31
        </td>
        <td>
          3489
        </td>
        <td>
          1728
          -
          1754
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          nvl.exprId.==(param.exprId)
        </td>
      </tr><tr>
        <td>
          31
        </td>
        <td>
          3488
        </td>
        <td>
          1742
          -
          1754
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.exprId
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          param.exprId
        </td>
      </tr><tr>
        <td>
          32
        </td>
        <td>
          3497
        </td>
        <td>
          1766
          -
          1869
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.cast
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.cast({
  &lt;artifact&gt; val qual$1: org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable = cast.child.asInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable];
  &lt;artifact&gt; val x$11: org.apache.spark.sql.types.DataType = newDT;
  &lt;artifact&gt; val x$12: String = qual$1.copy$default$1;
  &lt;artifact&gt; val x$13: Boolean = qual$1.copy$default$3;
  &lt;artifact&gt; val x$14: org.apache.spark.sql.catalyst.expressions.ExprId = qual$1.copy$default$4;
  &lt;artifact&gt; val x$15: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = qual$1.copy$default$5;
  qual$1.copy(x$12, x$11, x$13, x$14, x$15)
}, newDT)
        </td>
      </tr><tr>
        <td>
          32
        </td>
        <td>
          3496
        </td>
        <td>
          1766
          -
          1869
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.cast
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.cast({
  &lt;artifact&gt; val qual$1: org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable = cast.child.asInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable];
  &lt;artifact&gt; val x$11: org.apache.spark.sql.types.DataType = newDT;
  &lt;artifact&gt; val x$12: String = qual$1.copy$default$1;
  &lt;artifact&gt; val x$13: Boolean = qual$1.copy$default$3;
  &lt;artifact&gt; val x$14: org.apache.spark.sql.catalyst.expressions.ExprId = qual$1.copy$default$4;
  &lt;artifact&gt; val x$15: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = qual$1.copy$default$5;
  qual$1.copy(x$12, x$11, x$13, x$14, x$15)
}, newDT)
        </td>
      </tr><tr>
        <td>
          32
        </td>
        <td>
          3490
        </td>
        <td>
          1781
          -
          1825
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.asInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          cast.child.asInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable]
        </td>
      </tr><tr>
        <td>
          33
        </td>
        <td>
          3492
        </td>
        <td>
          1839
          -
          1839
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          qual$1.copy$default$3
        </td>
      </tr><tr>
        <td>
          33
        </td>
        <td>
          3495
        </td>
        <td>
          1781
          -
          1861
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          qual$1.copy(x$12, x$11, x$13, x$14, x$15)
        </td>
      </tr><tr>
        <td>
          33
        </td>
        <td>
          3494
        </td>
        <td>
          1839
          -
          1839
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          qual$1.copy$default$5
        </td>
      </tr><tr>
        <td>
          33
        </td>
        <td>
          3491
        </td>
        <td>
          1839
          -
          1839
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          qual$1.copy$default$1
        </td>
      </tr><tr>
        <td>
          33
        </td>
        <td>
          3493
        </td>
        <td>
          1839
          -
          1839
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          qual$1.copy$default$4
        </td>
      </tr><tr>
        <td>
          35
        </td>
        <td>
          3498
        </td>
        <td>
          1893
          -
          1897
        </td>
        <td>
          Ident
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.$anonfun.cast
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          cast
        </td>
      </tr><tr>
        <td>
          71
        </td>
        <td>
          3501
        </td>
        <td>
          3996
          -
          3996
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple5._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$1._1
        </td>
      </tr><tr>
        <td>
          71
        </td>
        <td>
          3504
        </td>
        <td>
          4041
          -
          4041
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple5._4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$1._4
        </td>
      </tr><tr>
        <td>
          71
        </td>
        <td>
          3503
        </td>
        <td>
          4029
          -
          4029
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple5._3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$1._3
        </td>
      </tr><tr>
        <td>
          71
        </td>
        <td>
          3502
        </td>
        <td>
          4023
          -
          4023
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple5._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$1._2
        </td>
      </tr><tr>
        <td>
          71
        </td>
        <td>
          3505
        </td>
        <td>
          4049
          -
          4049
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple5._5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$1._5
        </td>
      </tr><tr>
        <td>
          79
        </td>
        <td>
          3506
        </td>
        <td>
          4255
          -
          4372
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.aggregate.AggregateFunction.toAggregateExpression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ExpressionAggregates.apply(scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](count, sum1, sum2, ifExpr, useSum, useEvaluate), zero, add, sumType).toAggregateExpression()
        </td>
      </tr><tr>
        <td>
          90
        </td>
        <td>
          3507
        </td>
        <td>
          4753
          -
          4761
        </td>
        <td>
          Ident
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.function
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          function
        </td>
      </tr><tr>
        <td>
          92
        </td>
        <td>
          3509
        </td>
        <td>
          4789
          -
          4832
        </td>
        <td>
          Block
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.transformSumType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AggregateExpressions.this.transformSumType(function, sparam, sumType)
        </td>
      </tr><tr>
        <td>
          92
        </td>
        <td>
          3508
        </td>
        <td>
          4789
          -
          4832
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.transformSumType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AggregateExpressions.this.transformSumType(function, sparam, sumType)
        </td>
      </tr><tr>
        <td>
          94
        </td>
        <td>
          3510
        </td>
        <td>
          4851
          -
          4851
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
      </tr><tr>
        <td>
          94
        </td>
        <td>
          3522
        </td>
        <td>
          4842
          -
          4842
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$6
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$6
        </td>
      </tr><tr>
        <td>
          94
        </td>
        <td>
          3512
        </td>
        <td>
          4847
          -
          4890
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(sumType, nullable, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3), cref)
        </td>
      </tr><tr>
        <td>
          94
        </td>
        <td>
          3521
        </td>
        <td>
          4842
          -
          4842
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$5
        </td>
      </tr><tr>
        <td>
          94
        </td>
        <td>
          3511
        </td>
        <td>
          4851
          -
          4883
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply(sumType, nullable, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)
        </td>
      </tr><tr>
        <td>
          94
        </td>
        <td>
          3520
        </td>
        <td>
          4842
          -
          4842
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$4
        </td>
      </tr><tr>
        <td>
          94
        </td>
        <td>
          3523
        </td>
        <td>
          4842
          -
          5008
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply(scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(sumType, nullable, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3), cref), org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply(correctedFunction, scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable]({
  &lt;artifact&gt; val x$1: org.apache.spark.sql.types.DataType = sumType;
  &lt;artifact&gt; val x$2: String = sparam.copy$default$1;
  &lt;artifact&gt; val x$3: Boolean = sparam.copy$default$3;
  &lt;artifact&gt; val x$4: org.apache.spark.sql.catalyst.expressions.ExprId = sparam.copy$default$4;
  &lt;artifact&gt; val x$5: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = sparam.copy$default$5;
  sparam.copy(x$2, x$1, x$3, x$4, x$5)
}, cparam), hidden), name, org.apache.spark.sql.qualityFunctions.FunN.apply$default$4, org.apache.spark.sql.qualityFunctions.FunN.apply$default$5, org.apache.spark.sql.qualityFunctions.FunN.apply$default$6)
        </td>
      </tr><tr>
        <td>
          95
        </td>
        <td>
          3519
        </td>
        <td>
          4902
          -
          5001
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply(correctedFunction, scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable]({
  &lt;artifact&gt; val x$1: org.apache.spark.sql.types.DataType = sumType;
  &lt;artifact&gt; val x$2: String = sparam.copy$default$1;
  &lt;artifact&gt; val x$3: Boolean = sparam.copy$default$3;
  &lt;artifact&gt; val x$4: org.apache.spark.sql.catalyst.expressions.ExprId = sparam.copy$default$4;
  &lt;artifact&gt; val x$5: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = sparam.copy$default$5;
  sparam.copy(x$2, x$1, x$3, x$4, x$5)
}, cparam), hidden)
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          3516
        </td>
        <td>
          4959
          -
          4959
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          sparam.copy$default$5
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          3513
        </td>
        <td>
          4959
          -
          4959
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          sparam.copy$default$1
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          3515
        </td>
        <td>
          4959
          -
          4959
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          sparam.copy$default$4
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          3518
        </td>
        <td>
          4948
          -
          4992
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable]({
  &lt;artifact&gt; val x$1: org.apache.spark.sql.types.DataType = sumType;
  &lt;artifact&gt; val x$2: String = sparam.copy$default$1;
  &lt;artifact&gt; val x$3: Boolean = sparam.copy$default$3;
  &lt;artifact&gt; val x$4: org.apache.spark.sql.catalyst.expressions.ExprId = sparam.copy$default$4;
  &lt;artifact&gt; val x$5: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = sparam.copy$default$5;
  sparam.copy(x$2, x$1, x$3, x$4, x$5)
}, cparam)
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          3517
        </td>
        <td>
          4952
          -
          4983
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          sparam.copy(x$2, x$1, x$3, x$4, x$5)
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          3514
        </td>
        <td>
          4959
          -
          4959
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          sparam.copy$default$3
        </td>
      </tr><tr>
        <td>
          103
        </td>
        <td>
          3524
        </td>
        <td>
          5326
          -
          5378
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.asInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          params.apply(paramIndex).asInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable]
        </td>
      </tr><tr>
        <td>
          106
        </td>
        <td>
          3525
        </td>
        <td>
          5451
          -
          5459
        </td>
        <td>
          Ident
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.function
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          function
        </td>
      </tr><tr>
        <td>
          108
        </td>
        <td>
          3527
        </td>
        <td>
          5487
          -
          5530
        </td>
        <td>
          Block
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.transformSumType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AggregateExpressions.this.transformSumType(function, sparam, sumType)
        </td>
      </tr><tr>
        <td>
          108
        </td>
        <td>
          3526
        </td>
        <td>
          5487
          -
          5530
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.transformSumType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AggregateExpressions.this.transformSumType(function, sparam, sumType)
        </td>
      </tr><tr>
        <td>
          110
        </td>
        <td>
          3528
        </td>
        <td>
          5555
          -
          5599
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply(sumType, nullable, paramIndex)
        </td>
      </tr><tr>
        <td>
          110
        </td>
        <td>
          3546
        </td>
        <td>
          5540
          -
          5902
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunForward.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunForward.apply(scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(sumType, nullable, paramIndex), cref).:+[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]]({
  &lt;artifact&gt; val x$14: org.apache.spark.sql.catalyst.expressions.LambdaFunction = org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply(correctedFunction, {
    &lt;synthetic&gt; &lt;artifact&gt; val x$2: org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable = {
      &lt;artifact&gt; val x$9: org.apache.spark.sql.types.DataType = sumType;
      &lt;artifact&gt; val x$10: String = sparam.copy$default$1;
      &lt;artifact&gt; val x$11: Boolean = sparam.copy$default$3;
      &lt;artifact&gt; val x$12: org.apache.spark.sql.catalyst.expressions.ExprId = sparam.copy$default$4;
      &lt;artifact&gt; val x$13: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = sparam.copy$default$5;
      sparam.copy(x$10, x$9, x$11, x$12, x$13)
    };
    params.drop(1).+:[org.apache.spark.sql.catalyst.expressions.NamedExpression, Seq[org.apache.spark.sql.catalyst.expressions.NamedExpression]](x$2)(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.NamedExpression])
  }, hidden);
  &lt;artifact&gt; val x$15: Seq[org.apache.spark.sql.catalyst.expressions.Expression] @scala.reflect.internal.annotations.uncheckedBounds = funparams.updated[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](paramIndex, {
    &lt;artifact&gt; val qual$1: org.apache.spark.sql.qualityFunctions.RefExpression = funparams.apply(paramIndex).asInstanceOf[org.apache.spark.sql.qualityFunctions.RefExpression];
    &lt;artifact&gt; val x$6: org.apache.spark.sql.types.DataType = sumType;
    &lt;artifact&gt; val x$7: Boolean = qual$1.copy$default$2;
    &lt;artifact&gt; val x$8: Int = qual$1.copy$default$3;
    qual$1.copy(x$6, x$7, x$8)
  })(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]);
  &lt;artifact&gt; val x$16: Option[String] @scala.reflect.internal.annotations.uncheckedBounds = i.copy$default$3;
  &lt;artifact&gt; val x$17: Boolean = i.copy$default$4;
  &lt;artifact&gt; val x$18: Boolean = i.copy$default$5;
  &lt;artifact&gt; val x$19: Boolean = i.copy$default$6;
  i.copy(x$15, x$14, x$16, x$17, x$18, x$19)
})(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          110
        </td>
        <td>
          3545
        </td>
        <td>
          5551
          -
          5901
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.SeqLike.:+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(sumType, nullable, paramIndex), cref).:+[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]]({
  &lt;artifact&gt; val x$14: org.apache.spark.sql.catalyst.expressions.LambdaFunction = org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply(correctedFunction, {
    &lt;synthetic&gt; &lt;artifact&gt; val x$2: org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable = {
      &lt;artifact&gt; val x$9: org.apache.spark.sql.types.DataType = sumType;
      &lt;artifact&gt; val x$10: String = sparam.copy$default$1;
      &lt;artifact&gt; val x$11: Boolean = sparam.copy$default$3;
      &lt;artifact&gt; val x$12: org.apache.spark.sql.catalyst.expressions.ExprId = sparam.copy$default$4;
      &lt;artifact&gt; val x$13: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = sparam.copy$default$5;
      sparam.copy(x$10, x$9, x$11, x$12, x$13)
    };
    params.drop(1).+:[org.apache.spark.sql.catalyst.expressions.NamedExpression, Seq[org.apache.spark.sql.catalyst.expressions.NamedExpression]](x$2)(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.NamedExpression])
  }, hidden);
  &lt;artifact&gt; val x$15: Seq[org.apache.spark.sql.catalyst.expressions.Expression] @scala.reflect.internal.annotations.uncheckedBounds = funparams.updated[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](paramIndex, {
    &lt;artifact&gt; val qual$1: org.apache.spark.sql.qualityFunctions.RefExpression = funparams.apply(paramIndex).asInstanceOf[org.apache.spark.sql.qualityFunctions.RefExpression];
    &lt;artifact&gt; val x$6: org.apache.spark.sql.types.DataType = sumType;
    &lt;artifact&gt; val x$7: Boolean = qual$1.copy$default$2;
    &lt;artifact&gt; val x$8: Int = qual$1.copy$default$3;
    qual$1.copy(x$6, x$7, x$8)
  })(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]);
  &lt;artifact&gt; val x$16: Option[String] @scala.reflect.internal.annotations.uncheckedBounds = i.copy$default$3;
  &lt;artifact&gt; val x$17: Boolean = i.copy$default$4;
  &lt;artifact&gt; val x$18: Boolean = i.copy$default$5;
  &lt;artifact&gt; val x$19: Boolean = i.copy$default$6;
  i.copy(x$15, x$14, x$16, x$17, x$18, x$19)
})(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression])
        </td>
      </tr><tr>
        <td>
          110
        </td>
        <td>
          3544
        </td>
        <td>
          5607
          -
          5607
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]
        </td>
      </tr><tr>
        <td>
          111
        </td>
        <td>
          3540
        </td>
        <td>
          5622
          -
          5622
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.copy$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.copy$default$4
        </td>
      </tr><tr>
        <td>
          111
        </td>
        <td>
          3543
        </td>
        <td>
          5620
          -
          5901
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.copy(x$15, x$14, x$16, x$17, x$18, x$19)
        </td>
      </tr><tr>
        <td>
          111
        </td>
        <td>
          3539
        </td>
        <td>
          5622
          -
          5622
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.copy$default$3
        </td>
      </tr><tr>
        <td>
          111
        </td>
        <td>
          3542
        </td>
        <td>
          5622
          -
          5622
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.copy$default$6
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.copy$default$6
        </td>
      </tr><tr>
        <td>
          111
        </td>
        <td>
          3532
        </td>
        <td>
          5638
          -
          5742
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply(correctedFunction, {
  &lt;synthetic&gt; &lt;artifact&gt; val x$2: org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable = {
    &lt;artifact&gt; val x$9: org.apache.spark.sql.types.DataType = sumType;
    &lt;artifact&gt; val x$10: String = sparam.copy$default$1;
    &lt;artifact&gt; val x$11: Boolean = sparam.copy$default$3;
    &lt;artifact&gt; val x$12: org.apache.spark.sql.catalyst.expressions.ExprId = sparam.copy$default$4;
    &lt;artifact&gt; val x$13: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = sparam.copy$default$5;
    sparam.copy(x$10, x$9, x$11, x$12, x$13)
  };
  params.drop(1).+:[org.apache.spark.sql.catalyst.expressions.NamedExpression, Seq[org.apache.spark.sql.catalyst.expressions.NamedExpression]](x$2)(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.NamedExpression])
}, hidden)
        </td>
      </tr><tr>
        <td>
          111
        </td>
        <td>
          3541
        </td>
        <td>
          5622
          -
          5622
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.copy$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.copy$default$5
        </td>
      </tr><tr>
        <td>
          112
        </td>
        <td>
          3531
        </td>
        <td>
          5684
          -
          5733
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.SeqLike.+:
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          params.drop(1).+:[org.apache.spark.sql.catalyst.expressions.NamedExpression, Seq[org.apache.spark.sql.catalyst.expressions.NamedExpression]](x$2)(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.NamedExpression])
        </td>
      </tr><tr>
        <td>
          112
        </td>
        <td>
          3530
        </td>
        <td>
          5716
          -
          5716
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.NamedExpression]
        </td>
      </tr><tr>
        <td>
          112
        </td>
        <td>
          3529
        </td>
        <td>
          5731
          -
          5732
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          1
        </td>
      </tr><tr>
        <td>
          113
        </td>
        <td>
          3537
        </td>
        <td>
          5785
          -
          5785
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]
        </td>
      </tr><tr>
        <td>
          113
        </td>
        <td>
          3538
        </td>
        <td>
          5768
          -
          5900
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.SeqLike.updated
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          funparams.updated[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](paramIndex, {
  &lt;artifact&gt; val qual$1: org.apache.spark.sql.qualityFunctions.RefExpression = funparams.apply(paramIndex).asInstanceOf[org.apache.spark.sql.qualityFunctions.RefExpression];
  &lt;artifact&gt; val x$6: org.apache.spark.sql.types.DataType = sumType;
  &lt;artifact&gt; val x$7: Boolean = qual$1.copy$default$2;
  &lt;artifact&gt; val x$8: Int = qual$1.copy$default$3;
  qual$1.copy(x$6, x$7, x$8)
})(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression])
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          3534
        </td>
        <td>
          5862
          -
          5862
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.copy$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          qual$1.copy$default$2
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          3533
        </td>
        <td>
          5812
          -
          5861
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.asInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          funparams.apply(paramIndex).asInstanceOf[org.apache.spark.sql.qualityFunctions.RefExpression]
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          3536
        </td>
        <td>
          5812
          -
          5886
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          qual$1.copy(x$6, x$7, x$8)
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          3535
        </td>
        <td>
          5862
          -
          5862
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          qual$1.copy$default$3
        </td>
      </tr><tr>
        <td>
          131
        </td>
        <td>
          3547
        </td>
        <td>
          6369
          -
          6372
        </td>
        <td>
          Ident
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.fun
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          fun
        </td>
      </tr><tr>
        <td>
          133
        </td>
        <td>
          3549
        </td>
        <td>
          6400
          -
          6437
        </td>
        <td>
          Block
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.transformSumType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AggregateExpressions.this.transformSumType(fun, param, sumType)
        </td>
      </tr><tr>
        <td>
          133
        </td>
        <td>
          3548
        </td>
        <td>
          6400
          -
          6437
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.transformSumType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AggregateExpressions.this.transformSumType(fun, param, sumType)
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          3561
        </td>
        <td>
          6447
          -
          6447
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$4
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          3552
        </td>
        <td>
          6452
          -
          6489
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.qualityFunctions.RefExpression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(sumType, nullable, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3))
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          3564
        </td>
        <td>
          6447
          -
          6657
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply(scala.collection.Seq.apply[org.apache.spark.sql.qualityFunctions.RefExpression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(sumType, nullable, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)), org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply(org.apache.spark.sql.ShimUtils.cast(correctedFunction, sumType), scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable]({
  &lt;artifact&gt; val x$1: org.apache.spark.sql.types.DataType = sumType;
  &lt;artifact&gt; val x$2: String = param.copy$default$1;
  &lt;artifact&gt; val x$3: Boolean = param.copy$default$3;
  &lt;artifact&gt; val x$4: org.apache.spark.sql.catalyst.expressions.ExprId = param.copy$default$4;
  &lt;artifact&gt; val x$5: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = param.copy$default$5;
  param.copy(x$2, x$1, x$3, x$4, x$5)
}), hidden), name, org.apache.spark.sql.qualityFunctions.FunN.apply$default$4, org.apache.spark.sql.qualityFunctions.FunN.apply$default$5, org.apache.spark.sql.qualityFunctions.FunN.apply$default$6)
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          3563
        </td>
        <td>
          6447
          -
          6447
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$6
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$6
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          3551
        </td>
        <td>
          6456
          -
          6488
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply(sumType, nullable, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          3562
        </td>
        <td>
          6447
          -
          6447
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$5
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          3550
        </td>
        <td>
          6456
          -
          6456
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          3560
        </td>
        <td>
          6501
          -
          6650
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply(org.apache.spark.sql.ShimUtils.cast(correctedFunction, sumType), scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable]({
  &lt;artifact&gt; val x$1: org.apache.spark.sql.types.DataType = sumType;
  &lt;artifact&gt; val x$2: String = param.copy$default$1;
  &lt;artifact&gt; val x$3: Boolean = param.copy$default$3;
  &lt;artifact&gt; val x$4: org.apache.spark.sql.catalyst.expressions.ExprId = param.copy$default$4;
  &lt;artifact&gt; val x$5: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = param.copy$default$5;
  param.copy(x$2, x$1, x$3, x$4, x$5)
}), hidden)
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          3553
        </td>
        <td>
          6516
          -
          6548
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.cast
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.cast(correctedFunction, sumType)
        </td>
      </tr><tr>
        <td>
          137
        </td>
        <td>
          3558
        </td>
        <td>
          6610
          -
          6640
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          param.copy(x$2, x$1, x$3, x$4, x$5)
        </td>
      </tr><tr>
        <td>
          137
        </td>
        <td>
          3555
        </td>
        <td>
          6616
          -
          6616
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          param.copy$default$3
        </td>
      </tr><tr>
        <td>
          137
        </td>
        <td>
          3554
        </td>
        <td>
          6616
          -
          6616
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          param.copy$default$1
        </td>
      </tr><tr>
        <td>
          137
        </td>
        <td>
          3557
        </td>
        <td>
          6616
          -
          6616
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          param.copy$default$5
        </td>
      </tr><tr>
        <td>
          137
        </td>
        <td>
          3556
        </td>
        <td>
          6616
          -
          6616
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          param.copy$default$4
        </td>
      </tr><tr>
        <td>
          137
        </td>
        <td>
          3559
        </td>
        <td>
          6606
          -
          6641
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable]({
  &lt;artifact&gt; val x$1: org.apache.spark.sql.types.DataType = sumType;
  &lt;artifact&gt; val x$2: String = param.copy$default$1;
  &lt;artifact&gt; val x$3: Boolean = param.copy$default$3;
  &lt;artifact&gt; val x$4: org.apache.spark.sql.catalyst.expressions.ExprId = param.copy$default$4;
  &lt;artifact&gt; val x$5: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = param.copy$default$5;
  param.copy(x$2, x$1, x$3, x$4, x$5)
})
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          3565
        </td>
        <td>
          6925
          -
          6933
        </td>
        <td>
          Ident
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.function
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          function
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          3567
        </td>
        <td>
          6961
          -
          7005
        </td>
        <td>
          Block
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.transformSumType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AggregateExpressions.this.transformSumType(function, param, valueType)
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          3566
        </td>
        <td>
          6961
          -
          7005
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.transformSumType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AggregateExpressions.this.transformSumType(function, param, valueType)
        </td>
      </tr><tr>
        <td>
          150
        </td>
        <td>
          3579
        </td>
        <td>
          7015
          -
          7240
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.MapTransform.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.MapTransform.apply(org.apache.spark.sql.qualityFunctions.RefExpression.apply(sumType, r.nullable, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3), key, org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply(org.apache.spark.sql.ShimUtils.cast(correctedFunction, valueType), scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable]({
  &lt;artifact&gt; val x$6: org.apache.spark.sql.types.DataType = valueType;
  &lt;artifact&gt; val x$7: String = param.copy$default$1;
  &lt;artifact&gt; val x$8: Boolean = param.copy$default$3;
  &lt;artifact&gt; val x$9: org.apache.spark.sql.catalyst.expressions.ExprId = param.copy$default$4;
  &lt;artifact&gt; val x$10: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = param.copy$default$5;
  param.copy(x$7, x$6, x$8, x$9, x$10)
}), hidden), zeroF)
        </td>
      </tr><tr>
        <td>
          150
        </td>
        <td>
          3570
        </td>
        <td>
          7028
          -
          7062
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply(sumType, r.nullable, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)
        </td>
      </tr><tr>
        <td>
          150
        </td>
        <td>
          3569
        </td>
        <td>
          7028
          -
          7028
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
      </tr><tr>
        <td>
          150
        </td>
        <td>
          3568
        </td>
        <td>
          7051
          -
          7061
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.nullable
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          r.nullable
        </td>
      </tr><tr>
        <td>
          151
        </td>
        <td>
          3578
        </td>
        <td>
          7079
          -
          7232
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply(org.apache.spark.sql.ShimUtils.cast(correctedFunction, valueType), scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable]({
  &lt;artifact&gt; val x$6: org.apache.spark.sql.types.DataType = valueType;
  &lt;artifact&gt; val x$7: String = param.copy$default$1;
  &lt;artifact&gt; val x$8: Boolean = param.copy$default$3;
  &lt;artifact&gt; val x$9: org.apache.spark.sql.catalyst.expressions.ExprId = param.copy$default$4;
  &lt;artifact&gt; val x$10: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = param.copy$default$5;
  param.copy(x$7, x$6, x$8, x$9, x$10)
}), hidden)
        </td>
      </tr><tr>
        <td>
          151
        </td>
        <td>
          3571
        </td>
        <td>
          7094
          -
          7128
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.cast
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.cast(correctedFunction, valueType)
        </td>
      </tr><tr>
        <td>
          152
        </td>
        <td>
          3576
        </td>
        <td>
          7190
          -
          7222
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          param.copy(x$7, x$6, x$8, x$9, x$10)
        </td>
      </tr><tr>
        <td>
          152
        </td>
        <td>
          3573
        </td>
        <td>
          7196
          -
          7196
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          param.copy$default$3
        </td>
      </tr><tr>
        <td>
          152
        </td>
        <td>
          3572
        </td>
        <td>
          7196
          -
          7196
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          param.copy$default$1
        </td>
      </tr><tr>
        <td>
          152
        </td>
        <td>
          3575
        </td>
        <td>
          7196
          -
          7196
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          param.copy$default$5
        </td>
      </tr><tr>
        <td>
          152
        </td>
        <td>
          3577
        </td>
        <td>
          7186
          -
          7223
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable]({
  &lt;artifact&gt; val x$6: org.apache.spark.sql.types.DataType = valueType;
  &lt;artifact&gt; val x$7: String = param.copy$default$1;
  &lt;artifact&gt; val x$8: Boolean = param.copy$default$3;
  &lt;artifact&gt; val x$9: org.apache.spark.sql.catalyst.expressions.ExprId = param.copy$default$4;
  &lt;artifact&gt; val x$10: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = param.copy$default$5;
  param.copy(x$7, x$6, x$8, x$9, x$10)
})
        </td>
      </tr><tr>
        <td>
          152
        </td>
        <td>
          3574
        </td>
        <td>
          7196
          -
          7196
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable.copy$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          param.copy$default$4
        </td>
      </tr><tr>
        <td>
          159
        </td>
        <td>
          3580
        </td>
        <td>
          7552
          -
          7604
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.asInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          params.apply(paramIndex).asInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable]
        </td>
      </tr><tr>
        <td>
          162
        </td>
        <td>
          3581
        </td>
        <td>
          7677
          -
          7685
        </td>
        <td>
          Ident
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.function
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          function
        </td>
      </tr><tr>
        <td>
          164
        </td>
        <td>
          3582
        </td>
        <td>
          7713
          -
          7756
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.transformSumType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AggregateExpressions.this.transformSumType(function, sparam, sumType)
        </td>
      </tr><tr>
        <td>
          164
        </td>
        <td>
          3583
        </td>
        <td>
          7713
          -
          7756
        </td>
        <td>
          Block
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.transformSumType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AggregateExpressions.this.transformSumType(function, sparam, sumType)
        </td>
      </tr><tr>
        <td>
          166
        </td>
        <td>
          3603
        </td>
        <td>
          7766
          -
          8181
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunForward.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunForward.apply(scala.collection.Seq.apply[org.apache.spark.sql.qualityFunctions.RefExpression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(sumType, nullable, paramIndex)).:+[org.apache.spark.sql.catalyst.expressions.Expression with Serializable, Seq[org.apache.spark.sql.catalyst.expressions.Expression]]({
  &lt;artifact&gt; val x$19: org.apache.spark.sql.catalyst.expressions.LambdaFunction = org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply(org.apache.spark.sql.ShimUtils.cast(correctedFunction, sumType), {
    &lt;synthetic&gt; &lt;artifact&gt; val x$3: org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable = {
      &lt;artifact&gt; val x$14: org.apache.spark.sql.types.DataType = sumType;
      &lt;artifact&gt; val x$15: String = sparam.copy$default$1;
      &lt;artifact&gt; val x$16: Boolean = sparam.copy$default$3;
      &lt;artifact&gt; val x$17: org.apache.spark.sql.catalyst.expressions.ExprId = sparam.copy$default$4;
      &lt;artifact&gt; val x$18: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = sparam.copy$default$5;
      sparam.copy(x$15, x$14, x$16, x$17, x$18)
    };
    params.drop(1).+:[org.apache.spark.sql.catalyst.expressions.NamedExpression, Seq[org.apache.spark.sql.catalyst.expressions.NamedExpression]](x$3)(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.NamedExpression])
  }, hidden);
  &lt;artifact&gt; val x$20: Seq[org.apache.spark.sql.catalyst.expressions.Expression] @scala.reflect.internal.annotations.uncheckedBounds = funparams.updated[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](paramIndex, {
    &lt;artifact&gt; val qual$1: org.apache.spark.sql.qualityFunctions.RefExpression = funparams.apply(paramIndex).asInstanceOf[org.apache.spark.sql.qualityFunctions.RefExpression];
    &lt;artifact&gt; val x$11: org.apache.spark.sql.types.DataType = sumType;
    &lt;artifact&gt; val x$12: Boolean = qual$1.copy$default$2;
    &lt;artifact&gt; val x$13: Int = qual$1.copy$default$3;
    qual$1.copy(x$11, x$12, x$13)
  })(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]);
  &lt;artifact&gt; val x$21: Option[String] @scala.reflect.internal.annotations.uncheckedBounds = i.copy$default$3;
  &lt;artifact&gt; val x$22: Boolean = i.copy$default$4;
  &lt;artifact&gt; val x$23: Boolean = i.copy$default$5;
  &lt;artifact&gt; val x$24: Boolean = i.copy$default$6;
  i.copy(x$20, x$19, x$21, x$22, x$23, x$24)
})(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression with Serializable]))
        </td>
      </tr><tr>
        <td>
          166
        </td>
        <td>
          3602
        </td>
        <td>
          7777
          -
          8180
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.SeqLike.:+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.qualityFunctions.RefExpression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(sumType, nullable, paramIndex)).:+[org.apache.spark.sql.catalyst.expressions.Expression with Serializable, Seq[org.apache.spark.sql.catalyst.expressions.Expression]]({
  &lt;artifact&gt; val x$19: org.apache.spark.sql.catalyst.expressions.LambdaFunction = org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply(org.apache.spark.sql.ShimUtils.cast(correctedFunction, sumType), {
    &lt;synthetic&gt; &lt;artifact&gt; val x$3: org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable = {
      &lt;artifact&gt; val x$14: org.apache.spark.sql.types.DataType = sumType;
      &lt;artifact&gt; val x$15: String = sparam.copy$default$1;
      &lt;artifact&gt; val x$16: Boolean = sparam.copy$default$3;
      &lt;artifact&gt; val x$17: org.apache.spark.sql.catalyst.expressions.ExprId = sparam.copy$default$4;
      &lt;artifact&gt; val x$18: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = sparam.copy$default$5;
      sparam.copy(x$15, x$14, x$16, x$17, x$18)
    };
    params.drop(1).+:[org.apache.spark.sql.catalyst.expressions.NamedExpression, Seq[org.apache.spark.sql.catalyst.expressions.NamedExpression]](x$3)(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.NamedExpression])
  }, hidden);
  &lt;artifact&gt; val x$20: Seq[org.apache.spark.sql.catalyst.expressions.Expression] @scala.reflect.internal.annotations.uncheckedBounds = funparams.updated[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](paramIndex, {
    &lt;artifact&gt; val qual$1: org.apache.spark.sql.qualityFunctions.RefExpression = funparams.apply(paramIndex).asInstanceOf[org.apache.spark.sql.qualityFunctions.RefExpression];
    &lt;artifact&gt; val x$11: org.apache.spark.sql.types.DataType = sumType;
    &lt;artifact&gt; val x$12: Boolean = qual$1.copy$default$2;
    &lt;artifact&gt; val x$13: Int = qual$1.copy$default$3;
    qual$1.copy(x$11, x$12, x$13)
  })(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]);
  &lt;artifact&gt; val x$21: Option[String] @scala.reflect.internal.annotations.uncheckedBounds = i.copy$default$3;
  &lt;artifact&gt; val x$22: Boolean = i.copy$default$4;
  &lt;artifact&gt; val x$23: Boolean = i.copy$default$5;
  &lt;artifact&gt; val x$24: Boolean = i.copy$default$6;
  i.copy(x$20, x$19, x$21, x$22, x$23, x$24)
})(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression with Serializable])
        </td>
      </tr><tr>
        <td>
          166
        </td>
        <td>
          3584
        </td>
        <td>
          7781
          -
          7825
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply(sumType, nullable, paramIndex)
        </td>
      </tr><tr>
        <td>
          166
        </td>
        <td>
          3601
        </td>
        <td>
          7827
          -
          7827
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression with Serializable]
        </td>
      </tr><tr>
        <td>
          167
        </td>
        <td>
          3600
        </td>
        <td>
          7840
          -
          8180
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.copy(x$20, x$19, x$21, x$22, x$23, x$24)
        </td>
      </tr><tr>
        <td>
          167
        </td>
        <td>
          3585
        </td>
        <td>
          7873
          -
          7905
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.cast
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.cast(correctedFunction, sumType)
        </td>
      </tr><tr>
        <td>
          167
        </td>
        <td>
          3597
        </td>
        <td>
          7842
          -
          7842
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.copy$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.copy$default$4
        </td>
      </tr><tr>
        <td>
          167
        </td>
        <td>
          3599
        </td>
        <td>
          7842
          -
          7842
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.copy$default$6
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.copy$default$6
        </td>
      </tr><tr>
        <td>
          167
        </td>
        <td>
          3596
        </td>
        <td>
          7842
          -
          7842
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.copy$default$3
        </td>
      </tr><tr>
        <td>
          167
        </td>
        <td>
          3598
        </td>
        <td>
          7842
          -
          7842
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.copy$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.copy$default$5
        </td>
      </tr><tr>
        <td>
          167
        </td>
        <td>
          3589
        </td>
        <td>
          7858
          -
          8021
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply(org.apache.spark.sql.ShimUtils.cast(correctedFunction, sumType), {
  &lt;synthetic&gt; &lt;artifact&gt; val x$3: org.apache.spark.sql.catalyst.expressions.NamedLambdaVariable = {
    &lt;artifact&gt; val x$14: org.apache.spark.sql.types.DataType = sumType;
    &lt;artifact&gt; val x$15: String = sparam.copy$default$1;
    &lt;artifact&gt; val x$16: Boolean = sparam.copy$default$3;
    &lt;artifact&gt; val x$17: org.apache.spark.sql.catalyst.expressions.ExprId = sparam.copy$default$4;
    &lt;artifact&gt; val x$18: java.util.concurrent.atomic.AtomicReference[Any] @scala.reflect.internal.annotations.uncheckedBounds = sparam.copy$default$5;
    sparam.copy(x$15, x$14, x$16, x$17, x$18)
  };
  params.drop(1).+:[org.apache.spark.sql.catalyst.expressions.NamedExpression, Seq[org.apache.spark.sql.catalyst.expressions.NamedExpression]](x$3)(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.NamedExpression])
}, hidden)
        </td>
      </tr><tr>
        <td>
          168
        </td>
        <td>
          3588
        </td>
        <td>
          7963
          -
          8012
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.SeqLike.+:
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          params.drop(1).+:[org.apache.spark.sql.catalyst.expressions.NamedExpression, Seq[org.apache.spark.sql.catalyst.expressions.NamedExpression]](x$3)(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.NamedExpression])
        </td>
      </tr><tr>
        <td>
          168
        </td>
        <td>
          3587
        </td>
        <td>
          7995
          -
          7995
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.NamedExpression]
        </td>
      </tr><tr>
        <td>
          168
        </td>
        <td>
          3586
        </td>
        <td>
          8010
          -
          8011
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          1
        </td>
      </tr><tr>
        <td>
          169
        </td>
        <td>
          3594
        </td>
        <td>
          8064
          -
          8064
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]
        </td>
      </tr><tr>
        <td>
          169
        </td>
        <td>
          3595
        </td>
        <td>
          8047
          -
          8179
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.SeqLike.updated
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          funparams.updated[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](paramIndex, {
  &lt;artifact&gt; val qual$1: org.apache.spark.sql.qualityFunctions.RefExpression = funparams.apply(paramIndex).asInstanceOf[org.apache.spark.sql.qualityFunctions.RefExpression];
  &lt;artifact&gt; val x$11: org.apache.spark.sql.types.DataType = sumType;
  &lt;artifact&gt; val x$12: Boolean = qual$1.copy$default$2;
  &lt;artifact&gt; val x$13: Int = qual$1.copy$default$3;
  qual$1.copy(x$11, x$12, x$13)
})(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression])
        </td>
      </tr><tr>
        <td>
          170
        </td>
        <td>
          3591
        </td>
        <td>
          8141
          -
          8141
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.copy$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          qual$1.copy$default$2
        </td>
      </tr><tr>
        <td>
          170
        </td>
        <td>
          3590
        </td>
        <td>
          8091
          -
          8140
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.asInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          funparams.apply(paramIndex).asInstanceOf[org.apache.spark.sql.qualityFunctions.RefExpression]
        </td>
      </tr><tr>
        <td>
          170
        </td>
        <td>
          3593
        </td>
        <td>
          8091
          -
          8165
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          qual$1.copy(x$11, x$12, x$13)
        </td>
      </tr><tr>
        <td>
          170
        </td>
        <td>
          3592
        </td>
        <td>
          8141
          -
          8141
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          qual$1.copy$default$3
        </td>
      </tr><tr>
        <td>
          192
        </td>
        <td>
          3606
        </td>
        <td>
          9192
          -
          9192
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$6._3
        </td>
      </tr><tr>
        <td>
          192
        </td>
        <td>
          3605
        </td>
        <td>
          9183
          -
          9183
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$6._2
        </td>
      </tr><tr>
        <td>
          192
        </td>
        <td>
          3604
        </td>
        <td>
          9175
          -
          9175
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$6._1
        </td>
      </tr><tr>
        <td>
          194
        </td>
        <td>
          3609
        </td>
        <td>
          9242
          -
          9297
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.correctEvaluate
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AggregateExpressions.correctEvaluate(ExpressionAggregates.this.sumType, evaluate, true)
        </td>
      </tr><tr>
        <td>
          194
        </td>
        <td>
          3608
        </td>
        <td>
          9292
          -
          9296
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          194
        </td>
        <td>
          3607
        </td>
        <td>
          9258
          -
          9265
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.ExpressionAggregates.sumType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ExpressionAggregates.this.sumType
        </td>
      </tr><tr>
        <td>
          195
        </td>
        <td>
          3612
        </td>
        <td>
          9321
          -
          9370
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.correctSum
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AggregateExpressions.correctSum(ExpressionAggregates.this.sumType, sumWith, true)
        </td>
      </tr><tr>
        <td>
          195
        </td>
        <td>
          3611
        </td>
        <td>
          9365
          -
          9369
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          195
        </td>
        <td>
          3610
        </td>
        <td>
          9332
          -
          9339
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.ExpressionAggregates.sumType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ExpressionAggregates.this.sumType
        </td>
      </tr><tr>
        <td>
          197
        </td>
        <td>
          3615
        </td>
        <td>
          9422
          -
          9422
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple5._3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$7._3
        </td>
      </tr><tr>
        <td>
          197
        </td>
        <td>
          3614
        </td>
        <td>
          9416
          -
          9416
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple5._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$7._2
        </td>
      </tr><tr>
        <td>
          197
        </td>
        <td>
          3617
        </td>
        <td>
          9442
          -
          9442
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple5._5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$7._5
        </td>
      </tr><tr>
        <td>
          197
        </td>
        <td>
          3613
        </td>
        <td>
          9389
          -
          9389
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple5._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$7._1
        </td>
      </tr><tr>
        <td>
          197
        </td>
        <td>
          3616
        </td>
        <td>
          9434
          -
          9434
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple5._4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$7._4
        </td>
      </tr><tr>
        <td>
          205
        </td>
        <td>
          3618
        </td>
        <td>
          9656
          -
          9707
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](count, sum1, sum2, ifExpr, useSum, useEvaluate)
        </td>
      </tr><tr>
        <td>
          246
        </td>
        <td>
          3619
        </td>
        <td>
          11193
          -
          11210
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ExpressionAggregates.this.evaluate.dataType
        </td>
      </tr><tr>
        <td>
          248
        </td>
        <td>
          3620
        </td>
        <td>
          11247
          -
          11252
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          false
        </td>
      </tr><tr>
        <td>
          252
        </td>
        <td>
          3621
        </td>
        <td>
          11412
          -
          11413
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.ExpressionAggregates.RichAttribute.a
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RichAttribute.this.a
        </td>
      </tr><tr>
        <td>
          255
        </td>
        <td>
          3623
        </td>
        <td>
          11544
          -
          11605
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ExpressionAggregates.this.inputAggBufferAttributes.apply(ExpressionAggregates.this.aggBufferAttributes.indexOf[org.apache.spark.sql.catalyst.expressions.Expression](RichAttribute.this.a.from))
        </td>
      </tr><tr>
        <td>
          255
        </td>
        <td>
          3622
        </td>
        <td>
          11569
          -
          11604
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.GenSeqLike.indexOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ExpressionAggregates.this.aggBufferAttributes.indexOf[org.apache.spark.sql.catalyst.expressions.Expression](RichAttribute.this.a.from)
        </td>
      </tr><tr>
        <td>
          259
        </td>
        <td>
          3627
        </td>
        <td>
          11706
          -
          11734
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.ExpressionAggregates.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ExpressionAggregates.this.copy(newChildren, ExpressionAggregates.this.copy$default$2, ExpressionAggregates.this.copy$default$3, ExpressionAggregates.this.copy$default$4)
        </td>
      </tr><tr>
        <td>
          259
        </td>
        <td>
          3624
        </td>
        <td>
          11706
          -
          11706
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.ExpressionAggregates.copy$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ExpressionAggregates.this.copy$default$2
        </td>
      </tr><tr>
        <td>
          259
        </td>
        <td>
          3626
        </td>
        <td>
          11706
          -
          11706
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.ExpressionAggregates.copy$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ExpressionAggregates.this.copy$default$4
        </td>
      </tr><tr>
        <td>
          259
        </td>
        <td>
          3625
        </td>
        <td>
          11706
          -
          11706
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.ExpressionAggregates.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ExpressionAggregates.this.copy$default$3
        </td>
      </tr>
    </table>
          </div>
        </div>
      </body>
    </html>