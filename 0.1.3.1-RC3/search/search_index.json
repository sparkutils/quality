{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Quality - 0.1.3.1-RC3","text":"Coverage<p> Statement 95.04 Branch 96.15 </p>"},{"location":"#run-complex-data-quality-rules-using-simple-sql-in-a-batch-or-streaming-spark-application-at-scale","title":"Run complex data quality rules using simple SQL in a batch or streaming Spark application at scale.","text":"<p>Write rules using simple SQL or create re-usable functions via SQL Lambdas.</p> <p>Your rules are just versioned data, store them wherever convenient, use them by simply defining a column.</p> <ul> <li> test packages for Fabric</li> </ul> <p>Rules are evaluated lazily during Spark actions, such as writing a row, with results saved in a single predictable column.</p>"},{"location":"#enhanced-spark-functionality","title":"Enhanced Spark Functionality","text":"<p>Lookup Functions are distributed across the Spark cluster and held in memory, as such no shuffling is required where the shuffling introduced by joins may be too expensive:</p> <ul> <li>Support for massive Bloom Filters while retaining FPP (i.e. several billion items at 0.001 would not fit into a normal 2gb byte array)</li> <li>Map lookup expressions for exact lookups and contains tests, using broadcast variables under the hood they are a great fit for small reference data sets</li> <li> <p>View loading - manage the use of session views in your application through configuration and a pluggable DataFrameLoader </p> </li> <li> <p>Lambda Functions - user provided re-usable sql functions over late bound columns</p> </li> <li> <p>Fast PRNG's exposing RandomSource allowing pluggable and stable generation across the cluster</p> </li> <li> <p>Aggregate functions over Maps expandable with simple SQL Lambdas</p> </li> <li> <p>Row ID expressions including guaranteed unique row IDs (based on MAC address guarantees)</p> </li> </ul> <p>Plus a collection of handy functions to integrate it all.</p>"},{"location":"sqlfunctions/","title":"SQL Functions Documentation","text":""},{"location":"sqlfunctions/#_","title":"_","text":"<p>_( [ddl type], [nullable] ) provides PlaceHolders for lambda functions to allow partial application, use them in place of actual values or expressions to either change arity or allow use in _lambda_.</p> <p>The default type is Long / Bigint, you will have to provide the types directly when using something else.  By default the placeholders are assumed to be nullable (i.e. true), you can use false to state the field should not be null.</p>"},{"location":"sqlfunctions/#_lambda_","title":"_lambda_","text":"<p>_lambda_( user function ) extracts the Spark LambdaFunction from a resolved user function, this must have the correct types expected by the Spark HigherOrderFunction they are parameters for.</p> <p>This allows using user defined functions and lambdas with in-built Spark HigherOrderFunctions</p>"},{"location":"sqlfunctions/#agg_expr","title":"agg_Expr","text":"<p>agg_Expr( [ddl sum type], filter, sum, result) aggregates on rows which match the filter expression using the sum expression to aggregate then processes the results using the result expression.</p> <p>You can run multiple agg_Expr's in a single pass select, use the first parameter to thread DDL type information through to the sum and result functions.</p>"},{"location":"sqlfunctions/#as_uuid","title":"as_uuid","text":"<p>as_uuid(lower_long, higher_long) converts two longs into a uuid. Note: this is not functionally equivalent to rng_uuid(longPair(lower, higher)) despite having the same types.</p>"},{"location":"sqlfunctions/#big_bloom","title":"big_Bloom","text":"<p>big_Bloom(buildFrom, expectedSize, expectedFPP) creates an aggregated bloom filter using the buildFrom expression.  </p> <p>The blooms are stored on a shared filesystem using a generated uuid, they can scale to high numbers of items whilst keeping the FPP (e.g. millions at 0.01 would imply 99% probability, you may have to cast to double in Spark 3.2).</p> <p>buildFrom can be driven by digestToLongs or hashWith functions when using multiple fields.</p> <p>Alternatives:</p>  big_Bloom(buildFrom, expectedSize, expectedFPP, 'bloom_loc') - per above but uses a fixed string bloom_loc instead of a uuid"},{"location":"sqlfunctions/#callfun","title":"callFun","text":"<p>callFun( user function lambda variable, param1, param2, \u2026 paramN ) used within a lambda function it allows calling a lambda variable that contains a user function.</p> <p>Used from the top level sql it performs a similar function expecting either a full user function or a partially applied function, typically returned from another lambda user function.</p>"},{"location":"sqlfunctions/#coalesce_if_attributes_missing","title":"coalesce_If_Attributes_Missing","text":"<p>coalesce_If_Attributes_Missing(expr, replaceWith) substitutes expr with the replaceWith expression when expr has missing attributes in the source dataframe.    Your code must call the scala processIfAttributeMissing function before using in validate or ruleEngineRunner/ruleRunner:</p> <pre><code>val missingAttributesAreReplacedRS = processIfAttributeMissing(rs, struct)\n\nval (errors, _) = validate(struct, missingAttributesAreReplacedRS)\n\n// use it missingAttributesAreReplacedRS in your dataframe..\n</code></pre>"},{"location":"sqlfunctions/#coalesce_if_attributes_missing_disable","title":"coalesce_If_Attributes_Missing_Disable","text":"<p>coalesce_If_Attributes_Missing_Disable(expr) substitutes expr with the DisabledRule Integer result (-2) when expr has missing attributes in the source dataframe.  Your code must call the scala processIfAttributeMissing function before using in validate or ruleEngineRunner/ruleRunner:</p> <pre><code>val missingAttributesAreReplacedRS = processIfAttributeMissing(rs, struct)\n\nval (errors, _) = validate(struct, missingAttributesAreReplacedRS)\n\n// use it missingAttributesAreReplacedRS in your dataframe..\n</code></pre>"},{"location":"sqlfunctions/#comparable_maps","title":"comparable_Maps","text":"<p>comparable_Maps(struct | array | map) converts any maps in the input param into sorted arrays of a key, value struct.</p> <p>This allows developers to perform sorts, distincts, group bys and union set operations with Maps, currently not supported by Spark sql as of 3.4.</p> <p>The sorting behaviour uses Sparks existing odering logic but allows for extension during the calls to the registerQualityFunctions via the mapCompare parameter and the defaultMapCompare function.</p>"},{"location":"sqlfunctions/#digest_to_longs","title":"digest_To_Longs","text":"<p>digest_To_Longs('digestImpl', fields*) creates an array of longs based on creating the given MessageDigest impl.  A 128-bit impl will generate two longs from it's digest</p>"},{"location":"sqlfunctions/#digest_to_longs_struct","title":"digest_To_Longs_Struct","text":"<p>digest_To_Longs_Struct('digestImpl', fields*) creates structure of longs with i0 to iN named fields based on creating the given MessageDigest impl.</p>"},{"location":"sqlfunctions/#disabled_rule","title":"disabled_Rule","text":"<p>disabledRule() returns the DisabledRule Integer result (-2) for use in filtering and to disable rules (which may not signify a version bump)</p>"},{"location":"sqlfunctions/#drop_field","title":"drop_field","text":"<p>drop_field(structure_expr, 'field.subfield'*) removes fields from a structure, but will not remove parent nodes. </p> <p>This is a wrapped version of 3.4.1's dropField implementation.</p>"},{"location":"sqlfunctions/#failed","title":"failed","text":"<p>failed() returns the Failed Integer result (0) for use in filtering</p>"},{"location":"sqlfunctions/#field_based_id","title":"field_Based_ID","text":"<p>field_Based_ID('prefix', 'digestImpl', fields*) creates a variable bit length id by using a given MessageDigest impl over the fields, prefix is used with the _base, _i0 and _iN fields in the resulting structure</p>"},{"location":"sqlfunctions/#flatten_results","title":"flatten_Results","text":"<p>flatten_Results(dataQualityExpr) expands data quality results into a flat array</p>"},{"location":"sqlfunctions/#flatten_rule_results","title":"flatten_Rule_Results","text":"<p>flatten_Rule_Results(dataQualityExpr) expands data quality results into a structure of flattenedResults, salientRule (the one used to create the output) and the rule result.</p> <p>salientRule will be null if there was no matching rule</p>"},{"location":"sqlfunctions/#from_yaml","title":"from_yaml","text":"<p>from_yaml(string, 'ddlType') uses snakeyaml to convert yaml into Spark datatypes   </p>"},{"location":"sqlfunctions/#hash_field_based_id","title":"hash_Field_Based_ID","text":"<p>hash_Field_Based_ID('prefix', 'digestImpl', fields*) creates a variable bit length id by using a given Guava Hasher impl over the fields, prefix is used with the _base, _i0 and _iN fields in the resulting structure</p>"},{"location":"sqlfunctions/#hash_with","title":"hash_With","text":"<p>hash_With('HASH', fields*) Generates a hash value (array of longs) suitable for using in blooms based on the given Guava hash implementation.</p> <p>Note based on testing the digestToLongs function for SHA256 and MD5 are faster.  </p> <p>Valid hashes: MURMUR3_32, MURMUR3_128, MD5, SHA-1, SHA-256, SHA-512, ADLER32, CRC32, SIPHASH24. When an invalid HASH name is provided MURMUR3_128 will be chosen.</p> Open source Spark 3.1.2/3 issues<p>On Spark 3.1.2/3 open source this may get resolver errors due to a downgrade on guava version - 15.0 is used on Databricks, open source 3.0.3 uses 16.0.1, 3.1.2 drops this to 11 and misses crc32, sipHash24 and adler32.</p>"},{"location":"sqlfunctions/#hash_with_struct","title":"hash_With_Struct","text":"<p>per hash_With('HASH', fields*) but generates a struct with i0 to ix named longs.  This structure is not suitable for blooms</p>"},{"location":"sqlfunctions/#id_base64","title":"id_base64","text":"<p>id_base64(base, i0, i1, ix) Generates a base64 encoded representation of the id, either the single struct field or the individual parts</p> <p>Alternatives:</p>  id_base64(id_struct) Uses an id field to generate"},{"location":"sqlfunctions/#id_equal","title":"id_Equal","text":"<p>id_Equal(leftPrefix, rightPrefix) takes two prefixes which will be used to match leftPrefix_base = rightPrefix_base, i0 and i1 fields.  It does not currently support more than two i's </p>"},{"location":"sqlfunctions/#id_from_base64","title":"id_from_base64","text":"<p>id_from_base64(base64) Parses the base64 string with an expected default long size of two i.e. an 160bit ID, any string which is not of the correct size will return null</p> <p>Alternatives:</p>  id_from_base64(base64f, size) Uses a size, which must be literal, to specify the type"},{"location":"sqlfunctions/#id_raw_type","title":"id_raw_type","text":"<p>id_raw_type(idstruct) Given a prefixed id returns the fields without their prefix</p>"},{"location":"sqlfunctions/#id_size","title":"id_size","text":"<p>id_size(base64) Given a base64 from id_base64 returns the number of _i long fields</p>"},{"location":"sqlfunctions/#inc","title":"inc","text":"<p>inc() increments the current sum by 1</p> <p>Alternatives:</p>  inc( x ) use an expression of type Long to increment"},{"location":"sqlfunctions/#long_pair","title":"long_Pair","text":"<p>long_Pair(lower, higher) creates a structure with these lower and higher longs</p>"},{"location":"sqlfunctions/#long_pair_equal","title":"long_Pair_Equal","text":"<p>long_Pair_Equal(leftPrefix, rightPrefix) takes two prefixes which will be used to match leftPrefix_lower = rightPrefix_lower and leftPrefix_higher = rightPrefix_higher</p>"},{"location":"sqlfunctions/#long_pair_from_uuid","title":"long_Pair_From_UUID","text":"<p>long_Pair_From_UUID(expr) converts a UUID to a structure with lower and higher longs</p>"},{"location":"sqlfunctions/#map_contains","title":"map_Contains","text":"<p>map_Contains('mapid', expr) returns true if there is an item in the map</p>"},{"location":"sqlfunctions/#map_lookup","title":"map_Lookup","text":"<p>map_Lookup('mapid', expr) returns either the lookup in map specified by mapid or null</p>"},{"location":"sqlfunctions/#meanf","title":"meanF","text":"<p>meanF() simple mean on the results, expecting sum and count type Long</p>"},{"location":"sqlfunctions/#murmur3_id","title":"murmur3_ID","text":"<p>murmur3ID('prefix', fields*) Generates a 160bit id using murmer3 hashing over input fields, prefix is used with the _base, _i0 and _i1 fields in the resulting structure</p>"},{"location":"sqlfunctions/#pack_ints","title":"pack_Ints","text":"<p>pack_Ints(lower, higher) a packaged long from two ints, used within result compression</p>"},{"location":"sqlfunctions/#passed","title":"passed","text":"<p>passed() returns the Passed Integer for use in filtering: 10000</p>"},{"location":"sqlfunctions/#prefixed_to_long_pair","title":"prefixed_To_Long_Pair","text":"<p>prefixed_To_Long_Pair(field, 'prefix') converts a 128bit longpair field with the given prefix into a higher and lower long pair without prefix.</p> <p>This is suitable for converting provided id's into uuids for example via a further call to rngUUID.   </p>"},{"location":"sqlfunctions/#print_code","title":"print_Code","text":"<p>print_Code( [msg], expr ) prints the code generated by an expression, the value variable and the isNull variable and forwards eval calls / type etc. to the expression.</p> <p>The code is printed once per partition on the executors std. output.  You will have to check each executor to find the used nodes output.  To use with unit testing on a single host you may overwrite the writer function in registerQualityFunctions,  you should however use a top level object and var to write into (or stream), printCode will not be able to write to std out properly (spark redirects / captures stdout) or non top level objects (due to classloader / function instance issues).  Testing on other hosts without using stdout should do so to a shared file location or similar.</p> <p>!!! \"information\" It is not compatible with every expression     Aggregate expressions like aggExpr or sum etc. won't generate code so they aren't compatible with printCode.</p> <pre><code>\\_lambda\\_ is also incompatible with printCode both wrapping a user function and the \\_lambda\\_ function.  Similarly the \\_() placeholder function cannot be wrapped.\n\nAny function expecting a specific signature like aggExpr or other HigherOrderFunctions like aggregate or filter are unlikely to support wrapped arguements.\n</code></pre>"},{"location":"sqlfunctions/#print_expr","title":"print_Expr","text":"<p>print_Expr( [msg], expr ) prints the expression tree via toString with an optional msg </p> <p>The message is printed to the driver nodes std. output, often shown in notebooks as well.  To use with unit testing you may overwrite the writer function in registerQualityFunctions,  you should however use a top level object and var to write into (or stream).</p>"},{"location":"sqlfunctions/#probability","title":"probability","text":"<p>probability(expr) will translate probability rule results into a double, e.g. 1000 returns 0.01. This is useful for interpreting and filtering on probability based results: 0 -&gt; 10000 non-inclusive</p>"},{"location":"sqlfunctions/#probability_in","title":"probability_In","text":"<p>probability_In(expr, 'bloomid') returns the probability of the expr being in the bloomfilter specified by bloomid.  </p> <p>This function either returns 0.0, where it is definitely not present, or the original FPP where it may be present.</p> <p>You may use digestToLongs or hashWith as appropriate to use multiple columns safely.</p>"},{"location":"sqlfunctions/#provided_id","title":"provided_ID","text":"<p>provided_ID('prefix', existingLongs) creates an id for an existing array of longs, prefix is used with the _base, _i0 and _iN fields in the resulting structure</p>"},{"location":"sqlfunctions/#results_with","title":"results_With","text":"<p>results_With( x ) process results lambda x (e.g. (sum, count) -&gt; sum ) that takes sum from the aggregate, count from the number of rows counted.  Defaults both the sumtype and counttype as LongType</p> <p>Alternatives:</p>  results_With( [sum ddl type], x) Use the given ddl type for the sum type e.g. 'MAP&lt;STRING, DOUBLE&gt;'   results_With( [sum ddl type], [result ddl type], x) Use the given ddl type for the sum and result types"},{"location":"sqlfunctions/#return_sum","title":"return_Sum","text":"<p>return_Sum( sum type ddl ) just returns the sum and ignores the count param, expands to resultsWith( [sum ddl_type], (sum, count) -&gt; sum)</p>"},{"location":"sqlfunctions/#reverse_comparable_maps","title":"reverse_Comparable_Maps","text":"<p>reverses a call to comparableMaps</p>"},{"location":"sqlfunctions/#rng","title":"rng","text":"<p>rng() Generates a 128bit random id using XO_RO_SHI_RO_128_PP, encoded as a lower and higher long pair</p> <p>Alternatives:</p>  rng('algorithm') Uses Commons RNG RandomSource to implement the RNG   rng('algorithm', seedL) Uses Commons RNG RandomSource to implement the RNG with a long seed"},{"location":"sqlfunctions/#rng_bytes","title":"rng_Bytes","text":"<p>rng_Bytes() Generates a 128bit random id using XO_RO_SHI_RO_128_PP, encoded as a byte array</p> <p>Alternatives:</p>  rng_Bytes('algorithm') Uses Commons RNG RandomSource to implement the RNG   rng_Bytes('algorithm', seedL) Uses Commons RNG RandomSource to implement the RNG with a long seed   rng_Bytes('algorithm', seedL, byteCount) Uses Commons RNG RandomSource to implement the RNG with a long seed, with a specific byte length integer (e.g. 16 is two longs, 8 is integer)"},{"location":"sqlfunctions/#rng_id","title":"rng_ID","text":"<p>rng_ID('prefix') Generates a 160bit random id using XO_RO_SHI_RO_128_PP, prefix is used with the _base, _i0 and _i1 fields in the resulting structure</p> <p>Alternatives:</p>  rng_Id('prefix', 'algorithm') Uses Commons RNG RandomSource to implement the RNG, using other algorithm's may generate more long _iN fields   rng_Id('prefix', 'algorithm', seedL) Uses Commons RNG RandomSource to implement the RNG with a long seed, using other algorithm's may generate more long _iN fields"},{"location":"sqlfunctions/#rng_uuid","title":"rng_UUID","text":"<p>rng_UUID(expr) takes either a structure with lower and higher longs or a 128bit binary type and converts to a string uuid - use with, for example, the rng() function.</p> <p>If a simple conversion from two longs (lower, higher) to a uuid is desired then use as_uuid, rng_uuid applies the same transformations as the Spark uuid to the input higher and lower longs.</p>"},{"location":"sqlfunctions/#rule_result","title":"rule_result","text":"<p>rule_result(ruleSuiteResultColumn, packedRuleSuiteId, packedRuleSetId, packedRuleId) uses the packed long id's to retrieve the integer ruleResult (see below for ExpressionRunner) or null if it can't be found.  </p> <p>You can use pack_ints(id, version) to specify each id if you don't already have the packed long version.  This is suitable for retrieving individual rule results, for example to aggregate counts of a specific rule result, without having to resort to using filter and map values.</p> <p>rule_result works with ruleRunner (DQ) results (including details) and ExpressionRunner results.  ExpressionRunner results return a tuple of ruleResult and resultDDL, both strings, or if strip_result_ddl is called a string.</p>"},{"location":"sqlfunctions/#rule_suite_result_details","title":"rule_Suite_Result_Details","text":"<p>rule_Suite_Result_Details(dq) strips the overallResult from the dataquality results, suitable for keeping overall result as a top-level field with associated performance improvements</p>"},{"location":"sqlfunctions/#small_bloom","title":"small_Bloom","text":"<p>small_Bloom(buildFrom, expectedSize, expectedFPP) creates a simply bytearray bloom filter using the expected size and fpp - 0.01 is 99%, you may have to cast to double in Spark 3.2. buildFrom can be driven by digestToLongs or hashWith functions when using multiple fields.</p>"},{"location":"sqlfunctions/#soft_fail","title":"soft_Fail","text":"<p>soft_Fail(ruleexpr) will treat any rule failure (e.g. failed() ) as returning softFailed()</p>"},{"location":"sqlfunctions/#soft_failed","title":"soft_Failed","text":"<p>soft_Failed() returns the SoftFailed Integer result (-1) for use in filtering</p>"},{"location":"sqlfunctions/#strip_result_ddl","title":"strip_result_ddl","text":"<p>strip_result_ddl(expressionsResult) removes the resultDDL field from expressionsRunner results, leaving only the string result itself for more compact storage </p>"},{"location":"sqlfunctions/#sum_with","title":"sum_With","text":"<p>sum_With( x ) adds expression x for each row processed in an aggExpr with a default of LongType</p> <p>Alternatives:</p>  sum_With( [ddl type], x) Use the given ddl type e.g. 'MAP&lt;STRING, DOUBLE&gt;'"},{"location":"sqlfunctions/#to_yaml","title":"to_yaml","text":"<p>to_yaml(expression, [options map]) uses snakeyaml to convert Spark datatypes into yaml.</p> <p>Passing null into the function returns a null yaml (newline is appended):</p> <pre><code>null\n</code></pre> <p>All null values will be treated in this fashion.  The string \"null\" will be represented as (again new line is present):</p> <pre><code>'null'\n</code></pre> <p>The optional \"options map\" parameter currently supports the following output options:</p> <ul> <li>useFullScalarType, defaults to false.  Instead of using the default yaml tags uses the full classnames for scalars, reducing risk of precision loss if the yaml is to be used outside of the from_yaml function.</li> </ul> <p>sample usage:</p> <pre><code>val df = sparkSession.sql(\"select array(1,2,3,4,5) og\")\n.selectExpr(\"*\", \"to_yaml(og, map('useFullScalarType', 'true')) y\")\n.selectExpr(\"*\", \"from_yaml(y, 'array&lt;int&gt;') f\")\n.filter(\"f == og\")\n</code></pre> <p>snakeyaml is provided scope</p> <p>Databricks runtimes provide sparkyaml, so whilst Quality builds against the correct versions for Databricks it can onyl use provided scope.</p> <p>snakeyaml is 1.24 on DBRs below 13.1, but not present on OSS, so you may need to add the dependency yourself, tested compatible versions are 1.24 and 1.33. </p>"},{"location":"sqlfunctions/#unique_id","title":"unique_ID","text":"<p>uniqueID('prefix') Generates a 160bit guaranteed unique id (requires MAC address uniqueness) with contiguous higher values within a partition and overflow with timestamp ms., prefix is used with the _base, _i0 and _i1 fields in the resulting structure</p>"},{"location":"sqlfunctions/#unpack","title":"unpack","text":"<p>unpack(expr) takes a packed rule long and unpacks it to a .id and .version structure</p>"},{"location":"sqlfunctions/#unpack_id_triple","title":"unpack_Id_Triple","text":"<p>unpack_Id_Triple(expr) takes a packed rule triple of longs (ruleSuiteId, ruleSetId and ruleId) and unpacks it to (ruleSuiteId, ruleSuiteVersion, ruleSetId, ruleSetVersion, ruleId, ruleVersion)</p>"},{"location":"sqlfunctions/#update_field","title":"update_field","text":"<p>update_field(structure_expr, 'field.subfield', replaceWith, 'fieldN', replaceWithN) processes structures allowing you to replace sub items (think lens in functional programming) using the structure fields path name.</p> <p>This is a wrapped version of 3.4.1's withField implementation.</p>"},{"location":"sqlfunctions/#za_field_based_id","title":"za_Field_Based_ID","text":"<p>za_Field_Based_ID('prefix', 'digestImpl', fields*) creates a 64bit id (96bit including header) by using a given Zero Allocation impl over the fields, prefix is used with the _base and _i0 fields in the resulting structure.</p> <p>Prefer using the zaLongsFieldBasedID for less collisions</p>"},{"location":"sqlfunctions/#za_hash_longs_with","title":"za_Hash_Longs_With","text":"<p>za_Hash_Longs_With('HASH', fields*) generates a multi length long array but with a zero allocation implementation.  This structure is suitable for blooms, the default XXH3 algorithm is the 128bit version of that used by the internal bigBloom implementation.</p> <p>Available HASH functions are MURMUR3_128, XXH3</p>"},{"location":"sqlfunctions/#za_hash_longs_with_struct","title":"za_Hash_Longs_With_Struct","text":"<p>similar to za_Hash_Longs_With('HASH', fields*) but generates an ID relevant multi length long struct, which is not suitable for blooms</p>"},{"location":"sqlfunctions/#za_hash_with","title":"za_Hash_With","text":"<p>za_Hash_With('HASH', fields*) generates a single length long array always with 64 bits but with a zero allocation implementation.  This structure is suitable for blooms, the default XX algorithm is used by the internal bigBloom implementation.</p> <p>Available HASH functions are MURMUR3_64, CITY_1_1, FARMNA, FARMOU, METRO, WY_V3, XX</p>"},{"location":"sqlfunctions/#za_hash_with_struct","title":"za_Hash_With_Struct","text":"<p>similar to za_Hash_With('HASH', fields*) but generates an ID relevant multi length long struct (of one long), which is not suitable for blooms.</p> <p>Prefer zaHashLongsWithStruct for reduced collisions with either the MURMUR3_128 or XXH3 versions of hashes</p>"},{"location":"sqlfunctions/#za_longs_field_based_id","title":"za_Longs_Field_Based_ID","text":"<p>za_Longs_Field_Based_ID('prefix', 'digestImpl', fields*) creates a variable length id by using a given Zero Allocation impl over the fields, prefix is used with the _base, _i0 and _iN fields in the resulting structure. Murmur3_128 is faster than on the Guava implementation.</p>"},{"location":"advanced/aggregations/","title":"Aggregation Functions","text":""},{"location":"advanced/aggregations/#aggregation-functions","title":"Aggregation Functions","text":"<p>Quality adds a number of aggregation primitives to allow building across dataset functionality similar to Deequ and others but philosophically staying true to the customisation approach used throughout the library.</p> <p>You can aggregate using any number of agg_expr columns:</p> <ul> <li>agg_expr(ddl type, filter, lambda sum, lambda result ) - allows filter expressions to be used to build up aggregated BIGINT (long) results with lambda functions, leveraging simple lambda functions (note count is currently only BIGINT / LongType): <pre><code>// generates with an long id column from 1 to 20\nval df = sparkSession.range(1, 20)\n// filter odd numbers, add the them together with sumWith lambda for the \n// sum, then using resultsWith lambda variables divide them by the count \n// of filtered rows\nval res = df.select(expr(\"agg_expr('BIGINT', id % 2 &gt; 0, sum_with(sum -&gt; sum + id), \"+\n\"results_with( (sum, count) -&gt; sum / count ) )\").as(\"aggExpr\"))\nres.show() // will show aggExpr with 10.0 as a result, \n// sum + count would show 110..\n</code></pre></li> </ul> <p>The filter parameter lets you select rows you care about to aggregate, but does not stop you aggregating different filters in different columns and still process all columns in a single pass.  The sum function itself does the aggregation and finally the result function yields the last calculated result.  Both of these functions operate on MAPs of any key and value type.</p> <p>Spark lambda functions are incompatible with aggregation wrt. type inference which requires that the type is specified to agg_expr, it defaults to bigint when not specified.</p> <p>The ExpressionRunner provides a convenient way to manage multiple agg_expr aggregations in a single pass action via a RuleSuite, just like DQ rules. </p>"},{"location":"advanced/aggregations/#aggregation-lambda-functions","title":"Aggregation Lambda Functions","text":"<ul> <li>sum_with( lambda entry -&gt; entry ) - processes for each matched row the lambda with the given ddl type which defaults to LongType </li> <li>results_with( lambda (sum, count) -&gt; ex ) - process results lambda with sum and count types passed in.</li> <li>inc( [expr] ) - increments the current sum either by default 1 or by expr using type LongType </li> <li>meanF() - simple mean on the results, expecting sum and count type Long: <pre><code>// generates with an long id column from 1 to 20\nval df = sparkSession.range(1, 20)\n// filter odd numbers, add the them together with inc lambda for the sum, then using meanF expression to divide them by the count of filtered rows\nval res = df.select(expr(\"agg_expr(id % 2 &gt; 0, inc(id), meanF() )\").as(\"aggExpr\"))\nres.show() // will show aggExpr with 10.0 as a result, sum + count would show 110..\n</code></pre></li> <li>map_with( keyExpr, x ) - uses a map to group via keyExpr and apply x to each element: <pre><code>// a counting example expr - group by and count distinct equivalent\nexpr(\"agg_expr('MAP&lt;STRING, LONG&gt;', 1 &gt; 0, map_with(date || ', ' || product, entry -&gt; entry + 1 ), results_with( (sum, count) -&gt; sum ) )\").as(\"mapCountExpr\")\n// a summing example expr with embedded if's in the summing lambda for added fun\nexpr(\"agg_expr('MAP&lt;STRING, DOUBLE&gt;', 1 &gt; 0, map_with(date || ', ' || product, entry -&gt; entry + IF(ccy='CHF', value, value * ccyrate)  ), return_sum() )\").as(\"mapSumExpr\")\n</code></pre></li> <li>return_sum( ) - just returns the sum and ignores the count param, expands to results_with( (sum, count) -&gt; sum)</li> </ul>"},{"location":"advanced/aggregations/#column-dsl","title":"Column DSL","text":"<p>The same functionality is available in the functions package e.g.:</p> <pre><code>import com.sparkutils.quality.functions._\nval df: DataFrame = ...\ndf.select(agg_expr(DecimalType(38,18), df(\"dec\").isNotNull, sum_with(entry =&gt; df(\"dec\") + entry ), return_sum) as \"agg\")\n</code></pre>"},{"location":"advanced/aggregations/#type-lookup-and-monoidal-merging","title":"Type Lookup and Monoidal Merging","text":"<p>This section is very advanced but may be needed in a deeply nested type is to be aggregated.</p>"},{"location":"advanced/aggregations/#type-lookup","title":"Type Lookup","text":"<p>agg_expr, map_with, sum_with and return_sum all rely on type lookup.  The implementation uses sparks in-built DDL parsing to get types, but can be extended by supplying a custom function when registering functions e.g.: <pre><code>registerQualityFunctions(parseTypes = (str: String) =&gt; defaultParseTypes(str).orElse( logic goes here ) /* Option[DataType] */)\n</code></pre></p>"},{"location":"advanced/aggregations/#monoidal-merging","title":"Monoidal Merging","text":"<p>Unlike type lookup custom merging could well be required for special types.  Aggregation (as well as MapMerging and MapTransform) require a Zero value the defaultZero function can be extended or overwritten and passed into registerFunctions as per parseTypes. The defaultAdd function uses itself with an extension function parameter in order to supply map value monoidal associative add.  </p> <p>Note</p> <p>This works great for Maps and default numeric types but it requires custom monoidal 'add' functions to be provided for merging complex types.</p> <p>Whilst zero returns a value to use as zero you may need to recurse for nested structures of zero, add requires defining Expressions and takes a left and right Expression to perform it:</p> <pre><code>DataType =&gt; Option[( Expression, Expression ) =&gt; Expression]\n</code></pre> <p>Warning</p> <p>This is an area of functionality you should avoid unless needed as it often requires deep knowledge of Spark internals.  There be dragons.</p>"},{"location":"advanced/blooms/","title":"Bloom Filters","text":"<p>Bloom Filters are probabalistic data structures that, for a given number of items and a false positive probability (FPP) provides a mightContain function.  This function guarantees that if an item is not in the bloom filter it will return false, however if it returns true this is to a probability defined by the FPP value.</p> <p>In contrast to a Set which requires the items (or at least their hash values) to be stored individually blooms make use of multiple blocks and apply bit setting based on hashes of the input value over some function.  These resulting blocks and bitsets are far smaller in memory and storage usage than a typical set.  For example it's possible to store hundreds of millions of items within a bloom and still keep within a normal Java byte array boundary.</p> <p>This act of using bit flipping also allows blooms to be or'd for the same size and FPP, which is great for aggregation functions in Spark.</p> <p>Whilst blooms are great the guarantees break when:</p> <ol> <li>The number of items far exceeds the initial size used to create the bloom - false is still guaranteed to not be present but the true value will no longer represent FPP, the bloom has degraded</li> <li>The number of bits required to store the initial number of items at the FPP exceed what can be represented by the bloom algorithm.  </li> </ol> <p>If you attempt to store billions of items within a bloom at a high FPP you will quickly fall foul of 2, and this is easily done with both the Spark stats.package and the current bloom filters on Databricks.  This makes them next to useless for large dataset lookups on typical bloom implementations.</p>"},{"location":"advanced/blooms/#how-does-quality-change-this","title":"How does Quality change this?","text":"<p>It can't change the fundamental laws of bloom filters, if you use the number of bits up your bloom filter is next to useless.  You can however add multiple Java byte arrays and bucket the hashes across them.  This works great up to about 1.5b items in a typical aggregation function within Spark, however Spark only allows a maximum of 2Gb for an InternalRow - of which aggregates are stored in.</p> <p>Quality provides three bloom implementations the Spark stats package, small - which buckets within an InternalRow (1.2-1.5b items max whilst maintaining FPP) - and big which doesn't use Spark aggregations to store the results of aggregations but rather a shared file system such as Databricks dbfs.</p> <p>Both the small and big bloom functions use Parquet's bloom filter implementation which both significantly faster and has better statistical properties than Sparks/Guavas or Breezes.</p>"},{"location":"advanced/blooms/#what-are-bloom-maps","title":"What are Bloom Maps?","text":"<p>Bloom Maps are identifiers to a bloom filter.  The examples below show how to create the key is to use the SparkBloomFilter or bloomFilter functions to provide the value and the FPP is required.</p> <pre><code>registerBloomMapAndFunction(bloomFilterMap)\n</code></pre> <p>Both registers the Bloom Map, the small_bloom and big_bloom aggregation functions and the probabilityIn function.</p>"},{"location":"advanced/blooms/#using-the-spark-stats-package","title":"Using the Spark stats package","text":"<pre><code>// generate a dataframe with an id column\nval df = sqlContext.range(1, 20)\n// build a bloomfilter over the id's\nval bloom = df.stat.bloomFilter(\"id\", 20, 0.01)\n// get the fpp and build the map\nval fpp = 1.0 - bloom.expectedFpp()\nval bloomFilterMap = SparkSession.active.sparkContext.broadcast( Map(\"ids\" -&gt; (SparkBloomFilter(bloom), fpp)) )\n\n// register the map for this SparkSession\nregisterBloomMapAndFunction(bloomFilterMap)\n// lookup the result of adding column's a and b against that bloom filter for each row\notherSourceDF.withColumn(\"probabilityInIds\", expr(\"probability_in(a + b, 'ids')\"))\n</code></pre> <p>The stats package bloomFilter function has severe limitations on a single field and does not allow expressions but through the SparkBloomFilter lookup function is integrated with Quality anyway.</p>"},{"location":"advanced/blooms/#using-the-quality-bloom-filters","title":"Using the Quality bloom filters","text":"<p>The small and big bloom functions take a single expression parameter however it can be built from any number of fields or field types using the hash_with function.</p> <ul> <li>smallBloom( column, expected number of items, fpp ) - an SQL aggregate function which generates a BloomFilter Array[Byte] for use in probabilityIn or rowId: <pre><code> val aggrow = orig.select(expr(s\"small_bloom(uuid, $numRows, 0.01)\")).head()\nval thebytes = aggrow.getAs[Array[Byte]](0)\nval bf = bloomLookup(thebytes)\nval fpp = 0.99\nval blooms: BloomFilterMap = Map(\"ids\" -&gt; (bf, fpp))\n</code></pre></li> <li>bigBloom( column, expected number of items, fpp ) - can only be run on large memory sized workers and executors and can cover billions of rows while maintaining the FPP: <pre><code>// via the expression\nval interim = df.selectExpr(s\"big_bloom($bloomOn, $expectedSize, $fpp, '$bloomId')\").head.getAs[Array[Byte]](0)\nval bloom = com.sparkutils.quality.BloomModel.deserialize(interim)\nbloom.cleanupOthers()\n\nval blooms: BloomFilterMap = Map(\"ids\" -&gt; (bloomLookup(bloom), fpp))\n\n// via the utility function, defaults to 0.01 fpp\nval bloom = bloomFrom(df, \"id\", expectedsize)\nval blooms: BloomFilterMap = Map(\"ids\" -&gt; (bloomLookup(bloom), 1 - bloom.fpp))\n</code></pre></li> </ul> <p>In testing the bigBloom creation over 1.5b rows on a small 4 node cluster took less than 8m to generate, using a resulting bloom however is far easier to load and distribute and constant time for lookups.  Whilst the actual big bloom itself cannot be directly broadcast only the file location of the resulting bloom is and each node on the cluster directly loads it from the ADLS (or other hopefully fast store for the multiple GBs).</p> <p>To change the base location for blooms use the sparkSession.sparkContext.setLocalProperty(\"sparkutils.quality.bloom.root\") to specify the location root.</p>"},{"location":"advanced/blooms/#bloom-loading","title":"Bloom Loading","text":"<p>The interface and config row data types is similar to that of View Loader with loadBloomConfigs accepting these additional columns:</p> <pre><code>bigBloom: Boolean, value: String, numberOfElements: BIGINT, expectedFPP: DOUBLE\n</code></pre> <ul> <li>bigBloom specifies which function should be used, when true the bigBloom algorithm will be used, when false the smallBloom.  </li> <li>value is an expression string suitable for the bloom filter, the expression will not parse if the type is unsupported, complex types will need special handling but it's typically possible to convert to an array of longs via hash functions such as hash_with.</li> <li>numberOfElements is an estimated upper bound for the size of the bloom filter, too low, and many false possible results will be generated</li> <li>expectedFPP is the starting percentage of expected percentage of false positives produced, or what can be tolerated, a value of 0.01 implies 99% of the time you get a \"should contain\" result it will be accurate, and 0.01% of the time it won't be.  When using too small an numberOfElements the expected fpp cannot be met.  bigBloom will attempt to use both to derive the optimal size with the probability that the resulting fpp is different.     </li> </ul> <pre><code>import sparkSession.implicits._\n\nval (bloomConfigs, couldNotLoad) = loadBloomConfigs(loader, config.toDF(), expr(\"id.id\"), expr(\"id.version\"), Id(1,1),\ncol(\"name\"),col(\"token\"),col(\"filter\"),col(\"sql\"), col(\"bigBloom\"),\ncol(\"value\"), col(\"numberOfElements\"), col(\"expectedFPP\")\n)\nval blooms = loadBlooms(bloomConfigs)\n</code></pre> <p>with couldNotLoad holding a set of configuration rows that aren't possible to load (neither a DataFrameLoader token nor an sql).</p> <p>loadBlooms will process the resulting dataframe using bigBloom, value, numberOfElements and expectedFPP to create the appropriate blooms.  Views first loaded via view loader are available when executing the sql column (when token is null).</p>"},{"location":"advanced/blooms/#expressions-which-take-expression-parameters","title":"Expressions which take expression parameters","text":"<ul> <li>probability_in( content to lookup, bloomfilterName ) - returns the fpp value of a filter lookup against the bloomFilter with bloomFilterName in the registered BloomFilterMap, which works with the Spark stats package, small and big blooms.</li> </ul>"},{"location":"advanced/expressionDocs/","title":"Expression Documentation","text":"<p>As Quality is based on sql it can be useful to document it in place, particularly with Lambda and Output expressions, but also applies to rules and trigger rules.</p> <p>The basic format follows javadocs / scaladocs approach, without *'s on each line, but is possible to define on one line:</p> <pre><code>/** My Description @param name name desc @param othername othername desc @return return val*/ </code></pre> <p>This could also be written with newlines including markdown (if the renderer supports it):</p> <pre><code>/** \nMy Description:\n\n* bullet point\n* more points\n\n@param name name desc \n@param othername othername:\n\n* more description points \n\n@return return val\n*/ </code></pre> <p>Param's are optional and will generate a warning if the names don't match in the validate function or if params are used on a non-lambda expression.</p> <p>The return value is also optional but would apply to all expressions.</p> <p>Whilst an incorrect parameter name will be flagged and warned against you won't be forced to put a comment for every parameter.</p> <p>A couple of helpful utility functions: <pre><code>  val (errors, warnings, out, docs, expr) = validate(Left(struct), ruleSuite)\n\nimport com.sparkutils.quality.utils.{RuleSuiteDocs, RelativeWarningsAndErrors}\n\nval relative = RelativeWarningsAndErrors(\"../sampleDocsValidation/\", errors, warnings)\nval md = RuleSuiteDocs.createMarkdown(docs, ruleSuite, expr, qualityURLGOESHERE+\"/sqlfunctions/\", Some(relative))\n\nIOUtils.write(md, new FileOutputStream(\"./docs/advanced/sampleDocsOutput.md\"))\n\nval emd = RuleSuiteDocs.createErrorAndWarningMarkdown(docs, ruleSuite, relative.copy( relativePath = \"../sampleDocsOutput/\"))\nIOUtils.write(emd, new FileOutputStream(\"./docs/advanced/sampleDocsValidation.md\"))\n</code></pre> exist to generate docs of a ruleSuite and validation errors.  The validate function returns both of these inputs.  You must specify the quality url containing the sqlfunction documentation in order to link, hrefs are not carried across mike links yet. </p> <p>The sample docs and sample errors/warnings are generated from the DocMarkdownTest.</p>"},{"location":"advanced/expressionRunner/","title":"QualityExpressions","text":"<p>ExpressionRunner applies a RuleSuite over a dataset returning the results of any expression as yaml (json cannot support non string map keys).  When used with just aggregate expressions it allows running dataset level checks, run after DQ it also allows statistics on individual rule results.   </p> <p>It is important to note that if you are having multiple runners in the same data pipeline they should each use different RuleSuites, and you should consider .cache'ing the intermediate results.</p> <p>RuleSuites are built per the normal DQ rules and executed by adding an expressionRunner column:</p> <pre><code>val dqRuleSuite = ...\n\nval aggregateRuleSuite = val testDataDF = ...\nimport frameless._\nimport quality.implicits._\n\n// first add dataQuality, then ExpressionRunner\nval processed = taddDataQuality(sparkSession.range(1000).toDF, dqRuleSuite).select(expressionRunner(aggregateRuleSuite)).cache\n\nval res = processed.selectExpr(\"expressionResults.*\").as[GeneralExpressionsResult].head()\nassert(res == GeneralExpressionsResult(Id(10, 2), Map(Id(20, 1) -&gt; Map(\nId(30, 3) -&gt; GeneralExpressionResult(\"'499500'\\n\", \"BIGINT\"),\nId(31, 3) -&gt; GeneralExpressionResult(\"'500'\\n\", \"BIGINT\")\n))))\n\nval gres =\nprocessed.selectExpr(\"rule_result(expressionResults, pack_ints(10,2), pack_ints(20,1), pack_ints(31,3)) rr\")\n.selectExpr(\"rr.*\").as[GeneralExpressionResult].head\n\nassert(gres == GeneralExpressionResult(\"'500'\\n\", \"BIGINT\"))\n</code></pre> <p>To retrieve results in the correct type use from_yaml with the correct ddl.  As Spark needs an exact type for any expression you can't simply flatten or explode as with the other Quality runner types, each result can have it's own type.  As such it's recommended that the expressionRunner result row is cached and extraction is performed with one of the following pattern:</p> <pre><code>import sparkSession.implicits._\nval t31_3 = processed.select(from_yaml(rule_result(col(\"expressionResults\"), pack_ints(10,2), pack_ints(20,1), pack_ints(Id(31,3)), 'BIGINT'))).as[Long].head\n</code></pre> <p>Or parse the GeneralExpressionResult map directly and:</p> <pre><code>val t31_3 = sparkSession.sql(s\"from_yaml(${res.ruleSetResults(Id(20,1))(Id(30,3)).ruleResult}, 'BIGINT')\").as[Long].head\n</code></pre> <p>or, finally, and perhaps as a last resort, to use snakeyaml and consume using the resulting java:</p> <pre><code>import org.yaml.snakeyaml.Yaml\nval yaml = new Yaml();\nval obj = yaml.load[Int](res.ruleSetResults(Id(20,1))(Id(30,3)).ruleResult).toLong;\nprintln(obj);\n</code></pre> <p>However, as can be seen with the direct use of snakeyaml example the types may not always automatically align and, in this case or Decimal you risk losing precision.  Note that the yml spec allows the default implementation as the values have no bit-length for either integer or floats.</p> <p>To increase precision / accuracy of the yaml types if you are not using from_yaml, you can provide the renderOptions map with useFullScalarType = 'true'.  This changes the output considerably:</p> <pre><code>val processed = taddDataQuality(sparkSession.range(1000).toDF, rowrs).select(expressionRunner(rs, renderOptions = Map(\"useFullScalarType\" -&gt; \"true\")))\nval gres =\nprocessed.selectExpr(\"rule_result(expressionResults, pack_ints(10,2), pack_ints(20,1), pack_ints(31,3)) rr\")\n.selectExpr(\"rr.*\").as[GeneralExpressionResult].head\n\n// NOTE: the extra type information allows snakeyaml to process Long directly without precision loss\nassert(gres == GeneralExpressionResult(\"!!java.lang.Long '500'\\n\", \"BIGINT\"))\n\nimport org.yaml.snakeyaml.Yaml\nval yaml = new Yaml();\nval obj = yaml.load[Long](res.ruleSetResults(Id(20,1))(Id(30,3)).ruleResult);\nprintln(obj);\n</code></pre> <p>NB: Decimal's will be stored with a java.math.BigDecimal type, rather than scala.math.BigDecimal</p> <p>You can add tags back in if needed</p> <p>As using useFullScalarType -&gt; true adds yaml type tags on all output scalars it can increase storage costs considerably, as such it's disabled by default.  </p> <p>It can however be retrieved by simply calling from_yaml and to_yaml again with it enabled if the end result should be used outside of Spark.</p> Don't mix aggregation functions with non-aggregation functions<p>Spark may complain before running an action, but it's also possible to produce incorrect results.</p> <p>This is the equivalent of running:</p> <pre><code>select *, sum(id) from table\n</code></pre> <p>which will not work without group by's.</p>"},{"location":"advanced/expressionRunner/#strip_result_ddl","title":"strip_result_ddl","text":"<p>The resultType string is useful in debugging but may not be for storage, if you wish to trim this information from the results use the strip_result_ddl function.</p> <p>This turns the result from GeneralExpressionResult into a simple string:</p> <pre><code>    val stripped = processed.selectExpr(\"strip_result_ddl(expressionResults) rr\")\n\nval strippedRes = stripped.selectExpr(\"rr.*\").as[GeneralExpressionsResultNoDDL].head()\nassert(strippedRes == GeneralExpressionsResultNoDDL(Id(10, 2), Map(Id(20, 1) -&gt; Map(\nId(30, 3) -&gt; \"'499500'\\n\",\nId(31, 3) -&gt; \"'500'\\n\")\n)))\n\nval strippedGres = {\nimport sparkSession.implicits._\nstripped.selectExpr(\"rule_result(rr, pack_ints(10,2), pack_ints(20,1), pack_ints(31,3))\")\n.as[String].head\n}\n\nassert(strippedGres == \"'500'\\n\")\n</code></pre>"},{"location":"advanced/mapFunctions/","title":"Map Functions","text":"<p>A typical use case for processing DQ rules is that of cached value processing, reference data lookups or industry code checks etc.</p> <p>Quality's map functions reproduce the result of joining datasets but guarantees in memory operation only once they are loaded, no merges or joins required.  However, for larger data lookups either Bloom Filters should be preferred or simply use joins.</p> <p>Similarly, for cases involving more logic than a simple equality check you must use joins or starting in 3.4 (DBR 12.2) scalar sub queries, see View Loader for a way to manage the loading of views. </p>"},{"location":"advanced/mapFunctions/#map-loading","title":"Map Loading","text":"<p>The interface and config row data types is similar to that of View Loader with loadMapConfigs accepting these additional columns:</p> <pre><code>val (mapConfigs, couldNotLoad) = loadMapConfigs(loader, config.toDF(), expr(\"id.id\"), expr(\"id.version\"), Id(1,1),\ncol(\"name\"),col(\"token\"),col(\"filter\"),col(\"sql\"),col(\"key\"),col(\"value\")\n)\n\nval maps = loadMaps(mapConfigs)\n</code></pre> <p>with couldNotLoad holding a set of configuration rows that aren't possible to load (neither a DataFrameLoader token nor an sql).</p> <p>loadMaps will process the resulting dataframe using key and value as sql expressions in exactly the same way as mapLookupFromDFs, as such they must be valid expressions against the source dataframe.  Views first loaded via view loader are available when executing the sql column (when token is null).</p>"},{"location":"advanced/mapFunctions/#building-the-lookup-maps-directly","title":"Building the Lookup Maps Directly","text":"<p>In order to lookup values in the maps Quality requires a map of map id's to the actual maps.</p> <pre><code>// create a map from ID to a MapCreator type with the dataframe and underlying \n// columns, including returning structures / maps etc.\nval lookups = mapLookupsFromDFs(Map(\n\"countryCode\" -&gt; ( () =&gt; {\nval df = countryCodeCCY.toDF(\"country\", \"funnycheck\", \"ccy\")\n(df, new Column(\"country\"), functions.expr(\"struct(funnycheck, ccy)\"))\n} ),\n\"ccyRate\" -&gt; ( () =&gt; {\nval df = ccyRate.toDF(\"ccy\", \"rate\")\n(df, new Column(\"ccy\"), new Column(\"rate\"))\n})\n))\nregisterMapLookupsAndFunction(lookups)\n</code></pre> <p>In the countryCode map lookup case we are creating a map from country to a structure (funnycheck, ccy), whereas the ccyRate is a simple lookup between ccy and it's rate at point of loading.</p> <p>Map creation is not lazy and is forced at time of calling the registerMap\u2026 function, for streaming jobs this may be unacceptable.  Prefer to use new map id's and merge old sets if you need to guarantee repeated calls to registerMapLookupsAndFunctions are working with up to date data.</p> <p>It's possible to have multiple fields used as the key, where all must match, just use struct in the same way as the value example above. </p> <p>Note</p> <p>Repeated calls and streaming use cases have not been thoroughly tested, the Spark distribution method guarantees an object can be broadcast but no merging is automatically possible, users would be required to code this by hand.</p>"},{"location":"advanced/mapFunctions/#expressions-which-take-expression-parameters","title":"Expressions which take expression parameters","text":"<ul> <li>map_lookup('map name', x) - looks up x against the map specified in map name, full type transparency from the underlying map values are supported including deeply nested structures <pre><code>// show the map of data 'country' field against country code and get back the currency\ndf.select(col(\"*\"), expr(\"map_lookup('countryCode', country).ccy\")).show()\n</code></pre></li> <li>map_contains('map name', x) - returns true or false if an item is present as a key in the map</li> </ul>"},{"location":"advanced/prng/","title":"PRNG Functions","text":"<p>The existing Spark rand function has a few of limitations:</p> <ul> <li>It generates doubles</li> <li>Has a fixed implementation</li> <li>Only provides reseeding on each new parition ignoring splittable / jumpable algorithms</li> </ul> <p>The Quality psuedorandom generators produce either 128bit values (two longs) or a configurable number of bytes and, as a result, do not suffer precision issues, they also leverage RandomSource implementations allowing users to choose the algorithm used.</p> <p>In addition, by leveraging .isJumpable and the resulting jump function the Quality prng function can benefit from the implementations own approach to managing overalapping intervals across the cluster.</p>"},{"location":"advanced/prng/#rng-expressions","title":"RNG Expressions","text":"<ul> <li>rng_bytes( [number of bytes to fill - defaults to 16], [RandomSource RNG Impl - defaults to 'XO_RO_SHI_RO_128_PP'], [seed - defaults to 0] ) - Uses commons rng to create byte arrays, implementations can be plugged in, when seed is 0 the RNG's default seed generator is used.  Note when a given RNG <code>isJumpable</code> then it will use jumping for each partition where possible both improving speed and statistical results.</li> <li>rng( [RandomSource RNG Impl - defaults to 'XO_RO_SHI_RO_128_PP'], [seed - defaults to 0] ) - Uses commons rng to create byte arrays, implementations can be plugged in, when seed is 0 the RNG's default seed generator is used.  Note when a given RNG <code>isJumpable</code> then it will use jumping for each partition where possible both improving speed and statistical results.</li> <li>rng_uuid( expr ) - processes expr with either byte arrays or two longs into a UUID string, it's counterpart long_pair_from_uuid generates two longs</li> </ul>"},{"location":"advanced/rowIdFunctions/","title":"Row ID Functions","text":"<p>Row ID functions are at least 160bit, made of a lower base id and two longs.  There are 4 distinct implementations:</p> <ol> <li>Random Number, a 128bit payload based on XO_RO_SHI_RO_128_PP</li> <li>Field Based, 128bit MD5 payload based on fields e.g. for DataVault style approaches</li> <li>Provided, an Opaque ID payload, typically 128bit, provided by some upstream system fields (MD5 is not used under the hood)</li> <li>Guaranteed Unique, 160bit ID based on Twitters snowflake IDs at Spark scale - requires MAC addresses to be stable and unique on a driver  </li> </ol> <p>These IDs use the \"base\" field to provide extensibility but comparisons must include all three fields (or more longs should they be added).</p> <p>From a performance perspective you should transform the column to make the structure into top-level fields via <pre><code>selectExpr(\"*\",\"myIDField.*\").drop(\"myIDField\")\n</code></pre></p> <ul> <li>rng_id('prefix') - generates a Random 128bit number with each column name prefixed for easy extraction</li> <li>unique_id('prefix') - generates a unique 160bit ID with each column name prefixed for easy extraction</li> <li>field_based_id('prefix', 'messagedigest', exp1, exp2, *) - generates a digest based e.g. 'MD5' identifier based on an expression list</li> <li>provided_id('prefix', longArrayBasedExpression) - generates a providedID based on supplied array of two longs expression</li> <li>murmur3_id('prefix', exp1, exp2, *) - generates and ID using hashes based on a version of murmur3 - not cryptographically secure but fast</li> <li>id_equal('left_prefix', 'right_prefix') - (SQL only) tests the two top level field IDs by adding the prefixes, note this does allow predicate push-down / pruning etc. (NB further versions may be added when 160bit is exceeded)</li> </ul> <p>Id's can be 96-bit or larger multiples of 64</p> <p>The algorithm you chose to use for generating Ids will change the length of underlying longs, idEqual cannot be used on different lengths but you can easily replace this with a lambda of the correct length.</p> <p>There are many different hash impls</p> <p>The fieldBasedID functions have a family of alternatives for MessageDigest, ZA based hashes and Guava based Hashers.  See SQL Functions and look for the Hash and ID tags.</p>"},{"location":"advanced/rowIdFunctions/#fieldbasedid-with-md5-seems-far-slower-than-other-approaches","title":"fieldBasedID with MD5 - Seems far slower than other approaches","text":"<p>It's definitely slower than either uniqueId or rngID.  If your use case allows it, consider murmur3ID if this is sufficient, it's slightly faster as is the XXH3 za hash.  MD5 was chosen based on the ubiquity of implementations including on backends (e.g. allowing datavault style approaches).</p>"},{"location":"advanced/rowIdFunctions/#guaranteed-unique-id-how","title":"Guaranteed Unique ID - How?","text":"<p>In order to lock down a globally (within a Spark using routable IP address space) ID you need to make sure a given machine, point in time and partition (thread) is unique.</p> <p>Your networking / vendor setup should guarantee the machines MAC Address is unique for your Spark Driver, Spark guarantees that the partition id, although re-usable, does not get re-used within a Spark cluster and for a given ms since an epoch we can lock down a range of row numbers.  This leaves the following storage model:</p> <pre><code>gantt\n    dateFormat YYYY-MM-DD\n\taxisFormat %j\n    title       Bit Layout\n\ttodayMarker off\n    \n    section First Int\n    Unique ID Type and Reserved Space :active, start, 2021-01-01, 8d\n    First 3 Bytes of MAC               :  startmac, after start, 24d\n    \n    section First Long\n    Last 3 Bytes of MAC                :endmac, after startmac, 24d\n    Spark Partition      :partition, after endmac, 32d\n    First 8 bits of Timestamp    :starttimestamp, after partition, 8d\n\t\n\tsection Second Long\n\tRest of Timestamp\t:done, endtimestamp, after starttimestamp, 33d\n\tRow number in Partition :rowid, after endtimestamp, 31d</code></pre> <p>When Spark starts a new partition the uniqueID expression resets the timestamp and partition and each row evaluates the rowid.  When 32bits of rowid would be hit the timestamp is reset and the count resets to 0 allowing over a billion rows per ms.</p> <p>This approach is faster than rngID but also means rows written to the same partitions have statistically incrementing id's allowing Parquet statistical ranges to be used for all three values in predicate pushdowns.</p>"},{"location":"advanced/ruleEngine/","title":"Engine","text":"<p>Quality provides a basic rule engine for data quality rules the output of each rule however is always translated to RuleResult, encoded and persisted for audit reasons.</p> <p>The ruleEngineRunner function however allows you to take an action based on the passing of a rule and, via salience, choose the most appropriate output for a given row.</p> <p>You can understand QualityRules as a large scale auditable SQL case statement with \"when\" being the trigger rule and the \"then\" as the output expression.</p> <p>RuleSuites are built per the normal DQ rules however a RuleResultProcessor is supplied:</p> <pre><code>  val ruleResultProcessor = RunOnPassProcessor(salience, Id(outputId, outputVersion), RuleLogicUtils.expr(\"array(account_row('from', account), account_row('to', 'other_account1'))\")))\nval rule = Rule(Id(id, version), expressionRule, ruleResultProcessor)\nval ruleSuite = RuleSuite(Id(ruleSuiteId, ruleSuiteVersion), Seq(\nRuleSet(Id(ruleSetId, ruleSetVersion), Seq(rule)\n)))\n\nval rer = ruleEngineRunner(ruleSuite,\nDataType.fromDDL(\"ARRAY&lt;STRUCT&lt;`transfer_type`: STRING, `account`: STRING&gt;&gt;\"))\nval testDataDF = ...\nval outdf = testDataDF.withColumn(\"together\", rer).selectExpr(\"*\", \"together.result\")\n</code></pre> <p>The ruleEngineRunner takes a DataType parameter that must describe the type of the result column type.  An additional salientRule column is available that packs three the Id's that represent the ruleId chosen by salience.  If this is null then no rule was triggered and the output column will also be null (verifiable via debug mode), if however there is an entry but the output is null then this signifies that the output expression produced a null.</p> <p>The salientRule column may be pulled apart down to the id number and versions via the unpack expression or unpackIdTriple to unpack the lot in one go.  If you are using frameless encoders these longs can be converted to a triple of Id's.  </p> <p>The salience parameter to the RunOnPassProcessor is used to ensure the lowest value is returned for a ruleSuite.  It is the responsibility of the rule configuration to ensure there can only be one output.</p> <p>All of the existing functionality, lambadas etc. can be used to customise the results and, as per the normal DQ processing, is run in-process across the clusters when the spark action is taken (like writing the dataframe to disk).</p>"},{"location":"advanced/ruleEngine/#serializing","title":"Serializing","text":"<p>The serializing approach uses the same functions as normal DQ RuleSuites, the only difference is you should use toDS and provide the two additional ruleEngine parameters when reading from a DF:</p> <pre><code>  val withoutLambdasAndOutputExpressions = readRulesFromDF(rulesDF,\ncol(\"ruleSuiteId\"),\ncol(\"ruleSuiteVersion\"),\ncol(\"ruleSetId\"),\ncol(\"ruleSetVersion\"),\ncol(\"ruleId\"),\ncol(\"ruleVersion\"),\ncol(\"ruleExpr\"),\ncol(\"ruleEngineSalience\"), col(\"ruleEngineId\"),\ncol(\"ruleEngineVersion\")\n)\n\nval lambdas = ...\nval outputExpressions = readOutputExpressionsFromDF(so.toDF(),\ncol(\"ruleExpr\"),\ncol(\"functionId\"),\ncol(\"functionVersion\"),\ncol(\"ruleSuiteId\"),\ncol(\"ruleSuiteVersion\")\n)\n\nval (ruleMap, missing) = integrateOutputExpressions(withoutLambdasAndOutputExpressions, outputExpressions)    </code></pre> <p>The ruleExpr is only run for the lowest ruleEngineSalience result of any passing ruleExpr.  The missing result will contain any output expressions specified by a rule which do not exist in the output expression dataframe based by rulesuite id, if your rulesuite id is not present in the missing entries your RuleSuite is good to go.</p> <p>The rest of the serialization functions to combine lambdas etc. work as per normal DQ rules allowing you to use lambda functions in your QualityRules output rules as well.</p> <p>The result of toDS will contain the three ruleEngine fields, you can simply drop them if they are not needed.</p>"},{"location":"advanced/ruleEngine/#debugging","title":"Debugging","text":"<p>The RuleResult's indicate if a rule has not triggered but in the case of multiple matching rules it can be useful to see which rules would have been chosen.</p> <p>To enable this you can add the debugMode parameter to the ruleEngineRunner:</p> <pre><code>  val rer = ruleEngineRunner(ruleSuite,\nDataType.fromDDL(\"ARRAY&lt;STRUCT&lt;`transfer_type`: STRING, `account`: STRING&gt;&gt;\"),\ndebugMode = true)\n</code></pre> <p>This changes the output column 'result' field type to:</p> <pre><code>ARRAY&lt;STRUCT&lt;`salience`: INTEGER, `result`: ARRAY&lt;ORIGINGALRESULTTYPE&gt;&gt;\n</code></pre> <p>Why do I have a null</p> <p>There are two cases where you may get a null result:</p> <ol> <li>no rules have matched (you can verify this as you'll have no passed() rules).</li> <li>your rule actually returned a null (you can verify this by putting on debug mode, you'll see a salience but no result)</li> </ol>"},{"location":"advanced/ruleEngine/#flatten_rule_results","title":"flatten_rule_results","text":"<pre><code>  val outdf = testDataDF.withColumn(\"together\", rer).selectExpr(\"explode(flatten_rule_results(together)) as expl\").selectExpr(\"expl.*\")\n</code></pre> <p>This sql function behaves the same way as per flatten_results, however there are now two structures to 'explode'.  debugRules works as expected here as well.</p>"},{"location":"advanced/ruleEngine/#resolvewith","title":"resolveWith","text":"<p>Use with care - very experimental</p> <p>The resolveWith functionality has several issues with Spark compatibility which may lead to code failing when it looks like it should work. Known issues:</p> <ol> <li>Using filter then count will stop necessary attributes being produced for resolving, Spark optimises them out as count doesn't need them, however the rules definitely do need some attributes to be useful.</li> <li>You may not select different attributes, remove any, re-order them, or add extra attributes, this is likely to cause failure in show'ing or write'ing</li> <li>Spark is free to optimise other actions than just count, ymmv in which ones work.     </li> <li>The 0.1.0 implementation of update_field (based on the Spark impl) does not work in some circumstances (testSimpleProductionRules, testSalience will fail) - see #36</li> </ol> <p>resolveWith attempts to improve performance of planning for general spark operations by first using a reduced plan against the source dataframe.  The resulting Expression will have all functions and attributes resolved and is hidden from further processing by Spark until your rules actually run. </p> <pre><code>  val testDataDF = ....\n\nval rer = ruleEngineRunner(ruleSuite,\nDataType.fromDDL(DDL), debugMode = debugMode, resolveWith = resolveWith = Some(testDataDF))\n\nval withRules = rer.withColumn(\"ruleResults\", rer)\n\n// ... use the rules\n</code></pre>"},{"location":"advanced/ruleEngine/#why-is-this-needed","title":"Why is this needed?","text":"<p>For RuleSuites with 1000s of triggers the effort for Spark to prepare the rules is significant.  In tests 1k rule with 50 field evalutaions is already sufficient to cause a delay of over 1m for each action (show, write, count etc.) and the size of the data being processed is not relevant.</p> <p>After building the action QualityRules scale and perform as expected, but that initial costs of 1m per action is significant as it can only be improved by higher spec drivers.</p> <p>resolveWith, if it works for given use case, drastically reduces this cost, the above 1k example is a 30s evaluation up front and far less cost for each further action.</p> <p>With the rather horrible 1k rule example the clock time of running 1k rows through 1k rules with a simple show, then count and write for actions was 6m15s on an Azure b4ms, using resolveWith brings this down to 1m30s for the same actions.  Still not blazingly fast of course, but far more tolerable and becomes suitable for smaller batch jobs.</p>"},{"location":"advanced/ruleEngine/#any-reason-why-i-shouldnt-try-it","title":"Any reason why I shouldn't try it?","text":"<p>Not really but for production use cases where your trigger and output rules complexity is low you should prefer to not use it, it's likely fast enough and this solution is very much experimental.</p> <p>You definitely shouldn't use it when using relation or table fields in your expressions e.g. table.field this does not work (verify this by running JoinValidationTest using evalCodeGens instead of evalCodeGensNoResolve).  There be dragons.  This is known to fail on all OSS builds and OSS runtimes (up to and including 3.2.0).  10.2.dbr and 9.1.dbr actually do work running the tests in notebooks with resolveWith and relations (the test itself is not built for this however to ensure cross compilation on the OSS base).</p>"},{"location":"advanced/ruleEngine/#forcerunnereval","title":"forceRunnerEval","text":"<p>By default, QualityRules runs with an optimised wholestage codegen wherever possible.  This works by breaking out the nested structure of a RuleSuite into multiple index, salience and id arrays which are fixed for the duration of an action.  Whilst this reduces the overhead of array and temporary structure creation the compilation also unrolls the evaluation of trigger rules allowing jit optimisations to kick in.</p> <p>Using large RuleSuites, however, may cause large compilation times which are unsuitable for smaller batches, as such you can force the interpreted path to be used by setting this parameter to true.  Individual trigger and output expressions are still compiled but the evaluation will not be.</p>"},{"location":"advanced/ruleEngineWorkflow/","title":"Workflow","text":""},{"location":"advanced/ruleEngineWorkflow/#overview-and-terms","title":"Overview and terms","text":"<p>QualityRules is a matching engine which applies match/trigger rules to a Dataframe and, when these rules evaluate to passed (i.e. they match or trigger) output sql is run.</p> <p>Only one trigger rule may produce output, so salience is used as a tie-breaker, the lowest salience wins.</p> Aim to have unique salience for tie-breaking<p>If you have multiple trigger rules with the same salience that both trigger the \"winning\" output chosen is non-deterministic, chose your salience wisely.  </p> <p>An alternative way to think of this is the trigger rules are your if and the output expressions are the when, from a logic perspective it may be helpful to think of them as output verbs - when this is true do that.</p>"},{"location":"advanced/ruleEngineWorkflow/#suggested-approach-to-qualityrules-management","title":"Suggested approach to QualityRules management","text":"<ul> <li>Keep unrelated rules in their own RuleSuites, making things easier to reason about</li> <li>Make commonly used lambdas or output expressions global</li> <li>Use descriptive verbs for your output expressions<ul> <li>Keep duplication or complexity in lambdas</li> <li>Only use fields that change as parameters to those lambdas</li> </ul> </li> <li>Always start with test data you want to match against and your expected output</li> <li>Run all test cases for your RuleSuite for any change, don't assume because your rule worked that others won't stop working</li> <li>Use the validation and documentation functionality to document your lambdas and verify you've not made simple mistakes - Spark errors aren't always easy to understand </li> </ul> <p>This could be visualised as such:</p> <p></p> <p>Don't repeat yourself</p> <p>If you are typing the same trigger rule, output expression or even lambda text repeatedly - make another lambda and consider making it global </p>"},{"location":"advanced/ruleFolder/","title":"QualityFolder","text":"<p>The ruleFolderRunner function uses the same data formats and structures as the ruleEngineRunner (with the exception of RuleFolderResult) however it allows you to \"fold\" results over many matching rules.</p> <p>In contrast to ruleEngineRunner, which uses salience to select which output expression to run, ruleFolderRunner uses salience to order the execution of each matching output expression.  To facilitate this OutputExpressions in the ruleFolderRunner must be lambdas with one parameter.</p> <p>ruleFolderRunner takes a starter Column, which is evaluated against the row and then is passed as the parameter to the OutputExpression lambdas, in turn the result of these output lambdas is then fed in to the next matching OutputExpression and folded over until the last is run, which is returned.</p> <p>When using debugMode you get the salience and each output returned in the resulting array, as with ruleEngineRunner the Encoder derivations for RuleFolderResult work with both T and Seq[(Int, T)] where the Int is salience.</p> <p>RuleSuites are built per the normal DQ rules however a RuleResultProcessor is supplied with Lambda OutputExpressions:</p> <pre><code>  val ruleResultProcessor = RunOnPassProcessor(salience, Id(outputId, outputVersion), RuleLogicUtils.expr(\"thecurrent -&gt; update_field(thecurrent, 'account', concat(thecurrent.account, '_suffix') )\")))\nval rule = Rule(Id(id, version), expressionRule, ruleResultProcessor)\nval ruleSuite = RuleSuite(Id(ruleSuiteId, ruleSuiteVersion), Seq(\nRuleSet(Id(ruleSetId, ruleSetVersion), Seq(rule)\n)))\n\nval rer = ruleFolderRunner(ruleSuite,\nstruct($\"transfer_type\", $\"account\"))\nval testDataDF = ...\nval outdf = testDataDF.withColumn(\"together\", rer).selectExpr(\"*\", \"together.result\")\n</code></pre> <p>You may use multiple path and expression combinations in the same call, allowing the change of multiple fields at once - this will be faster than nesting calls to updateField.</p> Don't use 'current' for a variable on 2.4<p>It may be tempting to use 'current' as your lambda variable name, but this causes problems on 2.4 - every other version doesn't care.</p> Don't use resolveWith on 2.4<p>2.4 will NPE using withResolve, this does not occur on more recent Spark versions</p> Don't use select(*, ruleFolderRunner)<p>Spark will not NPE using withColumn but will using select(expr(\"*\"), ruleFolderRunner(ruleSuite)).  In order to thread the types through the resolving needs an additional projection, if you must avoid withColumn (e.g for performance reasons) then you may specify the DDL via the useType parameter.</p>"},{"location":"advanced/ruleFolder/#set","title":"Set","text":"<p>Although the use of lambda expressions allows you full control of your output expression it can be a bit verbose.  The common use case of defaulting is more easily expressed via the following syntax:</p> <pre><code>set( variable.path = expression to assign, variable2 = other expression, variable3 = expression using currentResult )\n</code></pre> <p>Only valid variable names and paths, followed by equal and valid expressions (however complex) are allowed.</p> <p>The following two folder expressions are equivalent, indeed the set call is translated into the lambda:</p> <pre><code>set( account = concat(currentResult.account, '_suffix'), ammount = 5 )\n\ncurrentResult -&gt; update_field(currentResult, 'account', concat(currentResult.account, '_suffix'), 'ammount', 5 )\n</code></pre> <p>The set syntax defaults the name of the lambda variable to \"currentResult\" and removes the odd looking quotes around the variable names. </p>"},{"location":"advanced/ruleFolder/#flatten_folder_results","title":"flatten_folder_results","text":"<pre><code>  val outdf = testDataDF.withColumn(\"together\", rer).selectExpr(\"explode(flatten_folder_results(together)) as expl\").selectExpr(\"expl.result\")\n</code></pre> <p>This sql function behaves the same way as per flatten_rule_results with debugRules working as expected.</p>"},{"location":"advanced/ruleFolder/#resolvewith","title":"resolveWith","text":"<p>Use with care - very experimental</p> <p>The resolveWith functionality has several issues with Spark compatibility which may lead to code failing when it looks like it should work. Known issues:</p> <ol> <li>Using filter then count will stop necessary attributes being produced for resolving, Spark optimises them out as count doesn't need them, however the rules definitely do need some attributes to be useful.</li> <li>You may not select different attributes, remove any, re-order them, or add extra attributes, this is likely to cause failure in show'ing or write'ing</li> <li>Spark is free to optimise other actions than just count, ymmv in which ones work.     </li> <li>The 0.1.0 implementation of update_field (based on the Spark impl) does not work in some circumstances (testSimpleProductionRules will fail) - see #36</li> </ol> <p>resolveWith attempts to improve performance of planning for general spark operations by first using a reduced plan against the source dataframe.  The resulting Expression will have all functions and attributes resolved and is hidden from further processing by Spark until your rules actually run. </p> <pre><code>  val testDataDF = ....\n\nval rer = ruleEngineRunner(sparkSession.sparkContext.broadcast(ruleSuite),\nDataType.fromDDL(DDL), debugMode = debugMode, resolveWith = resolveWith = Some(testDataDF))\n\nval withRules = rer.withColumn(\"ruleResults\", rer)\n\n// ... use the rules\n</code></pre> <p>You definitely shouldn't use it when using relation or table fields in your expressions e.g. table.field this does not work (verify this by running JoinValidationTest using evalCodeGens instead of evalCodeGensNoResolve).  There be dragons.  This is known to fail on all OSS builds and OSS runtimes (up to and including 3.2.0).  10.2.dbr and 9.1.dbr actually do work running the tests in notebooks with resolveWith and relations (the test itself is not built for this however to ensure cross compilation on the OSS base).</p>"},{"location":"advanced/sampleDocsOutput/","title":"sampleDocsOutput","text":""},{"location":"advanced/sampleDocsOutput/#rulesuite-id-0-1-3-errors-3-warnings","title":"RuleSuite Id 0, 1  - 3 Errors 3 Warnings","text":""},{"location":"advanced/sampleDocsOutput/#ruleset-id-1-1","title":"RuleSet Id - 1, 1","text":""},{"location":"advanced/sampleDocsOutput/#rule-id-2-1-1-warnings","title":"Rule Id - 2, 1  -  1 Warnings","text":"<p>description</p> Parameter Description fielda desc <pre><code>concat(fielda, fieldb)\n</code></pre> <p>Spark functions used:</p> <ul> <li> concat </li> </ul> <p>Triggers output rule with id 6, 1 Salience 0</p>"},{"location":"advanced/sampleDocsOutput/#rule-id-6-1-1-warnings","title":"Rule Id - 6, 1  -  1 Warnings","text":"<pre><code>fielda &gt; fieldb\n</code></pre> <p>Triggers output rule with id 1002, 1 Salience 0</p>"},{"location":"advanced/sampleDocsOutput/#rule-id-4-1-","title":"Rule Id - 4, 1  -","text":"<pre><code>testCaller2(fielda &gt; fieldb) and test(fieldb)\n</code></pre> <p>Lambda used:</p> <ul> <li> testCaller2 </li> <li> test </li> </ul>"},{"location":"advanced/sampleDocsOutput/#rule-id-5-1-","title":"Rule Id - 5, 1  -","text":"<pre><code>map_Lookup(fielda, fieldb) and test(fieldb)\n</code></pre> <p>Quality functions used:</p> <ul> <li> map_Lookup </li> </ul> <p>Lambda used:</p> <ul> <li> test </li> </ul>"},{"location":"advanced/sampleDocsOutput/#rule-id-16-1-3-errors","title":"Rule Id - 16, 1  -  3 Errors","text":"<pre><code>nonExistentFunction(fielda) and nonExistentFielda &gt; nonExistentFieldb\n</code></pre>"},{"location":"advanced/sampleDocsOutput/#output-rules","title":"Output Rules","text":""},{"location":"advanced/sampleDocsOutput/#output-rule-id-6-1-1-warnings","title":"Output Rule Id - 6, 1  -  1 Warnings","text":"<pre><code>testCaller2(fielda, fieldb)\n</code></pre> <p>Lambda used:</p> <ul> <li> testCaller2 </li> </ul> <p>Called by Rules:</p> <ul> <li> 2 - 1 </li> </ul>"},{"location":"advanced/sampleDocsOutput/#output-rule-id-1002-1-1-warnings","title":"Output Rule Id - 1002, 1  -  1 Warnings","text":"<p>description 2</p> Parameter Description fielda desc 2 <pre><code>concat(fielda, fieldb)\n</code></pre> <p>Spark functions used:</p> <ul> <li> concat </li> </ul> <p>Called by Rules:</p> <ul> <li> 6 - 1 </li> </ul>"},{"location":"advanced/sampleDocsOutput/#lambdas","title":"Lambdas","text":""},{"location":"advanced/sampleDocsOutput/#lambda-test","title":"Lambda test","text":""},{"location":"advanced/sampleDocsOutput/#rule-id-6-1-1-warnings_1","title":"Rule - Id - 6, 1  -  1 Warnings","text":"<p>Name test lambda description</p> Parameter Description fielda lambda desc <pre><code>variable -&gt; variable\n</code></pre> <p>Called by Rules:</p> <ul> <li> 5 - 1 </li> <li> 4 - 1 </li> </ul>"},{"location":"advanced/sampleDocsOutput/#lambda-testcaller2","title":"Lambda testCaller2","text":""},{"location":"advanced/sampleDocsOutput/#rule-id-7-2-","title":"Rule - Id - 7, 2  -","text":"<p>Name testCaller2</p> <pre><code>(outervariable1, variable2) -&gt; concat(outervariable1, variable2) and acos(fielda)\n</code></pre> <p>Spark functions used:</p> <ul> <li> concat </li> <li> acos </li> </ul> <p>Called by Lambdas:</p> <ul> <li> testCaller3 </li> </ul> <p>Called by Output Expressions:</p> <ul> <li> 6 - 1 </li> </ul> <p>Called by Rules:</p> <ul> <li> 4 - 1 </li> </ul>"},{"location":"advanced/sampleDocsOutput/#lambda-testcaller3","title":"Lambda testCaller3","text":""},{"location":"advanced/sampleDocsOutput/#rule-id-8-1-","title":"Rule - Id - 8, 1  -","text":"<p>Name testCaller3 lambda description only</p> <pre><code>(outervariable1, variable2, variable3) -&gt; testCaller2(outervariable1, variable2)\n</code></pre> <p>Lambda used:</p> <ul> <li> testCaller2 </li> </ul>"},{"location":"advanced/sampleDocsValidation/","title":"sampleDocsValidation","text":""},{"location":"advanced/sampleDocsValidation/#errors-summary","title":"Errors Summary","text":"Type Count RuleNameError Name nonExistentFielda is missing 1 RuleNameError Name nonExistentFieldb is missing 1 SparkFunctionNameError Name nonExistentFunction is missing 1"},{"location":"advanced/sampleDocsValidation/#warnings-summary","title":"Warnings Summary","text":"Type Count NonLambdaDocParameters Parameter documentation is present on a non lambda expression 2 ExtraDocParameter Parameter fielda is not found in the lambda expression 1"},{"location":"advanced/sampleDocsValidation/#errors-identified-for-rulesuite-id-0-1","title":"Errors Identified for RuleSuite - Id 0, 1","text":""},{"location":"advanced/sampleDocsValidation/#id-16-1","title":"Id 16, 1","text":"<p>RuleNameError Name nonExistentFielda is missing occurred when processing id Id(16,1) against expression</p> <pre><code>nonExistentFunction(fielda) and nonExistentFielda &gt; nonExistentFieldb\n</code></pre> <p>RuleNameError Name nonExistentFieldb is missing occurred when processing id Id(16,1) against expression</p> <pre><code>nonExistentFunction(fielda) and nonExistentFielda &gt; nonExistentFieldb\n</code></pre> <p>SparkFunctionNameError Name nonExistentFunction is missing occurred when processing id Id(16,1) against expression</p> <pre><code>nonExistentFunction(fielda) and nonExistentFielda &gt; nonExistentFieldb\n</code></pre>"},{"location":"advanced/sampleDocsValidation/#warnings-identified-for-rulesuite-id-0-1","title":"Warnings Identified for RuleSuite - Id 0, 1","text":""},{"location":"advanced/sampleDocsValidation/#id-2-1","title":"Id 2, 1","text":"<p>NonLambdaDocParameters Parameter documentation is present on a non lambda expression, occurred when processing id Id(2,1) against expression</p> <pre><code>/** description @param fielda desc */ concat(fielda, fieldb)\n</code></pre>"},{"location":"advanced/sampleDocsValidation/#id-1002-1","title":"Id 1002, 1","text":"<p>NonLambdaDocParameters Parameter documentation is present on a non lambda expression, occurred when processing id Id(1002,1) against expression</p> <pre><code>/** description 2 @param fielda desc 2 */ concat(fielda, fieldb)\n</code></pre>"},{"location":"advanced/sampleDocsValidation/#id-6-1","title":"Id 6, 1","text":"<p>ExtraDocParameter Parameter fielda is not found in the lambda expression, occurred when processing id Id(6,1) against expression</p> <pre><code>/** lambda description @param fielda lambda desc */ variable -&gt; variable\n</code></pre>"},{"location":"advanced/userFunctions/","title":"User Defined Functions","text":"<p>Users may register Lambda Functions using the sql lambda syntax: <pre><code>val rule = LambdaFunction(\"multValCCY\", \"(theValue, ccy) -&gt; theValue * ccy\", Id(1,2))\nregisterLambdaFunctions(Seq(rule))\n</code></pre> they may be then called in rules (or within any SQL expressions), in this case value and ccyrate from the data frame are provided to the function as parameters theValue and ccy:  <pre><code>val ndf = df.withColumn(\"newcalc\", expr(\"multValCCY(value, ccyrate)\"))\n</code></pre> The function parameter and return types are derived during the analysis phase, this may lead to errors if types do not match the expressions upon an action only, such as writing or calling show.</p> <p>Note</p> <p>Whilst you are free to add lambdas when not using a RuleSuite the library will not ensure that only functions registered as part of a RuleSuite are used in rules, such hygiene is necessarily left to the user.</p> <p>LambdaFunctions may have any number of parameters e.g. given a greaterThan lambda: <pre><code> (param1, param2) -&gt; param1 &gt; param2\n</code></pre>   you would be able to call it with two expressions <pre><code> greaterThan(col1, col2)\n</code></pre></p> <p>Single argument lambdas should not use brackets around the parameters and zero argument lambdas use no input or -&gt;.  In all cases the lambda can use the attributes from the surrounding dataframe - it's effectively global, you cannot use variables from surrounding / calling lambdas.</p> Don't use 'current'\u2026 as a lambda variable name on 2.4<p>Bizarrely this causes the parser to fail on 2.4 only, no more recent version suffers this.  Same goes for left or right as names.</p>"},{"location":"advanced/userFunctions/#what-about-default-parameter-or-different-length-parameter-length-lambdas","title":"What about default parameter or different length parameter length Lambdas?","text":"<p>To define multiple parameter length lambdas just define new lambdas with the same name but different argument lengths.  You can freely call the same lambda name with different parameters e.g.:</p> <pre><code>val rule = LambdaFunction(\"multValCCY\", \"multValCCY(value, ccyrate)\", Id(1,2))\nval rule1 = LambdaFunction(\"multValCCY\", \"theValue -&gt; multValCCY(theValue, ccyrate)\", Id(2,2))\nval rule2 = LambdaFunction(\"multValCCY\", \"(theValue, ccy) -&gt; theValue * ccy\", Id(3,2))\nregisterLambdaFunctions(Seq(rule, rule1, rule2))\n\n// all of these should work\ndf.withColumn(\"newcalc\", expr(\"multValCCY()\"))\ndf.withColumn(\"newcalc\", expr(\"multValCCY(value)\"))\ndf.withColumn(\"newcalc\", expr(\"multValCCY(value, ccyrate)\"))\n</code></pre>"},{"location":"advanced/userFunctions/#higher-order-functions","title":"Higher Order Functions","text":"<p>As Lambda's in Spark aren't first class citizens you can neither partially apply them (fill in parameters to derive new lambdas) nor pass them into a lambda.</p> <p>Quality experimentally adds three new concepts to the mix:</p> <ol> <li>Placeholders - <code>_()</code> - which represents a value which still needs to be filled (partial application)</li> <li>Application - <code>callFun()</code> - which, in a lambda, allows you to apply a function parameter</li> <li>Lambda Extraction - <code>_lambda_()</code> - which allows Lambdas to be used with existing Spark HigherOrderFunctions (like aggregate) </li> </ol> <p>Unfortunately the last piece of that puzzle of returning a higher order function isn't currently possible.</p> <p>Putting together 1 and 3 (straight out of the test suite):</p> <pre><code>val plus = LambdaFunction(\"plus\", \"(a, b) -&gt; a + b\", Id(1,2))\nval plus3 = LambdaFunction(\"plus3\", \"(a, b, c) -&gt; a + b + c\", Id(2,2))\nval hof = LambdaFunction(\"hof\", \"func -&gt; aggregate(array(1, 2, 3), 0, _lambda_(func))\", Id(3,2))\nregisterLambdaFunctions(Seq(plus, plus3, hof))\n\nimport sparkSession.implicits._\n\n// attempt to dropping a reference to a function where simple lambdas are expected.\n// control\nassert(6 == sparkSession.sql(\"SELECT aggregate(array(1, 2, 3), 0, (acc, x) -&gt; acc + x) as res\").as[Int].head)\n// all params would be needed with multiple aritys\nassert(6 == sparkSession.sql(\"SELECT aggregate(array(1, 2, 3), 0, _lambda_(plus(_('int'), _('int')))) as res\").as[Int].head)\n// can we play with partials?\nassert(21 == sparkSession.sql(\"SELECT aggregate(array(1, 2, 3), 0, _lambda_(plus3(_('int'), _('int'), 5))) as res\").as[Int].head)\n// hof'd\nassert(6 == sparkSession.sql(\"SELECT hof(plus(_('int'), _('int'))) as res\").as[Int].head)\n</code></pre> <p>In the above example you can see type's being specified to the placeholder function, this is needed because, similar to aggExpr, Spark can't know the types until after they are evaluated and resolved.  This does have the benefit of keeping the types at the partial application site. The default placeholder type is Long / Bigint.</p> <p>The lambda function extracts a fully resolved underlying Spark LambdaFunction, which means the types must be correct as it is provided to the function (use the placeholder function to specify types).  Similarly, you use the lambda function to extract the Spark LambdaFunction from a user provided parameter (as seen in the hof example).</p> <p>The aggregate function only accepts two parameters for its accumulator, but in the plus3 example we've 'injected' in a third.  Partially applying the plus3 with the value 5 in it's \"c\" position leaves the two arguments as new function.  Quality ensures the necessary transformations are done before it hits the aggregate expression.</p> <p>Great, but can I use it with aggExpr?  Yep:</p> <pre><code>select aggExpr('DECIMAL(38,18)', dec IS NOT NULL, myinc(_()), myretsum(_(), _())) as agg\n</code></pre> <p>allows you to define the myinc and myretsum elsewhere, you don't need to use the lambda function with aggExpr.</p> <p>What about application?  Using callFun:</p> <pre><code>val use = LambdaFunction(\"use\", \"(func, b) -&gt; callFun(func, b)\", Id(4,2))   </code></pre> <p>the first parameter must be the lambda variable referring to your function followed by the necessary parameters to pass in.  Func in this case has a single parameter but of course it could have started with 5 and had 4 partially applied.  Again you don't need to use lambda to pass the functions further down the line:</p> <pre><code>val deep = LambdaFunction(\"deep\", \"(func, a, b) -&gt; use(func, a, b)\", Id(2,2))\n</code></pre> <p>Deep takes the function and simply passes it to use where the callFun exists.</p> <p>Finally, you can also further partially apply your lambda variables:</p> <pre><code>val plus2 = LambdaFunction(\"plus\", \"(a, b) -&gt; a + b\", Id(3,2))\nval plus3 = LambdaFunction(\"plus\", \"(a, b, c) -&gt; plus(plus(a, b), c)\", Id(3,2))\nval papplyt = LambdaFunction(\"papplyt\", \"(func, a, b, c) -&gt; callFun(callFun(func, _(), _(), c), a, b)\", Id(2,2))\nregisterLambdaFunctions(Seq(plus2, plus3, papplyt))\n\nimport sparkSession.implicits._\n\nassert(6L == sparkSession.sql(\"select papplyt(plus(_(), _(), _()), 1L, 2L, 3L) as res\").as[Long].head)\n</code></pre> <p>Here the callFun directly applies the function afterwards but you could equally pass it to other functions.</p> <pre><code>callFun(callFun(func, _(), _(), c), a, b)\n</code></pre> <p>can then be read as partially apply func (plus with 3 arguments) parameter 3 with the lambda variable c, creating a new two argument function.  Then call that function with the a and b parameters.  Useless in this case perhaps but it should be illustrative. </p> <p>All that's missing is returning lambdas:</p> <pre><code>val plus2 = LambdaFunction(\"plus\", \"(a, b) -&gt; a + b\", Id(3,2))\nval plus3 = LambdaFunction(\"plus\", \"(a, b, c) -&gt; plus(plus(a, b), c)\", Id(3,2))\nval retLambda = LambdaFunction(\"retLambda\", \"(a, b) -&gt; plus(a, b, _())\", Id(2,2))\nregisterLambdaFunctions(Seq(plus2, plus3, retLambda))\n\nimport sparkSession.implicits._\n\nassert(6L == { val sql = sparkSession.sql(\"select callFun(retLambda(1L, 2L), 3L) as res\")\nsql.as[Long].head})\n</code></pre> <p>here the user function retLambda returns the plus with 3 arity applied over a and b, leaving a function of one arity to fill.  The top level callFun then applies the last argument (c).</p> <p>It's sql only</p> <p>As you can create your own functions based on Column transformations the functionality is not extended to the dsl where there are better host language based solutions.</p> It is experimental<p>Although behaviour has been tested with compilation and across the support DBRs it's entirely possible there are gaps in the trickery used.</p> <p>A good example of the experimental nature is the _() function, it's quite possible that is taken by Spark at a later stage.</p> lambda drop in call arguments to transform_values and transform_keys don't work on 3.0 and 3.1.2/3<p>They pattern match on List and not seq, later versions fix this.  To work around this you must explicitly use lambdas for these functions.</p> Do not mix higher order functions with subqueries<p>Per SPARK-47509 subqueries within a HOF can lead to correctness issues, such usage is not supported </p>"},{"location":"advanced/userFunctions/#controlling-compilation-tweaking-the-quality-optimisations","title":"Controlling compilation - Tweaking the Quality Optimisations","text":"<p>Normal Spark LambdaFunctions, NamedLambdaVariable and HigherOrderFunctions aren't compiled, this is - in part - due to the nature of having to thread the lambda variables across the Expression tree and calling bind.</p> <p>At the time of codegen bind has already been called however so the code is free to create a new tree just for compilation.  Quality makes use of this and replaces all NamedLambdaVariables expressions with a simple variable in the generated code.</p> <p>NamedLambdaVariables also use AtomicReferences, which was introduced to avoid a tree manipulation task - see here for the code introduction.  AtomicReferences are slower for both writes and reads of non-contended variables.  As such Quality does away with this in its compilation, the exprId is sufficient to track the actual id.</p> <p>Quality only attempts to replace it's own FunN and reverts to using NamedLambdaVariables if it encounters any other HigherOrderFunction.  Where it can replace it uses NamedLambdaVariableCodeGen with an ExprId specific code snippet.</p> <p>You can customise this logic via implementing:</p> <pre><code>  trait LambdaCompilationHandler {\n/**\n     *\n     * @param expr\n     * @return empty if the expression should be transformed (i.e. there is a custom solution for it).  Otherwise return the full set of NamedLambdaVariables found\n     */\ndef shouldTransform(expr: Expression): Seq[NamedLambdaVariable]\n\n/**\n     * Transform the expression using the scope of replaceable named lambda variable expression\n     * @param expr\n     * @param scope\n     * @return\n     */\ndef transform(expr: Expression, scope: Map[ExprId, NamedLambdaVariableCodeGen]): Expression\n}\n</code></pre> <p>and supplying it via the environment variable, System.property or via sparkSession.sparkContext.setLocalProperty quality.lambdaHandlers using this format:</p> <pre><code> name=className\n</code></pre> <p>where name is either a fully qualified class name of a HigherOrderFunction or of a lambda (FunN) function.</p> <p>The default org.apache.spark.sql.qualityFunctions.DoCodegenFallbackHandler allows you to disable any optimisation for a HigherOrderFunction.  It can be used to disable all FunN optimisations with:</p> <pre><code>-Dquality.lambdaHandlers=org.apache.spark.sql.qualityFunctions.FunN=org.apache.spark.sql.qualityFunctions.DoCodegenFallbackHandler\n</code></pre> <p>Alternatively if you have a hotspot with any inbuilt HoF such as array_transform, filter or transform_values you could replace the implementation for compilation with your own transformation. e.g.:</p> <pre><code>-Dquality.lambdaHandlers=org.apache.spark.sql.catalyst.expressions.TransformValues=org.mine.SuperfastTransformValues\n</code></pre>"},{"location":"advanced/userFunctions/#why-do-all-this","title":"Why do all this?","text":"<p>Speed, it's up to 40% faster. LambdaRowPerfTest, in the test suite, generates an increasing number of lambdas and only runs over 10k rows but still sees clear benefits e.g. (orange is compiled lambdas):</p> <p></p> <p>This difference is already noticeable with a small increment function in a folder:</p> <pre><code>thecurrent -&gt; updateField(thecurrent, 'thecount', thecurrent.thecount + 1)\n</code></pre> <p>The difference is typically higher with nested lambdas.  Should your compilation time exceed the execution time you may wish to disable compilation via the fallback handler.</p>"},{"location":"advanced/validation/","title":"Validation","text":"<p>Quality provides some validation utilities that can be used as part of your rule design activity to ensure sure you aren't using variables or functions that don't exist, or even possibly having recursive lambda calls.</p> <p>It comes in two distinct flavours:</p> <ol> <li>Schema Based - The schema representing your dictionary</li> <li>DataFrame Based - Use an actual DataFrame to provide your dictionary</li> </ol> <p>with the option of running the rules against your schema (or DataFrame) via the runnerFunction parameter.</p> <p>A simpler function for just assessing known Errors against a schema are also provided:</p> <pre><code>def validate(schema: StructType, ruleSuite: RuleSuite): Set[RuleError]\n</code></pre> <p>The validation result model is as follows:</p> <p></p> <p>so the simple version returns any known Errors backed by case classes so you can pattern match as needed or just display as is via the id and errorText functions.</p> <p>Resolution of function names are run against the functionRegistry, as such you must register any UDF's or database functions before calling validate.</p>"},{"location":"advanced/validation/#what-if-i-want-to-actually-test-the-rulesuite-runs","title":"What if I want to actually test the ruleSuite runs?","text":"<pre><code>def validate(schemaOrFrame: Either[StructType, DataFrame], ruleSuite: RuleSuite, showParams: ShowParams = ShowParams(), runnerFunction: Option[DataFrame =&gt; Column] = None, qualityName: String = \"Quality\", recursiveLambdasSOEIsOk: Boolean = false, transformBeforeShow: DataFrame =&gt; DataFrame = identity): (Set[RuleError], Set[RuleWarning], String, RuleSuiteDocs, Map[Id, ExpressionLookup])\n</code></pre> <p>Given you can either use a ruleRunner or a ruleEngineRunner and set a number of parameters on those Column functions the validate runnerFunction is as simple DataFrame =&gt; Column that allows you to tweak the output.  In the case of ruleEngineRunner you could use debug mode, try with different DDL output types etc.  Use the qualityName parameter if you want to store the output in another column.  If you don't provide the runnerFunction the resulting string will be empty.</p> <p>You don't actually have to provide a DataFrame, instead using just schema will generate an empty dataset to allow Spark to resolve against.  Using a DataFrame parameter will allow you to capture the output in the resulting tuples _3 String.</p> <p>There are a number of overloaded validate arity functions to help solve common cases, they all delegate to the above function, whic also returns the documentation objects for each expression in the RuleSuite via the RuleSuiteDocs object, this provides a base for the documentation of a RuleSuite.</p>"},{"location":"advanced/validation/#what-i-want-to-change-the-dataframe-before-i-show-it","title":"What I want to change the dataframe before I show it?","text":"<p>Using the transformBeforeShow parameter you can enhance, select or filter the DataFrame before showing it.</p>"},{"location":"advanced/validation/#why-do-i-get-a-javalangabstractmethoderror-when-validating","title":"Why do I get a java.lang.AbstractMethodError when validating?","text":"<p>The validation code also validates the sql documentation, checking documented parameters against lambda parameter names (or indeed that you have any parameters when not a lambda).</p> <p>You probably have a dependency on the Scala Compiler, due to the scala compiler requiring a different parser combinator library this may occur due to classpath issues.</p> <p>To remediate please make sure that Quality is higher up on your dependencies than the scala compiler is.  If need be manually specify the parser combinator library dependency, making sure to use the same version declared in Qualities pom. </p>"},{"location":"advanced/viewLoader/","title":"View Loading","text":"<p>As of Spark 3.4 sub queries become a great way to provide lookups and transformation logic in rules.  In order to support an easier use of views the following functions have been added in 0.1.0:</p> <pre><code>val (viewConfigs, failed) = loadViewConfigs(loader, config.toDF(), expr(\"id.id\"), expr(\"id.version\"), Id(1,1),\ncol(\"name\"),col(\"token\"),col(\"filter\"),col(\"sql\")\n)\n\nval results = loadViews(viewConfigs)\n</code></pre> <p>loadViewConfigs takes a DataFrameLoader as a parameter allowing Quality to load tables based on your integration logic.  There are two flavours, one expecting a table with the following schema:</p> <pre><code>STRUCT&lt; name : STRING, token : STRING nullable, filter : STRING nullable, sql: STRING nullable&gt; </code></pre> <p>and the other tied to a RuleSuite:</p> <pre><code>STRUCT&lt; ruleSuiteId: INT, ruleSuiteVersion: INT, name : STRING, token : STRING nullable, filter : STRING nullable, sql: STRING nullable&gt; </code></pre> <p>both versions return any rows for which token and sql are both null in the failed result and the resulting configuration in viewConfigs.</p> <p>Where token is present the loader will be called for it and the filter column applied (allowing re-use).</p> <p>After loading the ViewConfig's the loadViews function can be called, registering all the views via createOrReplaceTempView and returning a set of replaced views, failedToLoadDueToCycles and notLoadedViews, a set of unloaded views.  In the event that views refer to other views not present in ViewConfig a MissingViewAnalysisException will be thrown, ViewLoaderAnalysisException for other analysis exceptions, as will parsing exceptions as per normal Spark.</p> <p>loadViews will attempt to automatically attempt to resolve ViewConfigs that depend on other ViewConfigs, where there is a cycle that is 2x the number of ViewConfigs the call will return with failedToLoadDueToCycles as true.</p> <p>These calls must be made before running any dq, engine or folder using views.</p> <p>View names must be quoted if using special characters</p> <p>A good rule of thumb is minus', dot's etc. that you wouldn't be able to use as a table name in any other sql dialect must be `quoted` in backticks. On Spark versions less than 3.2 any missing views will not contain back ticks, this can lead to situations on earlier Spark versions where views are not loaded and will result in the MissingViewAnalysisException.missingRelationNames also not having backticks returned. Quality will attempt to work around this limitation when resolving dependencies. </p>"},{"location":"background/about/","title":"History","text":""},{"location":"background/about/#why-quality","title":"Why Quality?","text":"<p>When looking at the Data Quality options for a data mesh standard runtime offering we identified gaps in the available platforms, so we asked: <pre><code>What would our Data Quality library look like? \n</code></pre> We ended up with a highly peformant and extensible row-level SQL based rule engine with low storage costs and a high degree of optimsation for both Spark and Databricks Runtimes.</p>"},{"location":"background/about/#gaps-in-existing-spark-offerings","title":"Gaps in existing Spark Offerings","text":"<p>Deequ and databricks dq were unsuitable for the meshes requirements, crucially these tools (and others such as OwlDQ) could not run at low cost with tight SLAs, typically requiring processing the data once to get DQ and then once more to save with DQ information or to handle streamed data, not too surprising given their focus on quality across large data sets rather than at a row processing level as a first class citizen.  An important use case for DQ rules within this mesh platform is the ability to filter out bad rows but also to allow the consumer of the data to decide what they filter, requiring the producers results to ideally be stored with data rows themselves.  Additionally, and perhaps most importantly, they do not support arbitrary user driven rules without recoding.</p> <p>As such our notional library needs to be:</p> <ul> <li>fast to integrate into existing Spark action without much overhead</li> <li>auditable, it should be clear which rule generated which results </li> <li>capable of handling streamed data</li> <li>capable of being scripted</li> <li>integrate with DataFrames directly, also allowing consumer driven rules in addition to upstream producer DQ</li> <li>be able to fit results into a single field (e.g. a map structure of name to results) stored with the row at time of writing the results</li> </ul>"},{"location":"background/about/#resulting-solution-space","title":"Resulting Solution Space","text":"<p>In order to execute efficiently with masses of data the calculation of data quality must scale with Spark, this requires either map functions, UDFs or better still Catalyst Expressions, enabling simple SQL to be used.  Storage of results for a row could be json, xml or using nested structures.</p> <p>The evaluation of these solutions can be found in the next sections.</p>"},{"location":"background/about/#how-did-rules-and-folder-come-about","title":"How did Rules and Folder come about?","text":"<p>Whilst developing a bookkeeping application a need for simple rules that generate an output was raised.  The initial approach taken, to effectively generate a case statement, ran into size and scale limitations.  The architect of the application asked - can you have an output sql statement for the DQ rules?  The result is QualityRules, although it should probably be called QualityCase\u2026</p> <p>QualityFolder came from a related application which had a need to transform data - providing defaulting in some circumstances - but still had to be  auditable and extensible as QualityRules was. </p>"},{"location":"background/changelog/","title":"Changelog","text":""},{"location":"background/changelog/#0131-3rd-december-2024","title":"0.1.3.1 3rd December, 2024","text":"<p>#68 - Test setup improvements for running testShades on Fabric (reduced logging and share Databricks behaviour)</p> <p>#69 - Use different scopes for OSS testShade builds for Fabric testing</p>"},{"location":"background/changelog/#013-4th-october-2024","title":"0.1.3 4th October, 2024","text":"<p>#53 - Docs parser is now more forgiving, empty descriptions are tolerated and normal scaladoc syntax is allowed</p> <p>#50 - typedExpressionRunner - audited capture of expressions with the same type</p> <p>#51 - Spark 3.5.0 support - NOTE ViewLoaderAnalysisException and MissingViewAnalysisException now have Exception causes</p> <p>#27 - Delta 3.0.0 (Spark 3.5.0) support - compatible version</p> <p>#55 - DBR 14.0/1 - Snake Yaml 2.0 support</p> <p>#58 - Migrate custom runtime usage to Shim</p> <p>#59 - DBR 13.3 LTS support</p> <p>#57 - DBR 14.3 support</p> <p>#61 - Use sparkutils frameless for 3.5, 13.3, 14.x builds - Due to encoding and shim changes this frameless fork version is not binary compatible with typelevel frameless proper </p> <p>#62 - SPARK-47509 workaround for Subqueries in lambdas - most common patterns are supported with 4.0 / 14.3 DBR</p> <p>#63 - Use actual struct functions where possible for drop_field/update_field functions - required due to 14.3 DBR introduced plan on local relations</p> <p>#66 - Bug fix - softFail result handling was double encoded - softFail result type is changed to double (breaking)</p> <p>#65 - Bug fix - Incorrect OverallResult and string result processing  </p>"},{"location":"background/changelog/#0121-4th-september-2023","title":"0.1.2.1 4th September, 2023","text":"<p>Maven Central build issues, code wise the same as 0.1.2.</p>"},{"location":"background/changelog/#012-4th-september-2023","title":"0.1.2 4th September, 2023","text":"<p>#48 - Bug fix - Enable Sub Queries in all runner types</p>"},{"location":"background/changelog/#011-9th-july-2023","title":"0.1.1 9th July, 2023","text":"<p>#42 - Improve expression runner to store yaml, unlike json, to_yaml and from_yaml allow complete support for roundtripping of Spark data types</p>"},{"location":"background/changelog/#010-10th-june-2023","title":"0.1.0 10th June, 2023","text":"<p>#29 - Quality OptimzerRule's run with Databricks sql display</p> <p>#35 - agg_expr and associated lambda support in the functions package </p> <p>#36 - improved update_field, added drop_field based on the Spark withField (3.4.1 impl)</p> <p>#34 - simplified quality package usage, column functions are now under the functions package.</p> <p>#32 - expressionRunner - saves the results of expressions as strings, suitable for aggregate statistics</p> <p>#28 - rule_result function - retrieves a rule result directly from a dq or expressionRunner result</p> <p>#15 - Addition of the loadXConfigs and loadX functions for maps and blooms, simplifying configuration management</p> <p>#24 - Remove saferId / rowid functions - use unique_id where required </p> <p>#18 - ViewLoader - simple view configuration via DataFrames  </p> <p>#30 - 3.3.2 and 3.4.1 builds - simple version bumps</p> <p>#20 - 3.5.0 starting support</p>"},{"location":"background/changelog/#003-17th-june-2023","title":"0.0.3 17th June, 2023","text":"<p>#25 - Use builtIn function registration by default - allows global views to be created using Quality functions</p>"},{"location":"background/changelog/#002-2nd-june-2023","title":"0.0.2 2nd June, 2023","text":"<p>#16 - Remove winutils requirements for testing and usage</p> <p>#13 - Support 3.4's sub query usage in rules/trigger, output expressions and lambdas </p> <p>#12 - Introduce the use of underscores instead of relying on camel case for function sql names, inline with Spark built-in functions</p> <p>#10 - Base64 functions added for RowID encoding and decoding via base64 (more suitable for BI tools)</p> <p>#9 - Add AsymmetricFilterExpressions with AsUUID and IDBase64 implementation, allows expressions used in field selects to be reversed, support added for optimiser rules through the SparkExtension </p> <p>#8 - Add set syntax for easier defaulting sql, removing duplicative cruft from intention</p> <p>#7 - SparkSessionExtension to auto register Quality functions - does not work in 2.4, starting with this release 2.4 support is deprecated</p> <p>#6 - Simple as_uuid function</p> <p>#5 - Spark 3.4 and DBR 12.2 LTS support</p> <p>#4 - comparableMaps / reverseComparableMaps functions, allowing map comparison / set operations (e.g. sort, distinct etc.)</p>"},{"location":"background/changelog/#001-8th-march-2023","title":"0.0.1 8th March, 2023","text":"<p>Initial OSS version.</p> <p>(many internal versions in between)</p>"},{"location":"background/changelog/#the-quality-exploration-starts-25th-april-2020","title":"the Quality exploration starts 25th April, 2020","text":"<p>Start of investigations into how to manage DQ more effectively within Spark and the mesh platform.</p>"},{"location":"background/evaluation_method/","title":"How should rules be evaluated?","text":"<p>Performance wise there is a clear winner as to approach for generating results:</p> <p></p> <p>The green row is using the map function which is unfortunately the most straightforward to program.  The blue is the baseline of processing a row without DQ and the orange is using withColumn.</p> <p>withColumn can use UDFs or inbuilt Catalyst style functions - the latter giving better performance and ability to more naturally integrate with spark, this review echos the findings and hinting at the effects of catalyst.</p> <p>Overall storage winner is nested columns, it has lower storage costs, is as fast as json to serialize (via an Expression) and faster to query with predicate push down support for faster filtering. Details of the analysis are below.</p> <p>Note</p> <p>Using withColumn is strongly discouraged, it very quickly introduces performance issues in spark code, prefer to use select and the Quality transform functions. A large part of the performance hit for using UDFs over Expressions is due to the conversion from user types to InternalRow - this cannot be avoided.</p>","tags":["performance"]},{"location":"background/evaluation_method/#catalyst-expression-performance","title":"Catalyst Expression Performance","text":"<p>This diagram illustrates the overhead of cost of using Expressions using a simulated complexity of rule suites with increasing number of column checks ( c here is the column number, for a simple even check ): (<code>$c</code> % 2) = 0</p> <p></p> <p>This measurement against 1k rows shows for the last column 230ms for 27 rules each with 27 columns applied, i.e. 0.23 ms per row for 84 rules total (albeit simple rules) on a single 4 core machine (24G heap).  Orange representing the default compiled evaluations.</p> <p>However, this doesn't illustrate very well how things can scale.  Running the 27 rules against 1m rows we see:</p> <p></p> <p>with a mean time of 80,562ms for 1m rows that's 0.08ms per row for 27 rules, again orange representing the default options for compilation.  Conversely, the same test run against 1m rows without rules has a mean of 14,052 - so 66,510ms overhead for processing 27m rules (i.e. 0.0025ms per simple rule).  </p> <p>Stepping the complexity up a bit to 150 columns at 100k (24G ram) with a baseline no rules time of 15,847ms.  Running with rules gives:</p> <p></p> <p>so for compiled at a mean of 174,583ms we have 15m rules run at 0.011ms per rule.  So although increased rule count obviously generates more work the overhead is still low per each rule even with larger counts and the benefit of the default (orange) compilation is visible (see the note at the bottom for when this may not be the case).</p> <p>When using RuleEngineRunners you should try to re-use output expressions (RunOnPassProcessor) wherever possible to improve performance.</p> <p>Sometimes Interpreted Is Better</p> <p>For very large complex rules (tested sample is 1k rules with over 50k expressions - over 30s compilation for a show and write) compilation can dominate time, as such you can set forceRunnerEval to true on RuleRunner and RuleEngineRunner to skip compilation. While compilation can be slow the execution is heavily optimised with minimal memory allocation, as such you should balance this out when using huge RuleSuites.</p> <p>Disabling compilation entirely is not a great idea</p> <p>Disabled generation, via <code>ruleRunner(ruleSuite, compileEvals = false, forceRunnerEval = true)</code>, takes 208,518ms for 150 rules over 100k data - 34s longer than the default, this of course adds up fast over millions of rows. </p>","tags":["performance"]},{"location":"background/storage_method/","title":"How should rule results be stored? -  JSON vs Structures","text":"<p>Note</p> <p>While Jackson is faster than circe serialization for JSON it doens't serialize easily so only used for comparison as its the fastest possible serialization framework.</p>","tags":["performance"]},{"location":"background/storage_method/#udf-created-structures","title":"UDF Created Structures","text":"<p>When serializing rule results to Nested Rows via UDF struct creation (shown as Orange) the results are very expensive, the more complex the rule setup the worse the performance. In comparison Jackson (shown as blue) keeps a low cost as it's just a string (the cost instead is in parsing, storage and filtering)</p> <p></p>","tags":["performance"]},{"location":"background/storage_method/#expression-created-structures","title":"Expression Created Structures","text":"<p>When serializing rule results with a custom Expression (shown as orange, using eval only - without custom compilation), Jackson (shown as blue) based serialisation looses it's clear lead with Expressions closing the gap as complexity increases:</p> <p></p>","tags":["performance"]},{"location":"background/storage_method/#filtering-costs","title":"Filtering Costs","text":"<p>Filtering on a nested column with deep queries (shown in red) is as expected faster the same query with a json structure.  Nested predicates can be pushed down to the underlying storage for efficient querying.</p> <p></p> <p>Note</p> <p>Depending on the Databricks runtime used the benefit from seperating the overallResult field to a top level field can be 10-20% faster.  While each new release of Spark and DBR closes this gap it is recommended to use addOverallResultsAndDetailsF to split the fields. This not only improves filter speed but also benefits with a simpler filter sql.</p>","tags":["performance"]},{"location":"background/storage_method/#structure-model-storage-costs","title":"Structure Model - storage costs","text":"<p>A naive structure representing RuleSuite, RuleSet and Rule results is actually less efficient than storage of JSON, however the current compressed model used by Quality has low overhead for even complex results.</p>","tags":["performance"]},{"location":"getting_started/","title":"Building and Setting Up","text":"","tags":["basic","getting started","beginner"]},{"location":"getting_started/#migrating-from-003-to-010","title":"Migrating from 0.0.3 to 0.1.0","text":"<p>The quality package has been trimmed down to common functionality only.  DSL / Column based functions and types have moved to specific packages similar to implicits:</p> <pre><code>import com.sparkutils.quality._\nimport functions._\nimport types._\nimport implicits._\n</code></pre> <p>The functions package aims to have an equivalent column dsl function for each bit of sql based functionality.  The notable exception to this is the lambda, callFun and _() functions, for which you are better off using your languages normal support for abstraction.  A number of the functions have been, due to naming choice, deprecated they will be removed in 0.2.0.   </p> <p>Spark 2.4 support is, as of this release, deprecated but not removed, future 0.1.x versions will continue to support but 0.2.0 will remove it entirely. </p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/#building-the-library","title":"Building The Library","text":"<ul> <li>fork, </li> <li>use the Scala dev environment of your choice,</li> <li>or build directly using Maven</li> </ul>","tags":["basic","getting started","beginner"]},{"location":"getting_started/#building-via-commandline","title":"Building via commandline","text":"<p>For OSS versions (non Databricks runtime - dbr):</p> <pre><code>mvn --batch-mode --errors --fail-at-end --show-version -DinstallAtEnd=true -DdeployAtEnd=true -DskipTests install -P Spark321\n</code></pre> <p>but dbr versions will not be able to run tests from the command line (typically not an issue in intellij):</p> <pre><code>mvn --batch-mode --errors --fail-at-end --show-version -DinstallAtEnd=true -DdeployAtEnd=true -DskipTests clean install -P 10.4.dbr\n</code></pre> <p>You may also build the shaded uber test jar for easy testing in Spark clusters for each profile:</p> <pre><code>mvn -f testShades/pom.xml --batch-mode --errors --fail-at-end --show-version -DinstallAtEnd=true -DdeployAtEnd=true -Dmaven.test.skip=true clean install -P 10.4.dbr\n</code></pre> <p>The uber test jar artefact starts with 'quality_testshade_' instead of just 'quality_' and is located in the testShades/target/ directory of a given build.  This is also true for the artefacts of a runtime build job within a full build gitlab pipeline.  All of the required jar's are shaded so you can quickly jump into using Quality in notebooks for example.</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/#running-the-tests","title":"Running the tests","text":"<p>In order to run the tests you must follow these instructions to create a fake winutils.</p> <p>Also ensure only the correct target Maven profile and source directories are enabled in your IDE of choice. </p> <p>The performance tests are not automated and must be manually run when needed.</p> <p>When running tests on jdk 17/21 you also need to add the following startup parameters:</p> <pre><code>--add-opens=java.base/java.lang=ALL-UNNAMED\n--add-opens=java.base/java.lang.invoke=ALL-UNNAMED\n--add-opens=java.base/java.lang.reflect=ALL-UNNAMED\n--add-opens=java.base/java.io=ALL-UNNAMED\n--add-opens=java.base/java.net=ALL-UNNAMED\n--add-opens=java.base/java.nio=ALL-UNNAMED\n--add-opens=java.base/java.util=ALL-UNNAMED\n--add-opens=java.base/java.util.concurrent=ALL-UNNAMED\n--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED\n--add-opens=java.base/sun.nio.ch=ALL-UNNAMED\n--add-opens=java.base/sun.nio.cs=ALL-UNNAMED\n--add-opens=java.base/sun.security.action=ALL-UNNAMED\n--add-opens=java.base/sun.util.calendar=ALL-UNNAMED\n</code></pre> <p>Also for Spark 4 builds requiring 17/21 you must use Scala SDK 2.13.12 or similar which supports higher jdk versions. </p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/#build-tool-dependencies","title":"Build tool dependencies","text":"<p>Quality is cross compiled for different versions of Spark, Scala and runtimes such as Databricks.  The format for artifact's is:</p> <pre><code>quality_RUNTIME_SPARKCOMPATVERSION_SCALACOMPATVERSION-VERSION.jar\n</code></pre> <p>e.g.</p> <pre><code>quality_3.4.1.oss_3.4_2.12-0.1.3.jar\n</code></pre> <p>The build poms generate those variables via maven profiles, but you are advised to use properties to configure e.g. for Maven:</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;com.sparkutils&lt;/groupId&gt;\n&lt;artifactId&gt;quality_${qualityRuntime}${sparkShortVersion}_${scalaCompatVersion}&lt;/artifactId&gt;\n&lt;version&gt;${qualityVersion}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>The full list of supported runtimes is below:</p> Spark Version sparkShortVersion qualityRuntime scalaCompatVersion 2.4.6 2.4 2.11 3.0.3 3.0 2.12 3.1.3 3.1 2.12 3.1.3 3.1 9.1.dbr_ 2.12 3.2.0 3.2 2.12 3.2.1 3.2 3.2.1.oss_ 2.12 3.2.1 3.2 10.4.dbr_ 2.12 3.3.2 3.3 3.3.2.oss_ 2.12 3.3.2 3.3 11.3.dbr_ 2.12 3.3.2 3.3 12.2.dbr_ 2.12 3.3.2 3.3 13.1.dbr_ 2.12 3.4.1 3.4 3.4.1.oss_ 2.12 3.4.1 3.4 13.1.dbr_ 2.12 3.4.1 3.4 13.3.dbr_ 2.12 3.5.0 3.5 3.5.0.oss_ 2.12 3.5.0 3.5 14.0.dbr_ 2.12 3.5.0 3.5 14.3.dbr_ 2.12 <p>Fabric 1.3 uses the 3.5.0.oss_ runtime, other Fabric runtimes may run on their equivalent OSS version.  Databricks 15.4 LTS is compatible with the 14.3.dbr_ runtime.</p> <p>2.4 support is deprecated and will be removed in a future version.  3.1.2 support is replaced by 3.1.3 due to interpreted encoder issues. </p> <p>Databricks 13.x support</p> <p>13.0 also works on the 12.2.dbr_ build as of 10th May 2023, despite the Spark version difference. 13.1 requires its own version as it backports 3.5 functionality.  The 13.1.dbr quality runtime build also works on 13.2 DBR.  13.3 LTS has its own runtime</p> <p>Databricks 14.x support</p> <p>Due to back-porting of SPARK-44913 frameless 0.16.0 (the 3.5.0 release) is not binary compatible with 14.2 and above which has back-ported this 4.0 interface change. Similarly, 4.0 / 14.2 introduces a change in resolution so a new runtime version is required upon a potential fix for 44913 in frameless. As such 14.3 has its own runtime</p> <p>0.1.3 Requires com.sparkutils.frameless for newer releases</p> <p>Quality 0.1.3 uses com.sparkutils.frameless for the 3.5, 13.3 and 14.x releases together with the shim project, allowing quicker releases of Databricks runtime supports going forward. The two frameless code bases are not binary compatible and will require recompilation. This may revert to org.typelevel.frameless in the future.</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/#sql-functions-vs-column-dsl","title":"Sql functions vs column dsl","text":"<p>Similar to normal Spark functions there Quality's functions have sql variants to use with select / sql or expr() and the dsl variants built around Column.</p> <p>You can use both the sql and dsl functions often without any other Quality runner usage, including lambdas.  To use the dsl functions, import quality.functions._, to use the sql functions you can either use the SparkExtension or the regsterXX functions available from the quality package.    </p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/#developing-for-a-databricks-runtime","title":"Developing for a Databricks Runtime","text":"<p>As there are many compatibility issues that Quality works around between the various Spark runtimes and their Databricks equivalents you will need to use two different runtimes when you do local testing (and of course you should do that):</p> <pre><code>&lt;properties&gt;\n&lt;qualityVersion&gt;0.1.3&lt;/qualityVersion&gt;\n&lt;qualityTestPrefix&gt;3.4.1.oss_&lt;/qualityTestPrefix&gt;\n&lt;qualityDatabricksPrefix&gt;13.1.dbr_&lt;/qualityDatabricksPrefix&gt;\n&lt;sparkShortVersion&gt;3.4&lt;/sparkShortVersion&gt;\n&lt;scalaCompatVersion&gt;2.12&lt;/scalaCompatVersion&gt;    &lt;/properties&gt;\n\n&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.sparkutils.&lt;/groupId&gt;\n&lt;artifactId&gt;quality_${qualityTestPrefix}${sparkShortVersion}_${scalaCompatVersion}&lt;/artifactId&gt;\n&lt;version&gt;${qualityVersion}&lt;/version&gt;\n&lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.sparkutils&lt;/groupId&gt;\n&lt;artifactId&gt;quality_${qualityDatabricksPrefix}${sparkShortVersion}_${scalaCompatVersion}&lt;/artifactId&gt;\n&lt;version&gt;${qualityVersion}&lt;/version&gt;\n&lt;scope&gt;compile&lt;/scope&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre> <p>That horrific looking \".\" on the test groupId is required to get Maven 3 to use different versions many thanks for finding this Zheng.</p> <p>It's safe to assume better build tools like gradle / sbt do not need such hackery. </p> <p>The known combinations requiring this approach is below:</p> Spark Version sparkShortVersion qualityTestPrefix qualityDatabricksPrefix scalaCompatVersion 3.2.1 3.2 3.2.1.oss_ 10.4.dbr_ 2.12 3.3.0 3.3 3.3.0.oss_ 11.3.dbr_ 2.12 3.3.2 3.3 3.3.2.oss_ 12.2.dbr_ 2.12 3.4.1 3.4 3.4.1.oss_ 13.1.dbr_ 2.12 3.5.0 3.5 3.5.0.oss_ 14.0.dbr_ 2.12 3.5.0 3.5 3.5.0.oss_ 14.3.dbr_ 2.12","tags":["basic","getting started","beginner"]},{"location":"getting_started/#using-the-sql-functions-on-spark-thrift-hive-servers","title":"Using the SQL functions on Spark Thrift (Hive) servers","text":"<p>Using the configuration option:</p> <pre><code>spark.sql.extensions=com.sparkutils.quality.impl.extension.QualitySparkExtension\n</code></pre> <p>when starting your cluster, with the appropriate compatible Quality runtime jars - the test Shade jar can also be used -, will automatically register the additional SQL functions from Quality.</p> <p>Spark 2.4 runtimes are not supported</p> <p>2.4 is not supported as Spark doesn't provide for SQL extensions in this version.</p> <p>Pure SQL only</p> <p>Lambdas, blooms and map's cannot be constructed via pure sql, so the functionality of these on Thrift/Hive servers is limited. </p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/#query-optimisations","title":"Query Optimisations","text":"<p>The Quality SparkExtension also provides query plan optimisers that re-write as_uuid and id_base64 usage when compared to strings.  This allows BI tools to use the results of view containing as_uuid or id_base64 strings in dashboards.  When the BI tool filters or selects on these strings passed down to the same view, the string is converted back into its underlying parts.  This allows for predicate pushdowns and other optimisations against the underlying parts instead of forcing conversions to string.</p> <p>These two currently existing optimisations are applied to joins and filters against =, &lt;=&gt;, &gt;, &gt;=, &lt;, &lt;= and \"in\".</p> <p>In order to use the query optimisations within normal job / calculator writing you must still register via spark.sql.extensions but you'll also be able to continue using the rest of the Quality functionality.  </p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/#configuring-on-databricks-runtimes","title":"Configuring on Databricks runtimes","text":"<p>In order to register the extensions on Databricks runtimes you need to additionally create a cluster init script much like:</p> <pre><code>#!/bin/bash\n\ncp /dbfs/FileStore/XXXX-quality_testshade_12_2_ver.jar /databricks/jars/quality_testshade_12_2_ver.jar\n</code></pre> <p>where the first path is your uploaded jar location.  You can create this script via a notebook on running cluster in the same workspace with throwaway code much like this:</p> <pre><code>val scriptName = \"/dbfs/add_quality_plugin.sh\"\nval script = s\"\"\"\n#!/bin/bash\n\ncp /dbfs/FileStore/XXXX-quality_testshade_12_2_ver.jar /databricks/jars/quality_testshade_12_2_ver.jar\n\"\"\"\nimport java.io._\n\nnew File(scriptName).createNewFile\nnew PrintWriter(scriptName) {write(script); close}\n</code></pre> <p>You must still register the Spark config extension attribute, but also make sure the Init script has the same path as the file you created in the above snippet.</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/#24-support-requires-246-or-janino-3016","title":"2.4 Support requires 2.4.6 or Janino 3.0.16","text":"<p>Due to Janino #90 using 2.4.5 directly will bring in 3.0.9 janino which can cause VerifyErrors, use 2.4.6 if you can't use a 3.x Spark.</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/a_first_rulesuite/","title":"Defining &amp; Running your first RuleSuite","text":"<pre><code>import com.sparkutils.quality._\n\n// setup all the Quality sql functions\nregisterQualityFunctions()\n\n// define a rule suite\nval rules = RuleSuite(rsId, Seq(\nRuleSet(Id(50, 1), Seq(\nRule(Id(100, 1), ExpressionRule(\"a % 2 == 0\")),\nRule(Id(100, 2), ExpressionRule(\"b + 20 &lt; 10\")),\nRule(Id(100, 3), ExpressionRule(\"(100 * c) + d &lt; e\"))\n)),\nRuleSet(Id(50, 2), Seq(\nRule(Id(100, 5), ExpressionRule(\"e &gt; 60 or e &lt; 30\"))...\n)),\nRuleSet(Id(50, 3), Seq(\nRule(Id(100, 9),ExpressionRule(\"i = 5\")),\n...\n))\n), Seq(\nLambdaFunction(\"isReallyNull\", \"param -&gt; isNull(param)\", Id(200,134)),\nLambdaFunction(\"isGreaterThan\", \"(a, b) -&gt; a &gt; b\", Id(201,131))\n))\n\n// add the ruleRunner expression to the DataFrame\nval withEvaluatedRulesDF = sparkSession.read.parquet(...).\nwithColumn(\"DataQuality\", ruleRunner(rules))\nwithEvaluatedRulesDF.write. ... // or show, or count, or some other action  \n</code></pre> <p>Your expressions used, in dq/triggers, output expressions (for Rules and Folder) and lambda functions can contain any valid SQL that does not include Nondeterministic functions such as rand(), uuid() or indeed the Quality random and unique_id() functions.</p> 3.4 &amp; Sub queries<p>Prior to 3.4 exists, in, and scalar subqueries (correlated or not) could not be used in any Quality rule SQL snippets.</p> <p>3.4 has allowed the use of most sub query patterns, such as checking foreign keys via an exists in a dq rule where the data is to large for maps, or selecting the maximum matching value in an output expression.  There are some oddities like you must use an alias on the input dataframe if a correlated subquery also has the same field names, not doing so results in either silent failure or at best an 'Expression \"XXX\" is not an rvalue' compilation error.  The ruleEngineWithStruct transformer will automatically add an alias of 'main' to the input dataframe.  </p> <p>Lambdas however introduce some complications, 3.4 quite reasonably had no intention of supporting the kind of thing Quality is doing, so there is code making it work for the obvious use case of DRY using row attributes.</p> <p>Spark 4.0 / 14.3 LTS introduces SPARK-47509 which limits support by blocking all possible usages.  Quality versions after 0.1.3-RC4 work around this by translating all lambda functions at call site to the direct expression.  This change has had the added benefit of allowing more complex re-use patterns but may result in more complex errors or the 47509 error.</p> <p>Per 47509, Quality enables this behaviour only when spark.sql.analyzer.allowSubqueryExpressionsInLambdasOrHigherOrderFunctions is false (the default for Spark 4) or not defined, otherwise the behaviour allows the usage as a higher order function (e.g. in transform etc.) and acts as prior to 0.1.3-RC4.</p> <pre><code>LambdaFunction(\"genMax\", \"ii -&gt; select max(i_s.i) from tableName i_s where i_s.i &gt; ii\", Id(2404,1)))\n</code></pre> <p>Calling with genMax(i) or genMax(i * 1) in an Rule or OutputExpression, where i is an column attribute will work and be translated as a join, per 47509 using it within transform will have correctness issues. </p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/a_first_rulesuite/#withcolumn-is-bad-how-else-can-i-add-columns","title":"withColumn is BAD - how else can I add columns?","text":"<p>I understand repeatedly calling withColumn/withColumnRenamed can cause performance issues due to excessive projections but how else can I add a RuleSuite in Spark?</p> <pre><code>// read a file and apply the rules storing results in the column DataQuality\nsparkSession.read.parquet(\"theFilePath\").\ntransform(addDataQualityF(rules, \"DataQuality\"))\n\n// read a file and apply the rules storing the overall result and details in the columns overallResult, dataQualityResults\nsparkSession.read.parquet(\"theFilePath\").\ntransform(addOverallResultsAndDetailsF(rules, \"overallResult\", \"dataQualityResults\"))\n</code></pre> <p>The transform functions allow easy chaining of operations on DataFrames.  However you can equally use the non \"xxxxxF\" functions such as addOverallResultsAndDetails with the same names to directly add columns and rule processing.</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/a_first_rulesuite/#filtering-the-results","title":"Filtering the Results","text":"<p>The two most common cases for running DQ rules is to report on and filter out bad rows.  Filtering can be implemented for a RuleSuiteResult with:</p> <pre><code>withEvaluatedRulesDF.filter(\"DataQuality.overallResult = passed()\")\n</code></pre> <p>Getting all of the rule results can be implemented with the flattenResults function: <pre><code>val exploded = withEvaluatedRulesDF.select(expr(\"*\"), expr(\"explode(flattenResults(DataQuality))\").\nas(\"struct\")).select(\"*\",\"struct.*\")\n</code></pre> Flatten results unpacks the resulting structure, including unpacking all the Id and Versions Ints combined into the single LongType for storage.</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/flavours/","title":"Those are some Quality flavours","text":"<p>Quality has four main flavours with sprinklings of other Quality ingredients like the sql function suite.</p> <p>These flavours are provided by four \"runners\" which add a Column to a Spark Dataset/Dataframe.</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/flavours/#quality-qualitydata-rulerunner","title":"Quality / QualityData - ruleRunner","text":"<p>Execute SQL based data validation rules, capture all the results and store them with your data for easy and fast access.</p> <p>Example Usage: Validating in-bound data or the results of a calculation.</p> <p>What is stored: </p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/flavours/#qualityrules-ruleenginerunner","title":"QualityRules - ruleEngineRunner","text":"<p>QualityRules extends the base Quality framework to provide the ability to generate output based on a single SQL rule matching the input data. Effectively an auditable large scale SQL case statement.</p> <p>Conceptually trigger rules are the when and Output rules are the then ordered by salience.</p> <p>Example Usage: Derivation Logic.</p> <p>What is stored: </p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/flavours/#qualityfolder-rulefolderrunner","title":"QualityFolder - ruleFolderRunner","text":"<p>QualityFolder extends QualityRules providing the ability to change values of attributes based on any number of SQL rules matching the input data.</p> <p>Unlike QualityRules which uses salience to select only one Output expression, Folder uses salience to order the execution of all the matching Trigger's paired Output Expressions - folding the results as it goes. </p> <p>Example Usage: Correction of in-bound data to enable subsequent calculators to process, defaulting etc.</p> <p>What is stored: </p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/flavours/#qualityexpressions-expressionrunner","title":"QualityExpressions - ExpressionRunner","text":"<p>QualityExpressions extends QualityRules providing the raw results as yaml strings (with type) for expressions and allowing aggregate expressions.</p> <p>Example Usage: Providing totals or other relevant aggregations over datasets or DQ results - e.g. only deem the data load correct when 90% of the rows have good DQ.</p> <p>What is stored:</p> <p>You can also use the typedExpressionRunner, which saves the results of expressions with the same type.</p> <p>Example Usage: Instead of checking if something exists in a view in a rule, then using the view's value in an Output expression, use typedExpressionRunner to save the lookup value directly.  The rule can check if rule_result is null, this can noticeably speed up view heavy queries.    </p> <p>What is stored: For a type of STRUCT","tags":["basic","getting started","beginner"]},{"location":"getting_started/key_functions/","title":"Key SQL Functions to use in your Rules","text":"","tags":["basic","getting started","beginner"]},{"location":"getting_started/key_functions/#expressions-with-constants","title":"Expressions with constants","text":"<ul> <li>passed() - the value representing a passed rule</li> <li>failed() - the value representing a failed rule</li> <li>soft_failed() - the value representing a failed rule which doesn't break the bank</li> <li>disabled_rule() - the value representing a rule which has been disabled and should be ignored</li> </ul>","tags":["basic","getting started","beginner"]},{"location":"getting_started/key_functions/#expressions-which-take-expression-parameters","title":"Expressions which take expression parameters","text":"<ul> <li>probability(x) - returns the probability (between 0.0 for a fail and 1.0 for pass) of a rule result</li> <li>pack_ints(lower, higher) - returns a Long with both the lower and higher int's packed in, used for id matching</li> <li>soft_fail( x ) - if the expression doesn't result in a Passed it returns softFailed() which does not trigger an overall failed() RuleSuite, this is ideal for when you want to flag a rule as passing a test you wish to query on later but do not care if it doesn't pass.  It can be treated as a \"warn\" or passed() expression.</li> <li>rule_suite_result_details( ruleSuiteResult ) - separates the RuleSuiteResult.overallResult from the rest of the structure should it be needed typically this is done via the addOverallResultsAndDetailsF</li> <li>rule_result(ruleSuiteResultColumn, packedRuleSuiteId, packedRuleSetId, packedRuleId) uses the packed long id's to retrieve the integer ruleResult (or ExpressionRunner result) or null if it can't be found.  </li> </ul>","tags":["basic","getting started","beginner"]},{"location":"getting_started/running_on_databricks/","title":"Running Quality on Databricks","text":"<p>The aim is to have explicit support for LTS', other interim versions may be supported as needed.</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/running_on_databricks/#running-31-builds-on-databricks-runtime-91-lts","title":"Running 3.1 builds on Databricks Runtime 9.1 LTS","text":"<p>Use the 9.1.dbr build / profile, the artefact name will also end with _9.1.dbr.  OSS 3.1 do not need to worry about this and should not use this profile.</p> <p>Databricks has back-ported TreePattern including the final nodePatterns in HigherOrderFunction and 3.2's Conf class.  As such very old versions of non-opensource Quality (&lt;=0.5.0) will fail with AbstractMethodError's when lambda's are used are 9.1 as the OSS binary version of HigherOrderFunction does not have nodePattern.  Similarly, the quality_testshade jar must use the 9.1.dbr version due to Conf changes.</p> <p>The 9.1.dbr build class files are built on the fake TreePattern and HigherOrderFunction present in the 9.1.dbr-scala source directory, they are however removed in the jar.</p> <p>ResolveTableValuedFunctions and ResolveCreateNamedStruct are removed from resolveWith as they are binary incompatible with OSS.  This does not seem to effect building namedstructs using resolveWith.</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/running_on_databricks/#running-321-builds-on-databricks-runtime-104","title":"Running 3.2.1 builds on Databricks Runtime 10.4","text":"<p>Use the 10.4.dbr build / profile, the artefact name will also end with _10.4.dbr.</p> <p>DBR 10.4 backports canonicalisation changes which allow Quality and any other code using explode and arrays to functionally run.  Performance is still known to be affected.  These fixes are not present in the 3.2.1 OSS release, although performance improvements may be back-ported.</p> <p>ResolveTables, ResolveAlterTableCommands and ResolveHigherOrderFunctions are removed from resolveWith as they are binary incompatible with OSS.</p> <p>Only 10.4 LTS is supported</p> <p>10.2 version support was removed in 0.0.1</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/running_on_databricks/#running-330-builds-on-databricks-runtime-113-lts","title":"Running 3.3.0 builds on Databricks Runtime 11.3 LTS","text":"<p>Use the 11.3.dbr build / profile, the artefact name will also end with _11.3.dbr.  Due to a backport of SPARK-39316 only 11.3 LTS is supported (although likely 11.2 will also run), this changed the result type of Add causing incorrect aggregation precision via aggExpr (Sum and Average stopped using Add for this reason).</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/running_on_databricks/#running-on-databricks-runtime-122-lts","title":"Running on Databricks Runtime 12.2 LTS","text":"<p>DBR 12.2 backports at least SPARK-41049 from 3.4 so the base build is closer to 3.4 than the advertised 3.3.2.  Building/Testing against 3.3.0 is the preferred approach for maximum compatibility. </p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/running_on_databricks/#running-on-databricks-runtime-130","title":"Running on Databricks Runtime 13.0","text":"<p>As of 6th June 2023 0.0.2 run against the 12.2.dbr LTS build also works on 13.0.</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/running_on_databricks/#running-on-databricks-runtime-131132","title":"Running on Databricks Runtime 13.1/13.2","text":"<p>13.1 backports a number of 3.5 oss changes, the 13.1.dbr build must be used.  The 13.1.dbr build is also successfully tested against 13.2 DBR.    </p> <p>The 13.1/2 runtimes, given the LTS version, are deprecated and will be removed in 0.1.4.</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/running_on_databricks/#running-on-databricks-runtime-133-lts","title":"Running on Databricks Runtime 13.3 LTS","text":"<p>13.3 backports yet more 3.5 so the 13.3.dbr build must be used.</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/running_on_databricks/#running-on-databricks-runtime-140141","title":"Running on Databricks Runtime 14.0/14.1","text":"<p>14.0 and 14.1 can be used with the 14.0.dbr runtime, 14.2 however is not compatible, it back-ports two changes that render Quality 0.1.3 impossible to run:</p> <ol> <li>44913 - StaticInvoke has changed breaking frameless binary compatibility</li> <li>ResolveReferences now takes catalogue as a parameter</li> </ol> <p>The 14.0/1 runtimes, given the LTS version, are deprecated and will be removed in 0.1.4.</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/running_on_databricks/#running-on-databricks-runtime-143-lts","title":"Running on Databricks Runtime 14.3 LTS","text":"<p>14.3, in addition to the 14.2 StaticInvoke and ResolveReferences changes also implements a new VarianceChecker that requires a new 14.3.dbr runtime.</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/running_on_databricks/#running-on-databricks-runtime-154-lts","title":"Running on Databricks Runtime 15.4 LTS","text":"<p>15.4 LTS is compatible with the 14.3.dbr runtime.</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/running_on_databricks/#testing-out-quality-via-notebooks","title":"Testing out Quality via Notebooks","text":"<p>You can use the appropriate runtime quality_testshade artefact jar (e.g. DBR 11.3) from maven to upload into your workspace / notebook env (or add via maven).  When using Databricks make sure to use the appropriate _Version.dbr builds.</p> <p>Then using:</p> <pre><code>import com.sparkutils.quality.tests.TestSuite\nimport com.sparkutils.qualityTests.SparkTestUtils\n\nSparkTestUtils.setPath(\"path_where_test_files_should_be_generated\")\nTestSuite.runTests\n</code></pre> <p>in your cell will run through all of the test suite used when building Quality.</p> <p>In Databricks notebooks you can set the path up via:</p> <pre><code>val fileLoc = \"/dbfs/databricks/quality_test\"\nSparkTestUtils.setPath(fileLoc)\n</code></pre> <p>Ideally at the end of your runs you'll see - after 10 minutes or so and some stdout - for example a run on DBR 15.4 provides:</p> <pre><code>...\nRunning: sequenceAsKeysDecimals(com.sparkutils.qualityTests.YamlTests), finished in: 1s\n\nTime: 633.686\n\nOK (416 tests)\n\nFinished. Result: Failures: 0. Ignored: 0. Tests run: 405. Time: 633686ms.\nimport com.sparkutils.quality.tests.TestSuite\nimport com.sparkutils.qualityTests.SparkTestUtils\nfileLoc: String = /dbfs/databricks/quality_test\n</code></pre>","tags":["basic","getting started","beginner"]},{"location":"getting_started/running_on_fabric/","title":"Running Quality on Fabric","text":"<p>Fabric support has been added since 0.1.3.1 and, at time of the 1.3 runtime, follows the OSS Spark codebase.  Other OSS stack to Synapse/Fabric runtimes may similarly \"just\" work. </p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/running_on_fabric/#running-on-fabric-13","title":"Running on Fabric 1.3","text":"<p>Use the OSS 3.5.0 build and testShades.</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/running_on_fabric/#testing-out-quality-via-notebooks","title":"Testing out Quality via Notebooks","text":"<p>This behaves the same way as per Databricks with one notable exception, System.out is not redirected so you also need:</p> <pre><code>// in case it's needed again \nval ogSysOut = System.out\nSystem.setOut(Console.out)\n</code></pre> <p>before running tests to see test progress.</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/serializing/","title":"Reading & Writing RuleSuites","text":"","tags":["basic","getting started","beginner"]},{"location":"getting_started/serializing/#reading-writing-rulesuites","title":"Reading &amp; Writing RuleSuites","text":"<p>Typically you'd save the RuleSuite in configuration tables within a Database or Delta or some other easy to edit store.</p> <p>Saving:</p> <pre><code>// The lambda functions from the RuleSuite\nval lambdaDF = toLambdaDS(rules)\nlambdaDF.write .....\n\n// The rest of the rules\nval ruleDF = toRuleSuiteDF(rules)\nruleDF.write .....\n</code></pre> <p>The field names used follow the convention of the default Product Encoder but can be renamed as desired.</p> <p>Similarly, reading the rules can be as simple as:</p> <pre><code>val rereadWithoutLambdas = readRulesFromDF(ruleDF,\ncol(\"ruleSuiteId\"),\ncol(\"ruleSuiteVersion\"),\ncol(\"ruleSetId\"),\ncol(\"ruleSetVersion\"),\ncol(\"ruleId\"),\ncol(\"ruleVersion\"),\ncol(\"ruleExpr\")\n)\n\nval reReadLambdas = readLambdasFromDF(lambdaDF.toDF(),\ncol(\"name\"),\ncol(\"ruleExpr\"),\ncol(\"functionId\"),\ncol(\"functionVersion\"),\ncol(\"ruleSuiteId\"),\ncol(\"ruleSuiteVersion\")\n)\n\nval reReadRuleSuite = integrateLambdas(rereadWithoutLambdas, reReadLambdas)\n</code></pre> <p>The column names used during reading are not assumed and must be specified.</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/serializing/#versioned-rule-datasets","title":"Versioned rule datasets","text":"<p>The user is completely free to chose their own version management approach, but the design is aimed at immutability and evidencing.</p> <p>To make things easy a simple scheme with library functions in the simpleVersioning package are provided:</p> <ol> <li> <p>Rules can be added to rulesets (or indeed new rulesets) with just a single row within the input DF, this must increase the RuleSet AND RuleSuites version:</p> ruleSuiteId ruleSuiteVersion ruleSetId ruleSetVersion ruleId ruleVersion ruleExpr 1 1 1 1 1 1 <code>/* existing rule rows */ true()</code> 1 2 1 2 2 1 <code>/* new rule */ failed()</code> </li> <li> <p>Similarly, you can change a rule by adding a new row which increments the Rule Id's, RuleSet AND RuleSuites versions:</p> ruleSuiteId ruleSuiteVersion ruleSetId ruleSetVersion ruleId ruleVersion ruleExpr 1 1 1 1 1 1 <code>/* existing rule row */ true()</code> 1 2 1 2 1 2 <code>/* new version of the above rule */ failed()</code> </li> <li> <p>To delete a rule you can either use disabled() to flag the rule is inactivated or DELETED to flag the rule to be removed from a RuleSet, as before each version must be incremented:</p> ruleSuiteId ruleSuiteVersion ruleSetId ruleSetVersion ruleId ruleVersion ruleExpr 1 1 1 1 1 1 <code>/* existing rule row */ true()</code> 1 2 1 2 1 2 DELETED </li> <li> <p>OutputExpressions may be re-used with different versions (be it for QualityRules or QualityFolder), each rule row that needs to use a later OutputExpression must increment all of it's Id versions.  You may are advised to use lambdas to soften the impact:</p> ruleSuiteId ruleSuiteVersion ruleSetId ruleSetVersion ruleId ruleVersion ruleExpr ruleEngineSalience ruleEngineId ruleEngineVersion 1 1 1 1 1 1 <code>true()</code> 60 100 1 1 2 1 2 1 2 <code>true()</code> 60 100 2 </li> <li> <p>Lambda Expressions for a RuleSuite simply take the latest version for a given lambda id.  If you want to delete a lambda (for example you have used a name that is now an official Spark sql function) you can add a DELETED row for a given RuleSuite with a higher version.</p> ruleSuiteId ruleSuiteVersion name functionId functionVersion ruleExpr 1 1 aToTrue 1 1 <code>/** oops */ a -&gt; a</code> 1 1 always1 2 1 <code>a -&gt; 1</code> 1 2 aToTrue 1 2 <code>/** corrected */ a -&gt; true()</code> 1 2 always1 2 2 DELETED </li> </ol> <p>To use these you replace the above with:</p> <pre><code>import com.sparkutils.quality._\nimport simpleVersioning._\n\nval rereadWithoutLambdas = readVersionedRulesFromDF(ruleDF,\n...\n)\n\nval reReadLambdas = readVersionedLambdasFromDF(lambdaDF.toDF(),\n...\n)\n\nval outputExpressions = readVersionedOutputExpressionsFromDF(outputDF,\n...\n)\nval rereadWithLambdas = integrateVersionedLambdas(rereadWithoutLambdas, lambdas)\nval (reread, missingOutputExpressions) = integrateVersionedOutputExpressions(rereadWithLambdas, outputExpressions)\n</code></pre> <p>The \"readVersioned\" functions modify the dataframe per the above logic to create full sets of ruleSuiteId + ruleSuiteVersion pairs.</p> <p>The \"integrateVersioned\" functions will first try the same ruleSuiteId + ruleSuiteVersion pairs and were not present will take the next lowest available version.  This runs on the assumption you if didn't need to change any OutputExpressions for a new ruleSuite version why should you need to create fake entries.</p>","tags":["basic","getting started","beginner"]},{"location":"model/","title":"Rule Model","text":"","tags":["model"]},{"location":"model/#rules","title":"Rules","text":"<p>VersionedIDs are used throughout, changes to a Rule should imply a new Rule version, a new RuleSet version and a new RuleSuite version.</p> <p>RunOnPassProcessor (output expressions) should only be provided when using the ruleEngineRunner and are treated, like Lambdas, as top level unique concepts.  You should organise using output expressions wherever possible as it's not only easier to conceptualise but it's also faster.  </p>","tags":["model"]},{"location":"model/#rule-results","title":"Rule Results","text":"<ul> <li>SoftFailed results do not cause the RuleSet or RuleSuite to fail</li> <li>DisabledRule results also do not cause the RuleSet or RuleSuite to fail but signal a rule has been disabled upstream</li> <li>Probability results with over 80 percent are deemed to have Passed, you may override this with the RuleSuite.withProbablePass function after creating the RuleSuite.</li> </ul> <p>RuleResultWithProcessor is only used when using the ruleEngineRunner and is not returned in the column, rather the result of the expression is - shown above as call to \"data\".</p>","tags":["model"]},{"location":"model/meta/","title":"Meta Rulesets?","text":"<p>Quality introduces a \"Meta Ruleset\" approach for added automation.  Meta Rule sets evaluate each column of a DataFrame to see if a Rule should be generated for that column.</p> <p>Null checks, type checks etc. may all be applied generically without laboriously copying the rule for each applicable column, just define a single argument lambda expression.  In order for this to work and be extensible you require stable ordering for each column used.</p> <pre><code>// if you wish to use Meta Rule Sets \nval metaRuleSets = readMetaRuleSetsFromDF(metaRuleDF,\n// an sql filter of the schema from a provided dataframe - name, \n//datatype (as DDL) and nullable can be filtered\ncol(\"columnFilter\"), // single arg lambda to apply to all fields from the column filter\ncol(\"ruleExpr\"), col(\"ruleSetId\"),\ncol(\"ruleSetVersion\"),\ncol(\"ruleSuiteId\"),\ncol(\"ruleSuiteVersion\")\n)\n\n// make sure we use the correct rule suites for the dataset, e.g.\nval filteredRuleSuites: RuleSuiteMap = Map(ruleSuiteId -&gt; rules)\n\nval theDataframe = sparkSession.read.parquet(\"theFilePath\")\n\n// Guarantee each column always returns the same unique position\nval stablePositionsFromColumnNames: String =&gt; Int = ???  // filter theDataframe columns and generate rules for each Meta \n//  RuleSet and re-integrate them \nval newRuleSuiteMap = integrateMetaRuleSets(theDataframe, filteredRuleSuites, metaRuleSets, stablePositionsFromColumnNames)\n</code></pre> <p>An optional last paramater for integrateMetaRuleSets allows transformation of a generated column dataframe, allowing joins with other lookup tables for the column definition or applicable rules to generate for the column for example.</p>","tags":["model"]},{"location":"model/storage/","title":"Storage Model","text":"<p>Nested columns, with nested columns, this lets you use Spark SQL to do filters and have predicate pushdown.  Sample filter:</p> <pre><code>df.select(expr(\"filter(map_values(DataQuality.ruleSetResults), \n  ruleSet -&gt; size(filter(map_values(ruleSet.ruleResults), \n  result -&gt;  probability(result) &gt; 0.3 )) &gt; 0)\").as(\"filtered\"))\n</code></pre> <p>actual type:</p> <pre><code>struct&lt;id: LongType, overallResult: IntegerType, ruleSetResults: map&lt;LongType, struct&lt;overallResult: IntegerType, ruleResults: map&lt;LongType, IntegerType&gt;&gt;&gt;&gt;\n</code></pre> <p>Alternatively when creating with addOverallResultsAndDetails you have the </p> <pre><code>overallResult: IntegerType\n</code></pre> <p>moved to the top level, leaving</p> <pre><code>details: struct&lt;id: LongType, ruleSetResults: map&lt;LongType, struct&lt;overallResult: IntegerType, ruleResults: map&lt;LongType, IntegerType&gt;&gt;&gt;&gt;\n</code></pre>","tags":["model"]},{"location":"model/storage/#where-have-all-the-versionids-and-ruleresults-gone","title":"Where have all the VersionIds and RuleResults gone?","text":"<p>In order to optimise storage and marshalling the VersionId parts are packed into a single LongType.  RuleResults are similarly encoded into an IntegerType:</p> <ul> <li>Failed =&gt; FailedInt // 0</li> <li>SoftFailed =&gt; SoftFailedInt // -1</li> <li>Disabled =&gt; DisabledInt // -2</li> <li>Passed =&gt; PassedInt // 100000</li> <li>Probability(percentage) =&gt; (percentage * PassedInt).toInt</li> </ul> <p>When the developer wishes to retrieve the objects they may use the encoders directly:</p> <pre><code>// frameless is used to encode\nimport frameless._\n// imports the encoders for RuleSuiteResult\nimport com.sparkutils.quality.implicits._\n// derive an encoder for the pair with a user type and the RuleSuiteResult for a given row\nimplicit val enc = TypedExpressionEncoder[(TestIdLeft, RuleSuiteResult)]\n// select the fields needed for the user type and the DataQuality result (or details with RuleResult, RuleSuiteResultDetails for seperate overall results and details)\nval ds = df.selectExpr(\"named_struct('left_lower', `1`, 'left_higher', `2`)\",\"DataQuality\").as[(TestIdLeft, RuleSuiteResult)]\n</code></pre> <p>the developer can then interegate the data quality results alongside their relevant data.</p>","tags":["model"]}]}