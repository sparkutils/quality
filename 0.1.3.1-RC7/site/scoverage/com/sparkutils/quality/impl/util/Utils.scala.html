<html>
      <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <title id="title">
          com/sparkutils/quality/impl/util/Utils.scala.html
        </title>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/css/theme.default.min.css" type="text/css"/><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/js/jquery.tablesorter.min.js"></script><link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css" type="text/css"/><script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script><script type="text/javascript">
        $(document).ready(function() {$(".tablesorter").tablesorter();});
      </script>
        <style>
          table.codegrid { font-family: monospace; font-size: 12px; width: auto!important; }table.statementlist { width: auto!important; font-size: 13px; } table.codegrid td { padding: 0!important; border: 0!important } table td.linenumber { width: 40px!important; } 
        </style>
      </head>
      <body style="font-family: monospace;">
        <ul class="nav nav-tabs">
          <li>
            <a href="#codegrid" data-toggle="tab">Codegrid</a>
          </li>
          <li>
            <a href="#statementlist" data-toggle="tab">Statement List</a>
          </li>
        </ul>
        <div class="tab-content">
          <div class="tab-pane active" id="codegrid">
            <pre style='font-size: 12pt; font-family: courier, monospace;'>1 <span style=''>package com.sparkutils.quality.impl.util
</span>2 <span style=''>
</span>3 <span style=''>import com.sparkutils.quality._
</span>4 <span style=''>import com.sparkutils.quality.impl.RuleLogicUtils
</span>5 <span style=''>import com.sparkutils.shim.expressions.{CreateNamedStruct1, GetStructField3}
</span>6 <span style=''>import frameless.TypedEncoder
</span>7 <span style=''>import org.apache.spark.sql.catalyst.InternalRow
</span>8 <span style=''>import org.apache.spark.sql.catalyst.expressions.codegen.{CodegenContext, CodegenFallback, ExprCode, JavaCode}
</span>9 <span style=''>import org.apache.spark.sql.catalyst.expressions.{Alias, BoundReference, Expression, If, IsNull, Literal, NamedExpression, Unevaluable, UnsafeArrayData}
</span>10 <span style=''>import org.apache.spark.sql.catalyst.util.ArrayData
</span>11 <span style=''>import org.apache.spark.sql.types.{BooleanType, DataType, StructField, StructType}
</span>12 <span style=''>
</span>13 <span style=''>import java.util.concurrent.atomic.AtomicBoolean
</span>14 <span style=''>import org.apache.spark.internal.Logging
</span>15 <span style=''>import org.apache.spark.sql.catalyst.analysis.{GetColumnByOrdinal, UnresolvedAttribute}
</span>16 <span style=''>import org.apache.spark.sql.{Encoder, ShimUtils, SparkSession}
</span>17 <span style=''>import org.apache.spark.sql.catalyst.expressions.codegen.Block.BlockHelper
</span>18 <span style=''>import org.apache.spark.sql.catalyst.expressions.objects.{InitializeJavaBean, Invoke, MapObjects, NewInstance, UnresolvedMapObjects}
</span>19 <span style=''>
</span>20 <span style=''>import scala.reflect.ClassTag
</span>21 <span style=''>
</span>22 <span style=''>object DebugTime extends Logging {
</span>23 <span style=''>
</span>24 <span style=''>  def debugTime[T](what: String, log: (Long, String)=&gt;Unit = (i, what) =&gt; {logDebug(s&quot;----&gt; ${i}ms for $what&quot;)} )( thunk: =&gt; T): T = {
</span>25 <span style=''>    val start = </span><span style='background: #AEF1AE'>System.currentTimeMillis</span><span style=''>
</span>26 <span style=''>    try {
</span>27 <span style=''>      </span><span style='background: #AEF1AE'>thunk</span><span style=''>
</span>28 <span style=''>    } finally </span><span style='background: #AEF1AE'>{
</span>29 <span style=''></span><span style='background: #AEF1AE'>      val stop = System.currentTimeMillis
</span>30 <span style=''></span><span style='background: #AEF1AE'>
</span>31 <span style=''></span><span style='background: #AEF1AE'>      log(stop - start, what)
</span>32 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>33 <span style=''>  }
</span>34 <span style=''>
</span>35 <span style=''>}
</span>36 <span style=''>
</span>37 <span style=''>trait PassThrough extends Expression {
</span>38 <span style=''>  override def nullable: Boolean = </span><span style='background: #F0ADAD'>true</span><span style=''>
</span>39 <span style=''>
</span>40 <span style=''>  override def eval(input: InternalRow): Any = </span><span style='background: #AEF1AE'>Literal(true).eval(input)</span><span style=''>
</span>41 <span style=''>
</span>42 <span style=''>  override def dataType: DataType = </span><span style='background: #AEF1AE'>BooleanType</span><span style=''>
</span>43 <span style=''>
</span>44 <span style=''>  // TODO #21 - migrate to withNewChildren when 2.4 is dropped
</span>45 <span style=''>  def withNewChilds(newChildren: IndexedSeq[Expression]): Expression
</span>46 <span style=''>}
</span>47 <span style=''>
</span>48 <span style=''>/**
</span>49 <span style=''> * Same as unevaluable but the queryplan runs.  This version requires compileEvals = true (rules are independent and
</span>50 <span style=''> * will not use Subexpression Elimination at eval time) and as such cannot be used with SubExprEvaluationRuntime
</span>51 <span style=''> * @param children
</span>52 <span style=''> */
</span>53 <span style=''>case class PassThroughCompileEvals(children: Seq[Expression]) extends PassThrough with CodegenFallback {
</span>54 <span style=''>
</span>55 <span style=''>  protected def withNewChildrenInternal(newChildren: IndexedSeq[Expression]): Expression = </span><span style='background: #AEF1AE'>copy(children = newChildren)</span><span style=''>
</span>56 <span style=''>
</span>57 <span style=''>  override def withNewChilds(newChildren: IndexedSeq[Expression]): Expression = </span><span style='background: #AEF1AE'>copy(children = newChildren)</span><span style=''>
</span>58 <span style=''>}
</span>59 <span style=''>
</span>60 <span style=''>/**
</span>61 <span style=''> * Same as unevaluable but the queryplan runs.  This version should only be used for eval (compileEvals = false) of
</span>62 <span style=''> * rules / triggers and for any output expressions, it may take part in SubExprEvaluationRuntime
</span>63 <span style=''> * @param children
</span>64 <span style=''> */
</span>65 <span style=''>case class PassThroughEvalOnly(children: Seq[Expression]) extends PassThrough {
</span>66 <span style=''>  protected def withNewChildrenInternal(newChildren: IndexedSeq[Expression]): Expression = </span><span style='background: #AEF1AE'>copy(children = newChildren)</span><span style=''>
</span>67 <span style=''>
</span>68 <span style=''>  protected def doGenCode(ctx: org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext, ev: org.apache.spark.sql.catalyst.expressions.codegen.ExprCode): org.apache.spark.sql.catalyst.expressions.codegen.ExprCode = </span><span style='background: #AEF1AE'>???</span><span style=''>
</span>69 <span style=''>
</span>70 <span style=''>  override def withNewChilds(newChildren: IndexedSeq[Expression]): Expression = </span><span style='background: #AEF1AE'>copy(children = newChildren)</span><span style=''>
</span>71 <span style=''>}
</span>72 <span style=''>
</span>73 <span style=''>/**
</span>74 <span style=''> * Should not be used in queryplanning
</span>75 <span style=''> * @param rules should be hidden from plans
</span>76 <span style=''> */
</span>77 <span style=''>case class NonPassThrough(rules: Seq[Expression]) extends Unevaluable {
</span>78 <span style=''>  override def nullable: Boolean = </span><span style='background: #AEF1AE'>true</span><span style=''>
</span>79 <span style=''>
</span>80 <span style=''>  override def dataType: DataType = </span><span style='background: #AEF1AE'>BooleanType</span><span style=''>
</span>81 <span style=''>
</span>82 <span style=''>  override def children: Seq[Expression] = </span><span style='background: #AEF1AE'>Seq(Literal(true))</span><span style=''>
</span>83 <span style=''>
</span>84 <span style=''>  protected def withNewChildrenInternal(newChildren: IndexedSeq[Expression]): Expression = this
</span>85 <span style=''>}
</span>86 <span style=''>
</span>87 <span style=''>sealed trait LookupType {
</span>88 <span style=''>  val name: String
</span>89 <span style=''>}
</span>90 <span style=''>
</span>91 <span style=''>case class MapLookupType(name: String) extends LookupType
</span>92 <span style=''>case class BloomLookupType(name: String) extends LookupType
</span>93 <span style=''>
</span>94 <span style=''>/**
</span>95 <span style=''> * Represents the results of lookups.  RuleRows will have empty expressions
</span>96 <span style=''> *
</span>97 <span style=''> * @param ruleSuite
</span>98 <span style=''> * @param ruleResults
</span>99 <span style=''> * @param lambdaResults it's not always possible to toString against an expression tree
</span>100 <span style=''> */
</span>101 <span style=''>case class LookupResults(ruleSuite: RuleSuite, ruleResults: ExpressionLookupResults[RuleRow], lambdaResults: ExpressionLookupResults[Id])
</span>102 <span style=''>
</span>103 <span style=''>case class ExpressionLookupResults[A](lookupConstants: Map[A, Set[LookupType]], lookupExpressions: Set[A])
</span>104 <span style=''>
</span>105 <span style=''>case class ExpressionLookupResult(constants: Set[LookupType], hasExpressionLookups: Boolean)
</span>106 <span style=''>
</span>107 <span style=''>
</span>108 <span style=''>object LookupIdFunctions {
</span>109 <span style=''>
</span>110 <span style=''>  def namesFromSchema(schema: StructType): Set[String] = {
</span>111 <span style=''>
</span>112 <span style=''>    def withParent(name: String, parent: String) =
</span>113 <span style=''>      if (</span><span style='background: #AEF1AE'>parent.isEmpty</span><span style=''>)
</span>114 <span style=''>        </span><span style='background: #AEF1AE'>name</span><span style=''>
</span>115 <span style=''>      else
</span>116 <span style=''>        </span><span style='background: #AEF1AE'>parent + &quot;.&quot; + name</span><span style=''>
</span>117 <span style=''>
</span>118 <span style=''>    def accumulate(set: Set[String], schema: StructType, parent: String): Set[String] =
</span>119 <span style=''>      </span><span style='background: #AEF1AE'>schema.foldLeft(set) {
</span>120 <span style=''></span><span style='background: #AEF1AE'>        (s, field) =&gt;
</span>121 <span style=''></span><span style='background: #AEF1AE'>          val name = withParent(field.name, parent)
</span>122 <span style=''></span><span style='background: #AEF1AE'>          field.dataType match {
</span>123 <span style=''></span><span style='background: #AEF1AE'>            case struct: StructType =&gt;
</span>124 <span style=''></span><span style='background: #AEF1AE'>              accumulate(s + name, struct, name)
</span>125 <span style=''></span><span style='background: #AEF1AE'>            case _ =&gt; s + name
</span>126 <span style=''></span><span style='background: #AEF1AE'>          }
</span>127 <span style=''></span><span style='background: #AEF1AE'>      }</span><span style=''>
</span>128 <span style=''>
</span>129 <span style=''>    </span><span style='background: #AEF1AE'>accumulate(Set.empty, schema, &quot;&quot;)</span><span style=''>
</span>130 <span style=''>  }
</span>131 <span style=''>
</span>132 <span style=''>  /**
</span>133 <span style=''>   * Use this function to identify which maps / blooms etc. are used by a given rulesuite
</span>134 <span style=''>   * collects all rules that are using lookup functions but without constant expressions and the list of lookups that are constants.
</span>135 <span style=''>   *
</span>136 <span style=''>   */
</span>137 <span style=''>  def identifyLookups(ruleSuite: RuleSuite): LookupResults = {
</span>138 <span style=''>    val olambdaResults =
</span>139 <span style=''>      </span><span style='background: #AEF1AE'>ruleSuite.lambdaFunctions.flatMap{r =&gt;
</span>140 <span style=''></span><span style='background: #AEF1AE'>        val exp = RuleLogicUtils.expr(r.rule)
</span>141 <span style=''></span><span style='background: #AEF1AE'>        LookupIdFunctionImpl.identifyLookups(exp).map((_,r))
</span>142 <span style=''></span><span style='background: #AEF1AE'>      }.foldLeft(ExpressionLookupResults[Id](Map.empty, Set.empty)) {
</span>143 <span style=''></span><span style='background: #AEF1AE'>        (acc, res) =&gt;
</span>144 <span style=''></span><span style='background: #AEF1AE'>          val r = acc.copy( lookupConstants = acc.lookupConstants + (res._2.id -&gt; res._1.constants))
</span>145 <span style=''></span><span style='background: #AEF1AE'>          if (res._1.hasExpressionLookups)
</span>146 <span style=''></span><span style='background: #AEF1AE'>            r.copy(lookupExpressions = r.lookupExpressions + res._2.id)
</span>147 <span style=''></span><span style='background: #AEF1AE'>          else
</span>148 <span style=''></span><span style='background: #AEF1AE'>            r
</span>149 <span style=''></span><span style='background: #AEF1AE'>      }</span><span style=''>
</span>150 <span style=''>    val lambdaResults = </span><span style='background: #AEF1AE'>olambdaResults.copy(lookupConstants = olambdaResults.lookupConstants.filter(_._2.nonEmpty))</span><span style=''>
</span>151 <span style=''>
</span>152 <span style=''>    </span><span style='background: #AEF1AE'>LookupResults(ruleSuite, ExpressionLookupResults(Map.empty, Set.empty), lambdaResults)</span><span style=''>
</span>153 <span style=''>  }
</span>154 <span style=''>}
</span>155 <span style=''>
</span>156 <span style=''>case class TSLocal[T](val initialValue: () =&gt; T) extends Serializable {
</span>157 <span style=''>  @volatile @transient private var threadLocal: ThreadLocal[T] = _
</span>158 <span style=''>  def get(): T = {
</span>159 <span style=''>    if (</span><span style='background: #AEF1AE'>threadLocal eq null</span><span style=''>) </span><span style='background: #AEF1AE'>{
</span>160 <span style=''></span><span style='background: #AEF1AE'>      val init = initialValue
</span>161 <span style=''></span><span style='background: #AEF1AE'>      this.synchronized {
</span>162 <span style=''></span><span style='background: #AEF1AE'>
</span>163 <span style=''></span><span style='background: #AEF1AE'>        threadLocal = new ThreadLocal[T] {
</span>164 <span style=''></span><span style='background: #AEF1AE'>          override def initialValue(): T = init()
</span>165 <span style=''></span><span style='background: #AEF1AE'>        }
</span>166 <span style=''></span><span style='background: #AEF1AE'>
</span>167 <span style=''></span><span style='background: #AEF1AE'>      }
</span>168 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>169 <span style=''>    </span><span style='background: #AEF1AE'>threadLocal.get()</span><span style=''>
</span>170 <span style=''>  }
</span>171 <span style=''>}
</span>172 <span style=''>
</span>173 <span style=''>case class TransientHolder[T](val initialise: () =&gt; T) extends Serializable {
</span>174 <span style=''>  @volatile @transient private var it: T = _
</span>175 <span style=''>  def get(): T = {
</span>176 <span style=''>    if (</span><span style='background: #AEF1AE'>it == null</span><span style=''>) {
</span>177 <span style=''>      </span><span style='background: #AEF1AE'>this.synchronized {
</span>178 <span style=''></span><span style='background: #AEF1AE'>
</span>179 <span style=''></span><span style='background: #AEF1AE'>        it = initialise()
</span>180 <span style=''></span><span style='background: #AEF1AE'>
</span>181 <span style=''></span><span style='background: #AEF1AE'>      }</span><span style=''>
</span>182 <span style=''>    }
</span>183 <span style=''>    </span><span style='background: #AEF1AE'>it</span><span style=''>
</span>184 <span style=''>  }
</span>185 <span style=''>  def reset: Unit ={
</span>186 <span style=''>    </span><span style='background: #F0ADAD'>this.synchronized {
</span>187 <span style=''></span><span style='background: #F0ADAD'>      it = null.asInstanceOf[T]
</span>188 <span style=''></span><span style='background: #F0ADAD'>    }</span><span style=''>
</span>189 <span style=''>  }
</span>190 <span style=''>}
</span>191 <span style=''>
</span>192 <span style=''>/**
</span>193 <span style=''> * Signifies that testing is being done, it should be ignored by users.
</span>194 <span style=''> */
</span>195 <span style=''>object Testing {
</span>196 <span style=''>  // horrible hack for testing, but at least attempt to make it performant
</span>197 <span style=''>  private val testingFlag = </span><span style='background: #AEF1AE'>new AtomicBoolean(false)</span><span style=''>
</span>198 <span style=''>
</span>199 <span style=''>  /**
</span>200 <span style=''>   * Should not be used by users but currently (0.0.2) only forces re-evaluation of the quality.lambdaHandlers configuration rather than caching once.
</span>201 <span style=''>   */
</span>202 <span style=''>  protected[quality] def setTesting() = {
</span>203 <span style=''>    </span><span style='background: #AEF1AE'>testingFlag.set(true)</span><span style=''>
</span>204 <span style=''>  }
</span>205 <span style=''>
</span>206 <span style=''>  def testing = </span><span style='background: #AEF1AE'>testingFlag.get</span><span style=''>
</span>207 <span style=''>
</span>208 <span style=''>  /**
</span>209 <span style=''>   * Should not be called by users of the library and is provided for testing support only
</span>210 <span style=''>   * @param thunk
</span>211 <span style=''>   */
</span>212 <span style=''>  def test(thunk: =&gt; Unit): Unit = try {
</span>213 <span style=''>    </span><span style='background: #AEF1AE'>setTesting()
</span>214 <span style=''></span><span style='background: #AEF1AE'>    thunk</span><span style=''>
</span>215 <span style=''>  } finally {
</span>216 <span style=''>    </span><span style='background: #AEF1AE'>testingFlag.set(false)</span><span style=''>
</span>217 <span style=''>  }
</span>218 <span style=''>}
</span>219 <span style=''>
</span>220 <span style=''>object Comparison {
</span>221 <span style=''>
</span>222 <span style=''>  /**
</span>223 <span style=''>   * Forwards to compareTo, allows for compareTo[Type] syntax with internal casts
</span>224 <span style=''>   * @param left
</span>225 <span style=''>   * @param right
</span>226 <span style=''>   * @tparam T
</span>227 <span style=''>   * @return
</span>228 <span style=''>   */
</span>229 <span style=''>  def compareTo[T &lt;: Comparable[T]](left: Any, right: Any): Int =
</span>230 <span style=''>    if (</span><span style='background: #F0ADAD'>left == null &amp;&amp; right != null</span><span style=''>)
</span>231 <span style=''>      </span><span style='background: #F0ADAD'>-100</span><span style=''>
</span>232 <span style=''>    else
</span>233 <span style=''>      </span><span style='background: #F0ADAD'>if (right == null &amp;&amp; left != null)
</span>234 <span style=''></span><span style='background: #F0ADAD'>        100
</span>235 <span style=''></span><span style='background: #F0ADAD'>      else
</span>236 <span style=''></span><span style='background: #F0ADAD'>        left.asInstanceOf[T].compareTo( right.asInstanceOf[T])</span><span style=''>
</span>237 <span style=''>
</span>238 <span style=''>  /**
</span>239 <span style=''>   * Forwards to compare, allows for compareToOrdering(ordering) syntax with internal casts
</span>240 <span style=''>   * @param left
</span>241 <span style=''>   * @param right
</span>242 <span style=''>   * @tparam T
</span>243 <span style=''>   * @return
</span>244 <span style=''>   */
</span>245 <span style=''>  def compareToOrdering[T](ordering: Ordering[T])(left: Any, right: Any): Int =
</span>246 <span style=''>    if (</span><span style='background: #AEF1AE'>left == null &amp;&amp; right != null</span><span style=''>)
</span>247 <span style=''>      </span><span style='background: #AEF1AE'>-100</span><span style=''>
</span>248 <span style=''>    else
</span>249 <span style=''>      </span><span style='background: #AEF1AE'>if (right == null &amp;&amp; left != null)
</span>250 <span style=''></span><span style='background: #AEF1AE'>        100
</span>251 <span style=''></span><span style='background: #AEF1AE'>      else
</span>252 <span style=''></span><span style='background: #AEF1AE'>        ordering.compare(left.asInstanceOf[T], right.asInstanceOf[T])</span><span style=''>
</span>253 <span style=''>
</span>254 <span style=''>}
</span>255 <span style=''>
</span>256 <span style=''>object Optional {
</span>257 <span style=''>  def toOptional[T](option: Option[T]): java.util.Optional[T] =
</span>258 <span style=''>    if (</span><span style='background: #AEF1AE'>option.isEmpty</span><span style=''>)
</span>259 <span style=''>      </span><span style='background: #AEF1AE'>java.util.Optional.empty()</span><span style=''>
</span>260 <span style=''>    else
</span>261 <span style=''>      </span><span style='background: #AEF1AE'>java.util.Optional.of(option.get)</span><span style=''>
</span>262 <span style=''>}
</span>263 <span style=''>
</span>264 <span style=''>object Arrays {
</span>265 <span style=''>  /**
</span>266 <span style=''>   * UnsafeArrayData doesn't allow calling .array, foreach when needed and for others use array
</span>267 <span style=''>   *
</span>268 <span style=''>   * @param array
</span>269 <span style=''>   * @param dataType
</span>270 <span style=''>   * @param f
</span>271 <span style=''>   * @return
</span>272 <span style=''>   */
</span>273 <span style=''>  def mapArray[T: ClassTag](array: ArrayData, dataType: DataType, f: Any =&gt; T): Array[T] =
</span>274 <span style=''>    array match {
</span>275 <span style=''>      case _: UnsafeArrayData =&gt;
</span>276 <span style=''>        val res = </span><span style='background: #AEF1AE'>Array.ofDim[T](array.numElements())</span><span style=''>
</span>277 <span style=''>        </span><span style='background: #AEF1AE'>array.foreach(dataType, (i, v) =&gt; res.update(i, f(v)))</span><span style=''>
</span>278 <span style=''>        res
</span>279 <span style=''>      case _ =&gt; </span><span style='background: #AEF1AE'>array.array.map(f)</span><span style=''>
</span>280 <span style=''>    }
</span>281 <span style=''>
</span>282 <span style=''>  /**
</span>283 <span style=''>   * gets an array out of UnsafeArrayData or others
</span>284 <span style=''>   * @param array
</span>285 <span style=''>   * @param dataType
</span>286 <span style=''>   * @return
</span>287 <span style=''>   */
</span>288 <span style=''>  def toArray(array: ArrayData, dataType: DataType): Array[Any] =
</span>289 <span style=''>    array match {
</span>290 <span style=''>      case _: UnsafeArrayData =&gt;
</span>291 <span style=''>        </span><span style='background: #AEF1AE'>mapArray(array, dataType, identity)</span><span style=''>
</span>292 <span style=''>      case _ =&gt; </span><span style='background: #AEF1AE'>array.array</span><span style=''>
</span>293 <span style=''>    }
</span>294 <span style=''>
</span>295 <span style=''>}
</span>296 <span style=''>
</span>297 <span style=''>/**
</span>298 <span style=''> * With the introduction of the 4 runtime folder needs different
</span>299 <span style=''> * resolved behaviour on lazytyperef, as such these move here
</span>300 <span style=''> * from TestUtils
</span>301 <span style=''> */
</span>302 <span style=''>object SparkVersions {
</span>303 <span style=''>
</span>304 <span style=''>  lazy val sparkFullVersion = {
</span>305 <span style=''>    val pos = classOf[Expression].getPackage.getSpecificationVersion
</span>306 <span style=''>    if ((pos eq null) || pos == &quot;0.0&quot;) // DBR is always null, Fabric 0.0
</span>307 <span style=''>      SparkSession.active.version
</span>308 <span style=''>    else
</span>309 <span style=''>      pos
</span>310 <span style=''>  }
</span>311 <span style=''>
</span>312 <span style=''>  lazy val sparkVersion = sparkFullVersion.split('.').take(2).mkString(&quot;.&quot;)
</span>313 <span style=''>
</span>314 <span style=''>  lazy val sparkMajorVersion = sparkFullVersion.split('.').head
</span>315 <span style=''>}
</span>316 <span style=''>
</span>317 <span style=''>/**
</span>318 <span style=''> * Frameless sets path in foldable encoders to nullable == false, but it really is nullable
</span>319 <span style=''> * Spark then just accesses the struct which is null.  This forces codegen only
</span>320 <span style=''> */
</span>321 <span style=''>case class ForceNullable(child: Expression) extends Expression {
</span>322 <span style=''>
</span>323 <span style=''>  val children = </span><span style='background: #AEF1AE'>Seq(child)</span><span style=''>
</span>324 <span style=''>
</span>325 <span style=''>  override def nullable: Boolean = </span><span style='background: #AEF1AE'>true</span><span style=''>
</span>326 <span style=''>
</span>327 <span style=''>  override def eval(input: InternalRow): Any = </span><span style='background: #AEF1AE'>child.eval(input)</span><span style=''>
</span>328 <span style=''>
</span>329 <span style=''>  override protected def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {
</span>330 <span style=''>    val c = </span><span style='background: #AEF1AE'>child.genCode(ctx)</span><span style=''>
</span>331 <span style=''>    val typ = </span><span style='background: #AEF1AE'>JavaCode.javaType(dataType)</span><span style=''>
</span>332 <span style=''>    val boxed = </span><span style='background: #AEF1AE'>JavaCode.boxedType(dataType)</span><span style=''>
</span>333 <span style=''>    </span><span style='background: #AEF1AE'>ev.copy(code =
</span>334 <span style=''></span><span style='background: #AEF1AE'>      code&quot;&quot;&quot;
</span>335 <span style=''></span><span style='background: #AEF1AE'>            ${c.code}
</span>336 <span style=''></span><span style='background: #AEF1AE'>            boolean ${ev.isNull} = true;
</span>337 <span style=''></span><span style='background: #AEF1AE'>            $typ ${ev.value} = null;
</span>338 <span style=''></span><span style='background: #AEF1AE'>            if (${c.value} != null) {
</span>339 <span style=''></span><span style='background: #AEF1AE'>              ${ev.isNull} = false;
</span>340 <span style=''></span><span style='background: #AEF1AE'>              ${ev.value} = ($boxed) ${c.value};
</span>341 <span style=''></span><span style='background: #AEF1AE'>            }
</span>342 <span style=''></span><span style='background: #AEF1AE'>            &quot;&quot;&quot;)</span><span style=''>
</span>343 <span style=''>  }
</span>344 <span style=''>
</span>345 <span style=''>
</span>346 <span style=''>  override def dataType: DataType = </span><span style='background: #AEF1AE'>child.dataType</span><span style=''>
</span>347 <span style=''>
</span>348 <span style=''>  protected def withNewChildrenInternal(newChildren: IndexedSeq[Expression]): Expression =
</span>349 <span style=''>    </span><span style='background: #AEF1AE'>copy(child = newChildren.head)</span><span style=''>
</span>350 <span style=''>}
</span>351 <span style=''>
</span>352 <span style=''>object Encoding {
</span>353 <span style=''>
</span>354 <span style=''>  /**
</span>355 <span style=''>   * Wraps a non Frameless encoder in a TypedEncoder, adjusting paths as needed.
</span>356 <span style=''>   *
</span>357 <span style=''>   * This is not intended for general use and is used by the ProcessFunctions.
</span>358 <span style=''>   *
</span>359 <span style=''>   * @param outputType
</span>360 <span style=''>   * @tparam T
</span>361 <span style=''>   * @return
</span>362 <span style=''>   */
</span>363 <span style=''>  def fromNormalEncoder[T: Encoder](outputType: DataType): TypedEncoder[T] = {
</span>364 <span style=''>    val oexpr = </span><span style='background: #AEF1AE'>ShimUtils.expressionEncoder(implicitly[Encoder[T]])</span><span style=''>
</span>365 <span style=''>
</span>366 <span style=''>    implicit val cltag = </span><span style='background: #AEF1AE'>oexpr.clsTag</span><span style=''>
</span>367 <span style=''>
</span>368 <span style=''>    </span><span style='background: #AEF1AE'>new</span><span style=''> TypedEncoder[T] {
</span>369 <span style=''>
</span>370 <span style=''>      override def nullable: Boolean = </span><span style='background: #AEF1AE'>true</span><span style=''>
</span>371 <span style=''>
</span>372 <span style=''>      override def jvmRepr: DataType = </span><span style='background: #AEF1AE'>oexpr.deserializer.dataType</span><span style=''>
</span>373 <span style=''>
</span>374 <span style=''>      override def catalystRepr: DataType = {
</span>375 <span style=''>        val se = </span><span style='background: #AEF1AE'>oexpr.serializer</span><span style=''>
</span>376 <span style=''>        if (</span><span style='background: #AEF1AE'>se.length == 1</span><span style=''>)
</span>377 <span style=''>          </span><span style='background: #AEF1AE'>se.head.dataType</span><span style=''>
</span>378 <span style=''>        else
</span>379 <span style=''>          </span><span style='background: #AEF1AE'>StructType( // 2.4 only cast
</span>380 <span style=''></span><span style='background: #AEF1AE'>            se.map(n =&gt; StructField(n.asInstanceOf[NamedExpression].qualifiedName, n.dataType, n.nullable))
</span>381 <span style=''></span><span style='background: #AEF1AE'>          )</span><span style=''>
</span>382 <span style=''>      }
</span>383 <span style=''>
</span>384 <span style=''>      override def fromCatalyst(path: Expression): Expression = {
</span>385 <span style=''>        val de = </span><span style='background: #AEF1AE'>oexpr.deserializer</span><span style=''>
</span>386 <span style=''>        val r =
</span>387 <span style=''>          de match {
</span>388 <span style=''>            case a: Alias =&gt;
</span>389 <span style=''>              </span><span style='background: #AEF1AE'>a.child</span><span style=''> match {
</span>390 <span style=''>                case m: UnresolvedMapObjects =&gt; </span><span style='background: #AEF1AE'>a.withNewChildren(Seq( m.copy(child = path) ))</span><span style=''>
</span>391 <span style=''>                case a =&gt; </span><span style='background: #AEF1AE'>a.transformUp {
</span>392 <span style=''></span><span style='background: #AEF1AE'>                  case _: GetColumnByOrdinal =&gt;
</span>393 <span style=''></span><span style='background: #AEF1AE'>                    path
</span>394 <span style=''></span><span style='background: #AEF1AE'>                }</span><span style=''>
</span>395 <span style=''>              }
</span>396 <span style=''>            case m: UnresolvedMapObjects =&gt; </span><span style='background: #AEF1AE'>m.copy(child = path)</span><span style=''>
</span>397 <span style=''>            case n: NewInstance =&gt;
</span>398 <span style=''>              val o = </span><span style='background: #AEF1AE'>outputType.asInstanceOf[StructType].zipWithIndex.map{case (e,i) =&gt; e.name -&gt; i }.toMap</span><span style=''>
</span>399 <span style=''>
</span>400 <span style=''>              </span><span style='background: #AEF1AE'>If(IsNull(ForceNullable(path)), Literal(null),
</span>401 <span style=''></span><span style='background: #AEF1AE'>                n.withNewChildren(n.children map {
</span>402 <span style=''></span><span style='background: #AEF1AE'>                  _.transform {
</span>403 <span style=''></span><span style='background: #AEF1AE'>                    case u: UnresolvedAttribute if o.contains(u.name) =&gt;
</span>404 <span style=''></span><span style='background: #AEF1AE'>                      GetStructField3(path, o(u.name))
</span>405 <span style=''></span><span style='background: #AEF1AE'>                  }
</span>406 <span style=''></span><span style='background: #AEF1AE'>                })
</span>407 <span style=''></span><span style='background: #AEF1AE'>              )</span><span style=''>
</span>408 <span style=''>            case i: InitializeJavaBean =&gt;
</span>409 <span style=''>              val o = </span><span style='background: #AEF1AE'>outputType.asInstanceOf[StructType].zipWithIndex.map{case (e,i) =&gt; e.name -&gt; i }.toMap</span><span style=''>
</span>410 <span style=''>
</span>411 <span style=''>              </span><span style='background: #AEF1AE'>If(IsNull(ForceNullable(path)), Literal(null),
</span>412 <span style=''></span><span style='background: #AEF1AE'>                i.copy(setters =
</span>413 <span style=''></span><span style='background: #AEF1AE'>                  i.setters.map{ p =&gt;
</span>414 <span style=''></span><span style='background: #AEF1AE'>                    (p._1, p._2.transform {
</span>415 <span style=''></span><span style='background: #AEF1AE'>                      case u: UnresolvedAttribute if o.contains(u.name) =&gt;
</span>416 <span style=''></span><span style='background: #AEF1AE'>                        GetStructField3(path, o(u.name))
</span>417 <span style=''></span><span style='background: #AEF1AE'>                    })
</span>418 <span style=''></span><span style='background: #AEF1AE'>                  }
</span>419 <span style=''></span><span style='background: #AEF1AE'>                )
</span>420 <span style=''></span><span style='background: #AEF1AE'>              )</span><span style=''>
</span>421 <span style=''>            // all single fields from a struct
</span>422 <span style=''>            case i: Invoke =&gt;
</span>423 <span style=''>              </span><span style='background: #AEF1AE'>i.transformUp {
</span>424 <span style=''></span><span style='background: #AEF1AE'>                case _: GetColumnByOrdinal =&gt;
</span>425 <span style=''></span><span style='background: #AEF1AE'>                  path
</span>426 <span style=''></span><span style='background: #AEF1AE'>              }</span><span style=''>
</span>427 <span style=''>            case a =&gt; </span><span style='background: #AEF1AE'>a.transformUp {
</span>428 <span style=''></span><span style='background: #AEF1AE'>              case _: GetColumnByOrdinal =&gt;
</span>429 <span style=''></span><span style='background: #AEF1AE'>                path
</span>430 <span style=''></span><span style='background: #AEF1AE'>            }</span><span style=''>
</span>431 <span style=''>          }
</span>432 <span style=''>        r
</span>433 <span style=''>
</span>434 <span style=''>      }
</span>435 <span style=''>
</span>436 <span style=''>      // only used by resolveAndBind
</span>437 <span style=''>      override def toCatalyst(path: Expression): Expression = {
</span>438 <span style=''>        val se = </span><span style='background: #AEF1AE'>oexpr.serializer</span><span style=''>
</span>439 <span style=''>        if (</span><span style='background: #AEF1AE'>se.length == 1</span><span style=''>)
</span>440 <span style=''>          </span><span style='background: #AEF1AE'>se.head match {
</span>441 <span style=''></span><span style='background: #AEF1AE'>            case a: Alias =&gt;
</span>442 <span style=''></span><span style='background: #AEF1AE'>              a.child match {
</span>443 <span style=''></span><span style='background: #AEF1AE'>                case m: MapObjects =&gt; a.withNewChildren(Seq( m.copy(inputData = path) ))
</span>444 <span style=''></span><span style='background: #AEF1AE'>                case a =&gt; a.transformUp {
</span>445 <span style=''></span><span style='background: #AEF1AE'>                  case b: BoundReference =&gt; path
</span>446 <span style=''></span><span style='background: #AEF1AE'>                }
</span>447 <span style=''></span><span style='background: #AEF1AE'>              }
</span>448 <span style=''></span><span style='background: #AEF1AE'>            case m: MapObjects =&gt; m.copy(inputData = path)
</span>449 <span style=''></span><span style='background: #AEF1AE'>            case a =&gt; a.transformUp {
</span>450 <span style=''></span><span style='background: #AEF1AE'>              case b: BoundReference =&gt; path
</span>451 <span style=''></span><span style='background: #AEF1AE'>            }
</span>452 <span style=''></span><span style='background: #AEF1AE'>          }</span><span style=''>
</span>453 <span style=''>        else </span><span style='background: #AEF1AE'>{
</span>454 <span style=''></span><span style='background: #AEF1AE'>          val o = outputType.asInstanceOf[StructType]
</span>455 <span style=''></span><span style='background: #AEF1AE'>
</span>456 <span style=''></span><span style='background: #AEF1AE'>          val dealiased = se.map {
</span>457 <span style=''></span><span style='background: #AEF1AE'>            case a: Alias =&gt;
</span>458 <span style=''></span><span style='background: #AEF1AE'>              a.name -&gt; a.child.transformUp {
</span>459 <span style=''></span><span style='background: #AEF1AE'>                case b: BoundReference =&gt; path
</span>460 <span style=''></span><span style='background: #AEF1AE'>              }
</span>461 <span style=''></span><span style='background: #AEF1AE'>          }.toMap
</span>462 <span style=''></span><span style='background: #AEF1AE'>
</span>463 <span style=''></span><span style='background: #AEF1AE'>          CreateNamedStruct1(
</span>464 <span style=''></span><span style='background: #AEF1AE'>            o.fields.map(f =&gt; f.name -&gt; dealiased(f.name)).flatMap {
</span>465 <span style=''></span><span style='background: #AEF1AE'>              case (name, e) =&gt;
</span>466 <span style=''></span><span style='background: #AEF1AE'>                Seq[Expression](Literal(name), e)
</span>467 <span style=''></span><span style='background: #AEF1AE'>            }
</span>468 <span style=''></span><span style='background: #AEF1AE'>          )
</span>469 <span style=''></span><span style='background: #AEF1AE'>        }</span><span style=''>
</span>470 <span style=''>      }
</span>471 <span style=''>    }
</span>472 <span style=''>
</span>473 <span style=''>  }
</span>474 <span style=''>
</span>475 <span style=''>}
</span>476 <span style=''>
</span>477 <span style=''>/**
</span>478 <span style=''> * wrap subexprs so we can correctly identify the subquery post bindreferences
</span>479 <span style=''> * @param children
</span>480 <span style=''> */
</span>481 <span style=''>case class SubQueryWrapper(children: Seq[Expression]) extends Expression {
</span>482 <span style=''>
</span>483 <span style=''>  override def nullable: Boolean = </span><span style='background: #AEF1AE'>children.head.nullable</span><span style=''>
</span>484 <span style=''>  override def foldable: Boolean = </span><span style='background: #AEF1AE'>false</span><span style=''>
</span>485 <span style=''>  override protected def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {
</span>486 <span style=''>    val expr = </span><span style='background: #AEF1AE'>children.head.genCode(ctx)</span><span style=''>
</span>487 <span style=''>    expr
</span>488 <span style=''>  }
</span>489 <span style=''>
</span>490 <span style=''>  override def eval(input: InternalRow): Any = </span><span style='background: #AEF1AE'>children.head.eval(input)</span><span style=''>
</span>491 <span style=''>
</span>492 <span style=''>  override def dataType: DataType = </span><span style='background: #AEF1AE'>children.head.dataType</span><span style=''>
</span>493 <span style=''>
</span>494 <span style=''>  protected def withNewChildrenInternal(newChildren: IndexedSeq[Expression]): Expression = </span><span style='background: #AEF1AE'>copy(children = newChildren)</span><span style=''>
</span>495 <span style=''>}
</span>496 <span style=''>
</span>497 <span style=''>object SubQueryWrapper {
</span>498 <span style=''>  def hasASubQuery(expr: Expression): Boolean =
</span>499 <span style=''>    (</span><span style='background: #AEF1AE'>expr.collectFirst {
</span>500 <span style=''></span><span style='background: #AEF1AE'>      case s: SubQueryWrapper =&gt; s
</span>501 <span style=''></span><span style='background: #AEF1AE'>    }.isDefined</span><span style=''>)
</span>502 <span style=''>}
</span></pre>
          </div>
          <div class="tab-pane" id="statementlist">
            <table cellspacing="0" cellpadding="0" class="table statementlist">
      <tr>
        <th>Line</th>
        <th>Stmt Id</th>
        <th>Pos</th>
        <th>Tree</th>
        <th>Symbol</th>
        <th>Tests</th>
        <th>Code</th>
      </tr><tr>
        <td>
          25
        </td>
        <td>
          7299
        </td>
        <td>
          1348
          -
          1372
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.System.currentTimeMillis
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          java.lang.System.currentTimeMillis()
        </td>
      </tr><tr>
        <td>
          27
        </td>
        <td>
          7300
        </td>
        <td>
          1389
          -
          1394
        </td>
        <td>
          Ident
        </td>
        <td>
          com.sparkutils.quality.impl.util.DebugTime.thunk
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          thunk
        </td>
      </tr><tr>
        <td>
          28
        </td>
        <td>
          7304
        </td>
        <td>
          1409
          -
          1489
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  val stop: Long = java.lang.System.currentTimeMillis();
  log.apply(stop.-(start), what)
}
        </td>
      </tr><tr>
        <td>
          29
        </td>
        <td>
          7301
        </td>
        <td>
          1428
          -
          1452
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.System.currentTimeMillis
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          java.lang.System.currentTimeMillis()
        </td>
      </tr><tr>
        <td>
          31
        </td>
        <td>
          7302
        </td>
        <td>
          1464
          -
          1476
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Long.-
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          stop.-(start)
        </td>
      </tr><tr>
        <td>
          31
        </td>
        <td>
          7303
        </td>
        <td>
          1460
          -
          1483
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          log.apply(stop.-(start), what)
        </td>
      </tr><tr>
        <td>
          38
        </td>
        <td>
          7305
        </td>
        <td>
          1572
          -
          1576
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          true
        </td>
      </tr><tr>
        <td>
          40
        </td>
        <td>
          7306
        </td>
        <td>
          1625
          -
          1650
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Literal.eval
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Literal.apply(true).eval(input)
        </td>
      </tr><tr>
        <td>
          42
        </td>
        <td>
          7307
        </td>
        <td>
          1688
          -
          1699
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.BooleanType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.BooleanType
        </td>
      </tr><tr>
        <td>
          55
        </td>
        <td>
          7308
        </td>
        <td>
          2290
          -
          2318
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.PassThroughCompileEvals.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          PassThroughCompileEvals.this.copy(newChildren)
        </td>
      </tr><tr>
        <td>
          57
        </td>
        <td>
          7309
        </td>
        <td>
          2400
          -
          2428
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.PassThroughCompileEvals.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          PassThroughCompileEvals.this.copy(newChildren)
        </td>
      </tr><tr>
        <td>
          66
        </td>
        <td>
          7310
        </td>
        <td>
          2843
          -
          2871
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.PassThroughEvalOnly.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          PassThroughEvalOnly.this.copy(newChildren)
        </td>
      </tr><tr>
        <td>
          68
        </td>
        <td>
          7311
        </td>
        <td>
          3096
          -
          3099
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Predef.???
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.???
        </td>
      </tr><tr>
        <td>
          70
        </td>
        <td>
          7312
        </td>
        <td>
          3181
          -
          3209
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.PassThroughEvalOnly.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          PassThroughEvalOnly.this.copy(newChildren)
        </td>
      </tr><tr>
        <td>
          78
        </td>
        <td>
          7313
        </td>
        <td>
          3411
          -
          3415
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          80
        </td>
        <td>
          7314
        </td>
        <td>
          3453
          -
          3464
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.BooleanType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.BooleanType
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          7316
        </td>
        <td>
          3509
          -
          3527
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Literal](org.apache.spark.sql.catalyst.expressions.Literal.apply(true))
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          7315
        </td>
        <td>
          3513
          -
          3526
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Literal.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Literal.apply(true)
        </td>
      </tr><tr>
        <td>
          113
        </td>
        <td>
          7317
        </td>
        <td>
          4503
          -
          4517
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.isEmpty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          parent.isEmpty()
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          7318
        </td>
        <td>
          4527
          -
          4531
        </td>
        <td>
          Ident
        </td>
        <td>
          com.sparkutils.quality.impl.util.LookupIdFunctions.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          name
        </td>
      </tr><tr>
        <td>
          116
        </td>
        <td>
          7320
        </td>
        <td>
          4551
          -
          4570
        </td>
        <td>
          Block
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          parent.+(&quot;.&quot;).+(name)
        </td>
      </tr><tr>
        <td>
          116
        </td>
        <td>
          7319
        </td>
        <td>
          4551
          -
          4570
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          parent.+(&quot;.&quot;).+(name)
        </td>
      </tr><tr>
        <td>
          119
        </td>
        <td>
          7327
        </td>
        <td>
          4666
          -
          4934
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.foldLeft
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          schema.foldLeft[Set[String]](set)(((s: Set[String], field: org.apache.spark.sql.types.StructField) =&gt; {
  val name: String = withParent(field.name, parent);
  field.dataType match {
    case (struct @ (_: org.apache.spark.sql.types.StructType)) =&gt; accumulate(s.+(name), struct, name)
    case _ =&gt; s.+(name)
  }
}))
        </td>
      </tr><tr>
        <td>
          121
        </td>
        <td>
          7322
        </td>
        <td>
          4732
          -
          4762
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.LookupIdFunctions.withParent
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          withParent(field.name, parent)
        </td>
      </tr><tr>
        <td>
          121
        </td>
        <td>
          7321
        </td>
        <td>
          4743
          -
          4753
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          field.name
        </td>
      </tr><tr>
        <td>
          122
        </td>
        <td>
          7323
        </td>
        <td>
          4773
          -
          4787
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          field.dataType
        </td>
      </tr><tr>
        <td>
          124
        </td>
        <td>
          7325
        </td>
        <td>
          4849
          -
          4883
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.LookupIdFunctions.accumulate
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          accumulate(s.+(name), struct, name)
        </td>
      </tr><tr>
        <td>
          124
        </td>
        <td>
          7324
        </td>
        <td>
          4860
          -
          4868
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SetLike.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          s.+(name)
        </td>
      </tr><tr>
        <td>
          125
        </td>
        <td>
          7326
        </td>
        <td>
          4906
          -
          4914
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SetLike.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          s.+(name)
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          7329
        </td>
        <td>
          4970
          -
          4972
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          7328
        </td>
        <td>
          4951
          -
          4960
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.generic.ImmutableSetFactory.empty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.empty[String]
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          7330
        </td>
        <td>
          4940
          -
          4973
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.LookupIdFunctions.accumulate
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          accumulate(scala.Predef.Set.empty[String], schema, &quot;&quot;)
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          7336
        </td>
        <td>
          5346
          -
          5346
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[(com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction)]
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          7332
        </td>
        <td>
          5370
          -
          5397
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleLogicUtils.expr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.RuleLogicUtils.expr(r.rule)
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          7331
        </td>
        <td>
          5390
          -
          5396
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.HasRuleText.rule
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          r.rule
        </td>
      </tr><tr>
        <td>
          141
        </td>
        <td>
          7335
        </td>
        <td>
          5406
          -
          5458
        </td>
        <td>
          ApplyImplicitView
        </td>
        <td>
          scala.Option.option2Iterable
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.this.Option.option2Iterable[(com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction)](LookupIdFunctionImpl.identifyLookups(exp).map[(com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction)](((x$1: com.sparkutils.quality.impl.util.ExpressionLookupResult) =&gt; scala.Tuple2.apply[com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction](x$1, r))))
        </td>
      </tr><tr>
        <td>
          141
        </td>
        <td>
          7334
        </td>
        <td>
          5406
          -
          5458
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Option.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          LookupIdFunctionImpl.identifyLookups(exp).map[(com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction)](((x$1: com.sparkutils.quality.impl.util.ExpressionLookupResult) =&gt; scala.Tuple2.apply[com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction](x$1, r)))
        </td>
      </tr><tr>
        <td>
          141
        </td>
        <td>
          7333
        </td>
        <td>
          5452
          -
          5457
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction](x$1, r)
        </td>
      </tr><tr>
        <td>
          142
        </td>
        <td>
          7353
        </td>
        <td>
          5313
          -
          5803
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.foldLeft
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ruleSuite.lambdaFunctions.flatMap[(com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction), Seq[(com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction)]](((r: com.sparkutils.quality.impl.LambdaFunction) =&gt; {
  val exp: org.apache.spark.sql.catalyst.expressions.Expression = com.sparkutils.quality.impl.RuleLogicUtils.expr(r.rule);
  scala.this.Option.option2Iterable[(com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction)](LookupIdFunctionImpl.identifyLookups(exp).map[(com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction)](((x$1: com.sparkutils.quality.impl.util.ExpressionLookupResult) =&gt; scala.Tuple2.apply[com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction](x$1, r))))
}))(collection.this.Seq.canBuildFrom[(com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction)]).foldLeft[com.sparkutils.quality.impl.util.ExpressionLookupResults[com.sparkutils.quality.Id]](ExpressionLookupResults.apply[com.sparkutils.quality.Id](scala.Predef.Map.empty[com.sparkutils.quality.Id, Nothing], scala.Predef.Set.empty[com.sparkutils.quality.Id]))(((acc: com.sparkutils.quality.impl.util.ExpressionLookupResults[com.sparkutils.quality.Id], res: (com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction)) =&gt; {
  val r: com.sparkutils.quality.impl.util.ExpressionLookupResults[com.sparkutils.quality.Id] = acc.copy[com.sparkutils.quality.Id](acc.lookupConstants.+[Set[com.sparkutils.quality.impl.util.LookupType]](scala.Predef.ArrowAssoc[com.sparkutils.quality.Id](res._2.id).-&gt;[Set[com.sparkutils.quality.impl.util.LookupType]](res._1.constants)), acc.copy$default$2[Nothing]);
  if (res._1.hasExpressionLookups)
    {
      &lt;artifact&gt; val x$1: scala.collection.immutable.Set[com.sparkutils.quality.Id] @scala.reflect.internal.annotations.uncheckedBounds = r.lookupExpressions.+(res._2.id);
      &lt;artifact&gt; val x$2: Map[com.sparkutils.quality.Id,Set[com.sparkutils.quality.impl.util.LookupType]] @scala.reflect.internal.annotations.uncheckedBounds = r.copy$default$1[Nothing];
      r.copy[com.sparkutils.quality.Id](x$2, x$1)
    }
  else
    r
}))
        </td>
      </tr><tr>
        <td>
          142
        </td>
        <td>
          7338
        </td>
        <td>
          5515
          -
          5524
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.generic.ImmutableSetFactory.empty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.empty[com.sparkutils.quality.Id]
        </td>
      </tr><tr>
        <td>
          142
        </td>
        <td>
          7337
        </td>
        <td>
          5504
          -
          5513
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.Map.empty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Map.empty[com.sparkutils.quality.Id, Nothing]
        </td>
      </tr><tr>
        <td>
          142
        </td>
        <td>
          7339
        </td>
        <td>
          5476
          -
          5525
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ExpressionLookupResults.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ExpressionLookupResults.apply[com.sparkutils.quality.Id](scala.Predef.Map.empty[com.sparkutils.quality.Id, Nothing], scala.Predef.Set.empty[com.sparkutils.quality.Id])
        </td>
      </tr><tr>
        <td>
          144
        </td>
        <td>
          7341
        </td>
        <td>
          5633
          -
          5649
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.util.ExpressionLookupResult.constants
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          res._1.constants
        </td>
      </tr><tr>
        <td>
          144
        </td>
        <td>
          7344
        </td>
        <td>
          5573
          -
          5573
        </td>
        <td>
          TypeApply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ExpressionLookupResults.copy$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          acc.copy$default$2[Nothing]
        </td>
      </tr><tr>
        <td>
          144
        </td>
        <td>
          7343
        </td>
        <td>
          5597
          -
          5650
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.immutable.Map.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          acc.lookupConstants.+[Set[com.sparkutils.quality.impl.util.LookupType]](scala.Predef.ArrowAssoc[com.sparkutils.quality.Id](res._2.id).-&gt;[Set[com.sparkutils.quality.impl.util.LookupType]](res._1.constants))
        </td>
      </tr><tr>
        <td>
          144
        </td>
        <td>
          7340
        </td>
        <td>
          5620
          -
          5629
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.LambdaFunction.id
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          res._2.id
        </td>
      </tr><tr>
        <td>
          144
        </td>
        <td>
          7342
        </td>
        <td>
          5620
          -
          5649
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Predef.ArrowAssoc.-&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.ArrowAssoc[com.sparkutils.quality.Id](res._2.id).-&gt;[Set[com.sparkutils.quality.impl.util.LookupType]](res._1.constants)
        </td>
      </tr><tr>
        <td>
          144
        </td>
        <td>
          7345
        </td>
        <td>
          5569
          -
          5651
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ExpressionLookupResults.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          acc.copy[com.sparkutils.quality.Id](acc.lookupConstants.+[Set[com.sparkutils.quality.impl.util.LookupType]](scala.Predef.ArrowAssoc[com.sparkutils.quality.Id](res._2.id).-&gt;[Set[com.sparkutils.quality.impl.util.LookupType]](res._1.constants)), acc.copy$default$2[Nothing])
        </td>
      </tr><tr>
        <td>
          145
        </td>
        <td>
          7346
        </td>
        <td>
          5666
          -
          5693
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.util.ExpressionLookupResult.hasExpressionLookups
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          res._1.hasExpressionLookups
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          7347
        </td>
        <td>
          5756
          -
          5765
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.LambdaFunction.id
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          res._2.id
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          7350
        </td>
        <td>
          5707
          -
          5766
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ExpressionLookupResults.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          r.copy[com.sparkutils.quality.Id](x$2, x$1)
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          7349
        </td>
        <td>
          5709
          -
          5709
        </td>
        <td>
          TypeApply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ExpressionLookupResults.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          r.copy$default$1[Nothing]
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          7348
        </td>
        <td>
          5734
          -
          5765
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SetLike.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          r.lookupExpressions.+(res._2.id)
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          7351
        </td>
        <td>
          5707
          -
          5766
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  &lt;artifact&gt; val x$1: scala.collection.immutable.Set[com.sparkutils.quality.Id] @scala.reflect.internal.annotations.uncheckedBounds = r.lookupExpressions.+(res._2.id);
  &lt;artifact&gt; val x$2: Map[com.sparkutils.quality.Id,Set[com.sparkutils.quality.impl.util.LookupType]] @scala.reflect.internal.annotations.uncheckedBounds = r.copy$default$1[Nothing];
  r.copy[com.sparkutils.quality.Id](x$2, x$1)
}
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          7352
        </td>
        <td>
          5794
          -
          5795
        </td>
        <td>
          Ident
        </td>
        <td>
          com.sparkutils.quality.impl.util.LookupIdFunctions.r
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          r
        </td>
      </tr><tr>
        <td>
          150
        </td>
        <td>
          7356
        </td>
        <td>
          5843
          -
          5843
        </td>
        <td>
          TypeApply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ExpressionLookupResults.copy$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          olambdaResults.copy$default$2[Nothing]
        </td>
      </tr><tr>
        <td>
          150
        </td>
        <td>
          7355
        </td>
        <td>
          5866
          -
          5918
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableLike.filter
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          olambdaResults.lookupConstants.filter(((x$2: (com.sparkutils.quality.Id, Set[com.sparkutils.quality.impl.util.LookupType])) =&gt; x$2._2.nonEmpty))
        </td>
      </tr><tr>
        <td>
          150
        </td>
        <td>
          7357
        </td>
        <td>
          5828
          -
          5919
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ExpressionLookupResults.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          olambdaResults.copy[com.sparkutils.quality.Id](olambdaResults.lookupConstants.filter(((x$2: (com.sparkutils.quality.Id, Set[com.sparkutils.quality.impl.util.LookupType])) =&gt; x$2._2.nonEmpty)), olambdaResults.copy$default$2[Nothing])
        </td>
      </tr><tr>
        <td>
          150
        </td>
        <td>
          7354
        </td>
        <td>
          5904
          -
          5917
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableOnce.nonEmpty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$2._2.nonEmpty
        </td>
      </tr><tr>
        <td>
          152
        </td>
        <td>
          7359
        </td>
        <td>
          5985
          -
          5994
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.generic.ImmutableSetFactory.empty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.empty[com.sparkutils.quality.impl.util.RuleRow]
        </td>
      </tr><tr>
        <td>
          152
        </td>
        <td>
          7361
        </td>
        <td>
          5925
          -
          6011
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.LookupResults.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          LookupResults.apply(ruleSuite, ExpressionLookupResults.apply[com.sparkutils.quality.impl.util.RuleRow](scala.Predef.Map.empty[com.sparkutils.quality.impl.util.RuleRow, Nothing], scala.Predef.Set.empty[com.sparkutils.quality.impl.util.RuleRow]), lambdaResults)
        </td>
      </tr><tr>
        <td>
          152
        </td>
        <td>
          7358
        </td>
        <td>
          5974
          -
          5983
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.Map.empty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Map.empty[com.sparkutils.quality.impl.util.RuleRow, Nothing]
        </td>
      </tr><tr>
        <td>
          152
        </td>
        <td>
          7360
        </td>
        <td>
          5950
          -
          5995
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ExpressionLookupResults.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ExpressionLookupResults.apply[com.sparkutils.quality.impl.util.RuleRow](scala.Predef.Map.empty[com.sparkutils.quality.impl.util.RuleRow, Nothing], scala.Predef.Set.empty[com.sparkutils.quality.impl.util.RuleRow])
        </td>
      </tr><tr>
        <td>
          159
        </td>
        <td>
          7362
        </td>
        <td>
          6185
          -
          6204
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.eq
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          TSLocal.this.threadLocal.eq(null)
        </td>
      </tr><tr>
        <td>
          159
        </td>
        <td>
          7368
        </td>
        <td>
          6206
          -
          6382
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  val init: () =&gt; T = TSLocal.this.initialValue;
  this.synchronized[Unit](TSLocal.this.threadLocal_=({
    final class $anon extends ThreadLocal[T] {
      def &lt;init&gt;(): &lt;$anon: ThreadLocal[T]&gt; = {
        $anon.super.&lt;init&gt;();
        ()
      };
      override def initialValue(): T = init.apply()
    };
    new $anon()
  }))
}
        </td>
      </tr><tr>
        <td>
          159
        </td>
        <td>
          7370
        </td>
        <td>
          6181
          -
          6181
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          159
        </td>
        <td>
          7369
        </td>
        <td>
          6181
          -
          6181
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          160
        </td>
        <td>
          7363
        </td>
        <td>
          6225
          -
          6237
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.util.TSLocal.initialValue
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          TSLocal.this.initialValue
        </td>
      </tr><tr>
        <td>
          161
        </td>
        <td>
          7367
        </td>
        <td>
          6244
          -
          6376
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.synchronized
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.synchronized[Unit](TSLocal.this.threadLocal_=({
  final class $anon extends ThreadLocal[T] {
    def &lt;init&gt;(): &lt;$anon: ThreadLocal[T]&gt; = {
      $anon.super.&lt;init&gt;();
      ()
    };
    override def initialValue(): T = init.apply()
  };
  new $anon()
}))
        </td>
      </tr><tr>
        <td>
          163
        </td>
        <td>
          7365
        </td>
        <td>
          6287
          -
          6290
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.TSLocal.$anon.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anon()
        </td>
      </tr><tr>
        <td>
          163
        </td>
        <td>
          7366
        </td>
        <td>
          6273
          -
          6367
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.TSLocal.threadLocal_=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          TSLocal.this.threadLocal_=({
  final class $anon extends ThreadLocal[T] {
    def &lt;init&gt;(): &lt;$anon: ThreadLocal[T]&gt; = {
      $anon.super.&lt;init&gt;();
      ()
    };
    override def initialValue(): T = init.apply()
  };
  new $anon()
})
        </td>
      </tr><tr>
        <td>
          164
        </td>
        <td>
          7364
        </td>
        <td>
          6351
          -
          6357
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function0.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          init.apply()
        </td>
      </tr><tr>
        <td>
          169
        </td>
        <td>
          7371
        </td>
        <td>
          6387
          -
          6404
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.ThreadLocal.get
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          TSLocal.this.threadLocal.get()
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          7377
        </td>
        <td>
          6558
          -
          6558
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          7378
        </td>
        <td>
          6558
          -
          6558
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          7372
        </td>
        <td>
          6562
          -
          6572
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          TransientHolder.this.it.==(null)
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          7376
        </td>
        <td>
          6582
          -
          6637
        </td>
        <td>
          Block
        </td>
        <td>
          java.lang.Object.synchronized
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.synchronized[Unit](TransientHolder.this.it_=(TransientHolder.this.initialise.apply()))
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          7375
        </td>
        <td>
          6582
          -
          6637
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.synchronized
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.synchronized[Unit](TransientHolder.this.it_=(TransientHolder.this.initialise.apply()))
        </td>
      </tr><tr>
        <td>
          179
        </td>
        <td>
          7374
        </td>
        <td>
          6611
          -
          6628
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.TransientHolder.it_=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          TransientHolder.this.it_=(TransientHolder.this.initialise.apply())
        </td>
      </tr><tr>
        <td>
          179
        </td>
        <td>
          7373
        </td>
        <td>
          6616
          -
          6628
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function0.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          TransientHolder.this.initialise.apply()
        </td>
      </tr><tr>
        <td>
          183
        </td>
        <td>
          7379
        </td>
        <td>
          6648
          -
          6650
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.util.TransientHolder.it
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          TransientHolder.this.it
        </td>
      </tr><tr>
        <td>
          186
        </td>
        <td>
          7383
        </td>
        <td>
          6680
          -
          6737
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.synchronized
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          this.synchronized[Unit](TransientHolder.this.it_=(null.asInstanceOf[T]))
        </td>
      </tr><tr>
        <td>
          187
        </td>
        <td>
          7380
        </td>
        <td>
          6711
          -
          6715
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          null
        </td>
      </tr><tr>
        <td>
          187
        </td>
        <td>
          7382
        </td>
        <td>
          6706
          -
          6731
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.TransientHolder.it_=
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          TransientHolder.this.it_=(null.asInstanceOf[T])
        </td>
      </tr><tr>
        <td>
          187
        </td>
        <td>
          7381
        </td>
        <td>
          6711
          -
          6731
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.asInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          null.asInstanceOf[T]
        </td>
      </tr><tr>
        <td>
          197
        </td>
        <td>
          7384
        </td>
        <td>
          6945
          -
          6969
        </td>
        <td>
          Apply
        </td>
        <td>
          java.util.concurrent.atomic.AtomicBoolean.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new java.util.concurrent.atomic.AtomicBoolean(false)
        </td>
      </tr><tr>
        <td>
          203
        </td>
        <td>
          7385
        </td>
        <td>
          7180
          -
          7201
        </td>
        <td>
          Apply
        </td>
        <td>
          java.util.concurrent.atomic.AtomicBoolean.set
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          Testing.this.testingFlag.set(true)
        </td>
      </tr><tr>
        <td>
          206
        </td>
        <td>
          7386
        </td>
        <td>
          7223
          -
          7238
        </td>
        <td>
          Apply
        </td>
        <td>
          java.util.concurrent.atomic.AtomicBoolean.get
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          Testing.this.testingFlag.get()
        </td>
      </tr><tr>
        <td>
          212
        </td>
        <td>
          7388
        </td>
        <td>
          7406
          -
          7428
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  Testing.this.setTesting();
  thunk
}
        </td>
      </tr><tr>
        <td>
          213
        </td>
        <td>
          7387
        </td>
        <td>
          7406
          -
          7418
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.Testing.setTesting
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          Testing.this.setTesting()
        </td>
      </tr><tr>
        <td>
          216
        </td>
        <td>
          7389
        </td>
        <td>
          7447
          -
          7469
        </td>
        <td>
          Apply
        </td>
        <td>
          java.util.concurrent.atomic.AtomicBoolean.set
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          Testing.this.testingFlag.set(false)
        </td>
      </tr><tr>
        <td>
          216
        </td>
        <td>
          7390
        </td>
        <td>
          7447
          -
          7469
        </td>
        <td>
          Block
        </td>
        <td>
          java.util.concurrent.atomic.AtomicBoolean.set
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          Testing.this.testingFlag.set(false)
        </td>
      </tr><tr>
        <td>
          230
        </td>
        <td>
          7392
        </td>
        <td>
          7745
          -
          7758
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.!=
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          right.!=(null)
        </td>
      </tr><tr>
        <td>
          230
        </td>
        <td>
          7391
        </td>
        <td>
          7737
          -
          7741
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          null
        </td>
      </tr><tr>
        <td>
          230
        </td>
        <td>
          7393
        </td>
        <td>
          7729
          -
          7758
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          left.==(null).&amp;&amp;(right.!=(null))
        </td>
      </tr><tr>
        <td>
          231
        </td>
        <td>
          7395
        </td>
        <td>
          7766
          -
          7770
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          -100
        </td>
      </tr><tr>
        <td>
          231
        </td>
        <td>
          7394
        </td>
        <td>
          7766
          -
          7770
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          -100
        </td>
      </tr><tr>
        <td>
          233
        </td>
        <td>
          7398
        </td>
        <td>
          7790
          -
          7819
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          right.==(null).&amp;&amp;(left.!=(null))
        </td>
      </tr><tr>
        <td>
          233
        </td>
        <td>
          7404
        </td>
        <td>
          7786
          -
          7906
        </td>
        <td>
          If
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          if (right.==(null).&amp;&amp;(left.!=(null)))
  100
else
  left.asInstanceOf[T].compareTo(right.asInstanceOf[T])
        </td>
      </tr><tr>
        <td>
          233
        </td>
        <td>
          7397
        </td>
        <td>
          7807
          -
          7819
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.!=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          left.!=(null)
        </td>
      </tr><tr>
        <td>
          233
        </td>
        <td>
          7396
        </td>
        <td>
          7799
          -
          7803
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          null
        </td>
      </tr><tr>
        <td>
          234
        </td>
        <td>
          7400
        </td>
        <td>
          7829
          -
          7832
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          100
        </td>
      </tr><tr>
        <td>
          234
        </td>
        <td>
          7399
        </td>
        <td>
          7829
          -
          7832
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          100
        </td>
      </tr><tr>
        <td>
          236
        </td>
        <td>
          7401
        </td>
        <td>
          7884
          -
          7905
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.asInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          right.asInstanceOf[T]
        </td>
      </tr><tr>
        <td>
          236
        </td>
        <td>
          7403
        </td>
        <td>
          7852
          -
          7906
        </td>
        <td>
          Block
        </td>
        <td>
          java.lang.Comparable.compareTo
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          left.asInstanceOf[T].compareTo(right.asInstanceOf[T])
        </td>
      </tr><tr>
        <td>
          236
        </td>
        <td>
          7402
        </td>
        <td>
          7852
          -
          7906
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Comparable.compareTo
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          left.asInstanceOf[T].compareTo(right.asInstanceOf[T])
        </td>
      </tr><tr>
        <td>
          246
        </td>
        <td>
          7407
        </td>
        <td>
          8163
          -
          8192
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          left.==(null).&amp;&amp;(right.!=(null))
        </td>
      </tr><tr>
        <td>
          246
        </td>
        <td>
          7406
        </td>
        <td>
          8179
          -
          8192
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.!=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          right.!=(null)
        </td>
      </tr><tr>
        <td>
          246
        </td>
        <td>
          7405
        </td>
        <td>
          8171
          -
          8175
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          null
        </td>
      </tr><tr>
        <td>
          247
        </td>
        <td>
          7409
        </td>
        <td>
          8200
          -
          8204
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          -100
        </td>
      </tr><tr>
        <td>
          247
        </td>
        <td>
          7408
        </td>
        <td>
          8200
          -
          8204
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          -100
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          7410
        </td>
        <td>
          8233
          -
          8237
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          null
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          7419
        </td>
        <td>
          8220
          -
          8347
        </td>
        <td>
          If
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          if (right.==(null).&amp;&amp;(left.!=(null)))
  100
else
  ordering.compare(left.asInstanceOf[T], right.asInstanceOf[T])
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          7412
        </td>
        <td>
          8224
          -
          8253
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          right.==(null).&amp;&amp;(left.!=(null))
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          7411
        </td>
        <td>
          8241
          -
          8253
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.!=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          left.!=(null)
        </td>
      </tr><tr>
        <td>
          250
        </td>
        <td>
          7413
        </td>
        <td>
          8263
          -
          8266
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          100
        </td>
      </tr><tr>
        <td>
          250
        </td>
        <td>
          7414
        </td>
        <td>
          8263
          -
          8266
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          100
        </td>
      </tr><tr>
        <td>
          252
        </td>
        <td>
          7416
        </td>
        <td>
          8325
          -
          8346
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.asInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          right.asInstanceOf[T]
        </td>
      </tr><tr>
        <td>
          252
        </td>
        <td>
          7415
        </td>
        <td>
          8303
          -
          8323
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.asInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          left.asInstanceOf[T]
        </td>
      </tr><tr>
        <td>
          252
        </td>
        <td>
          7418
        </td>
        <td>
          8286
          -
          8347
        </td>
        <td>
          Block
        </td>
        <td>
          scala.math.Ordering.compare
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ordering.compare(left.asInstanceOf[T], right.asInstanceOf[T])
        </td>
      </tr><tr>
        <td>
          252
        </td>
        <td>
          7417
        </td>
        <td>
          8286
          -
          8347
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.math.Ordering.compare
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ordering.compare(left.asInstanceOf[T], right.asInstanceOf[T])
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          7420
        </td>
        <td>
          8442
          -
          8456
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.isEmpty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          option.isEmpty
        </td>
      </tr><tr>
        <td>
          259
        </td>
        <td>
          7422
        </td>
        <td>
          8464
          -
          8490
        </td>
        <td>
          Block
        </td>
        <td>
          java.util.Optional.empty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          java.util.Optional.empty[T]()
        </td>
      </tr><tr>
        <td>
          259
        </td>
        <td>
          7421
        </td>
        <td>
          8464
          -
          8490
        </td>
        <td>
          Apply
        </td>
        <td>
          java.util.Optional.empty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          java.util.Optional.empty[T]()
        </td>
      </tr><tr>
        <td>
          261
        </td>
        <td>
          7425
        </td>
        <td>
          8506
          -
          8539
        </td>
        <td>
          Block
        </td>
        <td>
          java.util.Optional.of
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          java.util.Optional.of[T](option.get)
        </td>
      </tr><tr>
        <td>
          261
        </td>
        <td>
          7424
        </td>
        <td>
          8506
          -
          8539
        </td>
        <td>
          Apply
        </td>
        <td>
          java.util.Optional.of
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          java.util.Optional.of[T](option.get)
        </td>
      </tr><tr>
        <td>
          261
        </td>
        <td>
          7423
        </td>
        <td>
          8528
          -
          8538
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.get
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          option.get
        </td>
      </tr><tr>
        <td>
          276
        </td>
        <td>
          7426
        </td>
        <td>
          8898
          -
          8933
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.ofDim
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Array.ofDim[T](array.numElements())(evidence$1)
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          7428
        </td>
        <td>
          8976
          -
          8995
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Array.update
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          res.update(i, f.apply(v))
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          7427
        </td>
        <td>
          8990
          -
          8994
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function1.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          f.apply(v)
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          7429
        </td>
        <td>
          8942
          -
          8996
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.util.ArrayData.foreach
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          array.foreach(dataType, ((i: Int, v: Any) =&gt; res.update(i, f.apply(v))))
        </td>
      </tr><tr>
        <td>
          279
        </td>
        <td>
          7431
        </td>
        <td>
          9040
          -
          9040
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.this.Array.canBuildFrom[T](evidence$1)
        </td>
      </tr><tr>
        <td>
          279
        </td>
        <td>
          7430
        </td>
        <td>
          9025
          -
          9036
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.util.ArrayData.array
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          array.array
        </td>
      </tr><tr>
        <td>
          279
        </td>
        <td>
          7432
        </td>
        <td>
          9025
          -
          9043
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.genericArrayOps[Any](array.array).map[T, Array[T]](f)(scala.this.Array.canBuildFrom[T](evidence$1))
        </td>
      </tr><tr>
        <td>
          291
        </td>
        <td>
          7434
        </td>
        <td>
          9292
          -
          9327
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          com.sparkutils.quality.impl.util.Arrays.mapArray
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          Arrays.this.mapArray[Any](array, dataType, {
  ((x: Any) =&gt; scala.Predef.identity[Any](x))
})((ClassTag.Any: scala.reflect.ClassTag[Any]))
        </td>
      </tr><tr>
        <td>
          291
        </td>
        <td>
          7433
        </td>
        <td>
          9318
          -
          9326
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Predef.identity
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.identity[Any](x)
        </td>
      </tr><tr>
        <td>
          292
        </td>
        <td>
          7435
        </td>
        <td>
          9344
          -
          9355
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.util.ArrayData.array
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          array.array
        </td>
      </tr><tr>
        <td>
          323
        </td>
        <td>
          7437
        </td>
        <td>
          10182
          -
          10192
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](ForceNullable.this.child)
        </td>
      </tr><tr>
        <td>
          323
        </td>
        <td>
          7436
        </td>
        <td>
          10186
          -
          10191
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.util.ForceNullable.child
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ForceNullable.this.child
        </td>
      </tr><tr>
        <td>
          325
        </td>
        <td>
          7438
        </td>
        <td>
          10229
          -
          10233
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          327
        </td>
        <td>
          7439
        </td>
        <td>
          10282
          -
          10299
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.eval
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ForceNullable.this.child.eval(input)
        </td>
      </tr><tr>
        <td>
          330
        </td>
        <td>
          7440
        </td>
        <td>
          10397
          -
          10415
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.genCode
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ForceNullable.this.child.genCode(ctx)
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          7442
        </td>
        <td>
          10430
          -
          10457
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.JavaCode.javaType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.codegen.JavaCode.javaType(ForceNullable.this.dataType)
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          7441
        </td>
        <td>
          10448
          -
          10456
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.util.ForceNullable.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ForceNullable.this.dataType
        </td>
      </tr><tr>
        <td>
          332
        </td>
        <td>
          7443
        </td>
        <td>
          10493
          -
          10501
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.util.ForceNullable.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ForceNullable.this.dataType
        </td>
      </tr><tr>
        <td>
          332
        </td>
        <td>
          7444
        </td>
        <td>
          10474
          -
          10502
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.JavaCode.boxedType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.codegen.JavaCode.boxedType(ForceNullable.this.dataType)
        </td>
      </tr><tr>
        <td>
          333
        </td>
        <td>
          7455
        </td>
        <td>
          10510
          -
          10510
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.copy$default$3
        </td>
      </tr><tr>
        <td>
          333
        </td>
        <td>
          7454
        </td>
        <td>
          10510
          -
          10510
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.copy$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.copy$default$2
        </td>
      </tr><tr>
        <td>
          333
        </td>
        <td>
          7456
        </td>
        <td>
          10507
          -
          10789
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.copy(org.apache.spark.sql.catalyst.expressions.codegen.Block.BlockHelper(scala.StringContext.apply(&quot;\n            &quot;, &quot;\n            boolean &quot;, &quot; = true;\n            &quot;, &quot; &quot;, &quot; = null;\n            if (&quot;, &quot; != null) {\n              &quot;, &quot; = false;\n              &quot;, &quot; = (&quot;, &quot;) &quot;, &quot;;\n            }\n            &quot;)).code(c.code, ev.isNull, typ, ev.value, c.value, ev.isNull, ev.value, boxed, c.value), ev.copy$default$2, ev.copy$default$3)
        </td>
      </tr><tr>
        <td>
          334
        </td>
        <td>
          7445
        </td>
        <td>
          10528
          -
          10788
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;\n            &quot;, &quot;\n            boolean &quot;, &quot; = true;\n            &quot;, &quot; &quot;, &quot; = null;\n            if (&quot;, &quot; != null) {\n              &quot;, &quot; = false;\n              &quot;, &quot; = (&quot;, &quot;) &quot;, &quot;;\n            }\n            &quot;)
        </td>
      </tr><tr>
        <td>
          334
        </td>
        <td>
          7453
        </td>
        <td>
          10528
          -
          10788
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.Block.BlockHelper.code
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.codegen.Block.BlockHelper(scala.StringContext.apply(&quot;\n            &quot;, &quot;\n            boolean &quot;, &quot; = true;\n            &quot;, &quot; &quot;, &quot; = null;\n            if (&quot;, &quot; != null) {\n              &quot;, &quot; = false;\n              &quot;, &quot; = (&quot;, &quot;) &quot;, &quot;;\n            }\n            &quot;)).code(c.code, ev.isNull, typ, ev.value, c.value, ev.isNull, ev.value, boxed, c.value)
        </td>
      </tr><tr>
        <td>
          335
        </td>
        <td>
          7446
        </td>
        <td>
          10550
          -
          10556
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.code
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          c.code
        </td>
      </tr><tr>
        <td>
          336
        </td>
        <td>
          7447
        </td>
        <td>
          10580
          -
          10589
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.isNull
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.isNull
        </td>
      </tr><tr>
        <td>
          337
        </td>
        <td>
          7448
        </td>
        <td>
          10618
          -
          10626
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.value
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.value
        </td>
      </tr><tr>
        <td>
          338
        </td>
        <td>
          7449
        </td>
        <td>
          10654
          -
          10661
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.value
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          c.value
        </td>
      </tr><tr>
        <td>
          339
        </td>
        <td>
          7450
        </td>
        <td>
          10690
          -
          10699
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.isNull
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.isNull
        </td>
      </tr><tr>
        <td>
          340
        </td>
        <td>
          7452
        </td>
        <td>
          10749
          -
          10756
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.value
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          c.value
        </td>
      </tr><tr>
        <td>
          340
        </td>
        <td>
          7451
        </td>
        <td>
          10726
          -
          10734
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.value
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.value
        </td>
      </tr><tr>
        <td>
          346
        </td>
        <td>
          7457
        </td>
        <td>
          10832
          -
          10846
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ForceNullable.this.child.dataType
        </td>
      </tr><tr>
        <td>
          349
        </td>
        <td>
          7458
        </td>
        <td>
          10956
          -
          10972
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          newChildren.head
        </td>
      </tr><tr>
        <td>
          349
        </td>
        <td>
          7459
        </td>
        <td>
          10943
          -
          10973
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ForceNullable.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ForceNullable.this.copy(newChildren.head)
        </td>
      </tr><tr>
        <td>
          364
        </td>
        <td>
          7461
        </td>
        <td>
          11324
          -
          11375
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expressionEncoder
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expressionEncoder[T](scala.Predef.implicitly[org.apache.spark.sql.Encoder[T]](evidence$2))
        </td>
      </tr><tr>
        <td>
          364
        </td>
        <td>
          7460
        </td>
        <td>
          11352
          -
          11374
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Predef.implicitly
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.implicitly[org.apache.spark.sql.Encoder[T]](evidence$2)
        </td>
      </tr><tr>
        <td>
          366
        </td>
        <td>
          7462
        </td>
        <td>
          11402
          -
          11414
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.clsTag
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          oexpr.clsTag
        </td>
      </tr><tr>
        <td>
          368
        </td>
        <td>
          7578
        </td>
        <td>
          11420
          -
          11423
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.Encoding.$anon.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anon()
        </td>
      </tr><tr>
        <td>
          370
        </td>
        <td>
          7463
        </td>
        <td>
          11482
          -
          11486
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          372
        </td>
        <td>
          7464
        </td>
        <td>
          11527
          -
          11554
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          oexpr.deserializer.dataType
        </td>
      </tr><tr>
        <td>
          375
        </td>
        <td>
          7465
        </td>
        <td>
          11619
          -
          11635
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.serializer
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          oexpr.serializer
        </td>
      </tr><tr>
        <td>
          376
        </td>
        <td>
          7466
        </td>
        <td>
          11648
          -
          11662
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          se.length.==(1)
        </td>
      </tr><tr>
        <td>
          377
        </td>
        <td>
          7467
        </td>
        <td>
          11674
          -
          11690
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          se.head.dataType
        </td>
      </tr><tr>
        <td>
          377
        </td>
        <td>
          7468
        </td>
        <td>
          11674
          -
          11690
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          se.head.dataType
        </td>
      </tr><tr>
        <td>
          379
        </td>
        <td>
          7476
        </td>
        <td>
          11714
          -
          11862
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.StructType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructType.apply(se.map[org.apache.spark.sql.types.StructField, Seq[org.apache.spark.sql.types.StructField]](((n: org.apache.spark.sql.catalyst.expressions.NamedExpression) =&gt; org.apache.spark.sql.types.StructField.apply(n.asInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedExpression].qualifiedName, n.dataType, n.nullable, org.apache.spark.sql.types.StructField.apply$default$4)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.types.StructField]))
        </td>
      </tr><tr>
        <td>
          379
        </td>
        <td>
          7477
        </td>
        <td>
          11714
          -
          11862
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.types.StructType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructType.apply(se.map[org.apache.spark.sql.types.StructField, Seq[org.apache.spark.sql.types.StructField]](((n: org.apache.spark.sql.catalyst.expressions.NamedExpression) =&gt; org.apache.spark.sql.types.StructField.apply(n.asInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedExpression].qualifiedName, n.dataType, n.nullable, org.apache.spark.sql.types.StructField.apply$default$4)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.types.StructField]))
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          7470
        </td>
        <td>
          11826
          -
          11836
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          n.dataType
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          7472
        </td>
        <td>
          11767
          -
          11767
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.apply$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructField.apply$default$4
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          7475
        </td>
        <td>
          11755
          -
          11850
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          se.map[org.apache.spark.sql.types.StructField, Seq[org.apache.spark.sql.types.StructField]](((n: org.apache.spark.sql.catalyst.expressions.NamedExpression) =&gt; org.apache.spark.sql.types.StructField.apply(n.asInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedExpression].qualifiedName, n.dataType, n.nullable, org.apache.spark.sql.types.StructField.apply$default$4)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.types.StructField])
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          7469
        </td>
        <td>
          11779
          -
          11824
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedExpression.qualifiedName
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          n.asInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedExpression].qualifiedName
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          7471
        </td>
        <td>
          11838
          -
          11848
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.nullable
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          n.nullable
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          7474
        </td>
        <td>
          11761
          -
          11761
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.types.StructField]
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          7473
        </td>
        <td>
          11767
          -
          11849
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.StructField.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructField.apply(n.asInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedExpression].qualifiedName, n.dataType, n.nullable, org.apache.spark.sql.types.StructField.apply$default$4)
        </td>
      </tr><tr>
        <td>
          385
        </td>
        <td>
          7478
        </td>
        <td>
          11955
          -
          11973
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.deserializer
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          oexpr.deserializer
        </td>
      </tr><tr>
        <td>
          389
        </td>
        <td>
          7479
        </td>
        <td>
          12054
          -
          12061
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Alias.child
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.child
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          7481
        </td>
        <td>
          12143
          -
          12143
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.UnresolvedMapObjects.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy$default$3
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          7484
        </td>
        <td>
          12118
          -
          12164
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.withNewChildren(scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.objects.UnresolvedMapObjects]({
  &lt;artifact&gt; val x$1: org.apache.spark.sql.catalyst.expressions.Expression = path;
  &lt;artifact&gt; val x$2: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression @scala.reflect.internal.annotations.uncheckedBounds = m.copy$default$1;
  &lt;artifact&gt; val x$3: Option[Class[_]] @scala.reflect.internal.annotations.uncheckedBounds = m.copy$default$3;
  m.copy(x$2, x$1, x$3)
}))
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          7480
        </td>
        <td>
          12143
          -
          12143
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.UnresolvedMapObjects.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy$default$1
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          7483
        </td>
        <td>
          12136
          -
          12163
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.objects.UnresolvedMapObjects]({
  &lt;artifact&gt; val x$1: org.apache.spark.sql.catalyst.expressions.Expression = path;
  &lt;artifact&gt; val x$2: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression @scala.reflect.internal.annotations.uncheckedBounds = m.copy$default$1;
  &lt;artifact&gt; val x$3: Option[Class[_]] @scala.reflect.internal.annotations.uncheckedBounds = m.copy$default$3;
  m.copy(x$2, x$1, x$3)
})
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          7482
        </td>
        <td>
          12141
          -
          12161
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.UnresolvedMapObjects.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy(x$2, x$1, x$3)
        </td>
      </tr><tr>
        <td>
          391
        </td>
        <td>
          7485
        </td>
        <td>
          12205
          -
          12205
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.Encoding.$anon.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          391
        </td>
        <td>
          7486
        </td>
        <td>
          12191
          -
          12297
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.transformUp(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x1: A1, default: A1 =&gt; B1): B1 = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (_: org.apache.spark.sql.catalyst.analysis.GetColumnByOrdinal) =&gt; path
      case (defaultCase$ @ _) =&gt; default.apply(x1)
    };
    final def isDefinedAt(x1: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (_: org.apache.spark.sql.catalyst.analysis.GetColumnByOrdinal) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          396
        </td>
        <td>
          7488
        </td>
        <td>
          12360
          -
          12360
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.UnresolvedMapObjects.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy$default$3
        </td>
      </tr><tr>
        <td>
          396
        </td>
        <td>
          7487
        </td>
        <td>
          12360
          -
          12360
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.UnresolvedMapObjects.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy$default$1
        </td>
      </tr><tr>
        <td>
          396
        </td>
        <td>
          7489
        </td>
        <td>
          12358
          -
          12378
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.UnresolvedMapObjects.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy(x$5, x$4, x$6)
        </td>
      </tr><tr>
        <td>
          398
        </td>
        <td>
          7494
        </td>
        <td>
          12436
          -
          12522
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableOnce.toMap
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          outputType.asInstanceOf[org.apache.spark.sql.types.StructType].zipWithIndex[org.apache.spark.sql.types.StructField, Seq[(org.apache.spark.sql.types.StructField, Int)]](collection.this.Seq.canBuildFrom[(org.apache.spark.sql.types.StructField, Int)]).map[(String, Int), Seq[(String, Int)]](((x0$1: (org.apache.spark.sql.types.StructField, Int)) =&gt; x0$1 match {
  case (_1: org.apache.spark.sql.types.StructField, _2: Int)(org.apache.spark.sql.types.StructField, Int)((e @ _), (i @ _)) =&gt; scala.Predef.ArrowAssoc[String](e.name).-&gt;[Int](i)
}))(collection.this.Seq.canBuildFrom[(String, Int)]).toMap[String, Int](scala.Predef.$conforms[(String, Int)])
        </td>
      </tr><tr>
        <td>
          398
        </td>
        <td>
          7490
        </td>
        <td>
          12472
          -
          12472
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[(org.apache.spark.sql.types.StructField, Int)]
        </td>
      </tr><tr>
        <td>
          398
        </td>
        <td>
          7493
        </td>
        <td>
          12517
          -
          12517
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Predef.$conforms
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.$conforms[(String, Int)]
        </td>
      </tr><tr>
        <td>
          398
        </td>
        <td>
          7492
        </td>
        <td>
          12488
          -
          12488
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[(String, Int)]
        </td>
      </tr><tr>
        <td>
          398
        </td>
        <td>
          7491
        </td>
        <td>
          12503
          -
          12514
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Predef.ArrowAssoc.-&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.ArrowAssoc[String](e.name).-&gt;[Int](i)
        </td>
      </tr><tr>
        <td>
          400
        </td>
        <td>
          7497
        </td>
        <td>
          12570
          -
          12583
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Literal.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Literal.apply(null)
        </td>
      </tr><tr>
        <td>
          400
        </td>
        <td>
          7496
        </td>
        <td>
          12541
          -
          12568
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.IsNull.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.IsNull.apply(ForceNullable.apply(path))
        </td>
      </tr><tr>
        <td>
          400
        </td>
        <td>
          7495
        </td>
        <td>
          12548
          -
          12567
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ForceNullable.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ForceNullable.apply(path)
        </td>
      </tr><tr>
        <td>
          400
        </td>
        <td>
          7509
        </td>
        <td>
          12538
          -
          12850
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.If.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.If.apply(org.apache.spark.sql.catalyst.expressions.IsNull.apply(ForceNullable.apply(path)), org.apache.spark.sql.catalyst.expressions.Literal.apply(null), n.withNewChildren(n.children.map[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](((x$3: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; x$3.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x2: A1, default: A1 =&gt; B1): B1 = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; com.sparkutils.shim.expressions.GetStructField3.apply(path, o.apply(u.name), com.sparkutils.shim.expressions.GetStructField3.apply$default$3)
      case (defaultCase$ @ _) =&gt; default.apply(x2)
    };
    final def isDefinedAt(x2: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression])))
        </td>
      </tr><tr>
        <td>
          401
        </td>
        <td>
          7506
        </td>
        <td>
          12630
          -
          12630
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]
        </td>
      </tr><tr>
        <td>
          401
        </td>
        <td>
          7508
        </td>
        <td>
          12601
          -
          12834
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          n.withNewChildren(n.children.map[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](((x$3: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; x$3.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x2: A1, default: A1 =&gt; B1): B1 = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; com.sparkutils.shim.expressions.GetStructField3.apply(path, o.apply(u.name), com.sparkutils.shim.expressions.GetStructField3.apply$default$3)
      case (defaultCase$ @ _) =&gt; default.apply(x2)
    };
    final def isDefinedAt(x2: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          401
        </td>
        <td>
          7507
        </td>
        <td>
          12619
          -
          12815
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          n.children.map[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](((x$3: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; x$3.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x2: A1, default: A1 =&gt; B1): B1 = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; com.sparkutils.shim.expressions.GetStructField3.apply(path, o.apply(u.name), com.sparkutils.shim.expressions.GetStructField3.apply$default$3)
      case (defaultCase$ @ _) =&gt; default.apply(x2)
    };
    final def isDefinedAt(x2: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression])
        </td>
      </tr><tr>
        <td>
          402
        </td>
        <td>
          7505
        </td>
        <td>
          12654
          -
          12815
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.transform
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$3.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x2: A1, default: A1 =&gt; B1): B1 = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; com.sparkutils.shim.expressions.GetStructField3.apply(path, o.apply(u.name), com.sparkutils.shim.expressions.GetStructField3.apply$default$3)
      case (defaultCase$ @ _) =&gt; default.apply(x2)
    };
    final def isDefinedAt(x2: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          402
        </td>
        <td>
          7504
        </td>
        <td>
          12666
          -
          12666
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.Encoding.$anon.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          403
        </td>
        <td>
          7499
        </td>
        <td>
          12719
          -
          12737
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.contains
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          o.contains(u.name)
        </td>
      </tr><tr>
        <td>
          403
        </td>
        <td>
          7498
        </td>
        <td>
          12730
          -
          12736
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          u.name
        </td>
      </tr><tr>
        <td>
          404
        </td>
        <td>
          7503
        </td>
        <td>
          12763
          -
          12795
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.shim.expressions.GetStructField3.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.shim.expressions.GetStructField3.apply(path, o.apply(u.name), com.sparkutils.shim.expressions.GetStructField3.apply$default$3)
        </td>
      </tr><tr>
        <td>
          404
        </td>
        <td>
          7502
        </td>
        <td>
          12763
          -
          12763
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.shim.expressions.GetStructField3.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.shim.expressions.GetStructField3.apply$default$3
        </td>
      </tr><tr>
        <td>
          404
        </td>
        <td>
          7501
        </td>
        <td>
          12785
          -
          12794
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          o.apply(u.name)
        </td>
      </tr><tr>
        <td>
          404
        </td>
        <td>
          7500
        </td>
        <td>
          12787
          -
          12793
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          u.name
        </td>
      </tr><tr>
        <td>
          409
        </td>
        <td>
          7512
        </td>
        <td>
          12967
          -
          12967
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[(String, Int)]
        </td>
      </tr><tr>
        <td>
          409
        </td>
        <td>
          7514
        </td>
        <td>
          12915
          -
          13001
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableOnce.toMap
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          outputType.asInstanceOf[org.apache.spark.sql.types.StructType].zipWithIndex[org.apache.spark.sql.types.StructField, Seq[(org.apache.spark.sql.types.StructField, Int)]](collection.this.Seq.canBuildFrom[(org.apache.spark.sql.types.StructField, Int)]).map[(String, Int), Seq[(String, Int)]](((x0$2: (org.apache.spark.sql.types.StructField, Int)) =&gt; x0$2 match {
  case (_1: org.apache.spark.sql.types.StructField, _2: Int)(org.apache.spark.sql.types.StructField, Int)((e @ _), (i @ _)) =&gt; scala.Predef.ArrowAssoc[String](e.name).-&gt;[Int](i)
}))(collection.this.Seq.canBuildFrom[(String, Int)]).toMap[String, Int](scala.Predef.$conforms[(String, Int)])
        </td>
      </tr><tr>
        <td>
          409
        </td>
        <td>
          7511
        </td>
        <td>
          12982
          -
          12993
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Predef.ArrowAssoc.-&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.ArrowAssoc[String](e.name).-&gt;[Int](i)
        </td>
      </tr><tr>
        <td>
          409
        </td>
        <td>
          7510
        </td>
        <td>
          12951
          -
          12951
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[(org.apache.spark.sql.types.StructField, Int)]
        </td>
      </tr><tr>
        <td>
          409
        </td>
        <td>
          7513
        </td>
        <td>
          12996
          -
          12996
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Predef.$conforms
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.$conforms[(String, Int)]
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          7515
        </td>
        <td>
          13027
          -
          13046
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ForceNullable.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ForceNullable.apply(path)
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          7532
        </td>
        <td>
          13017
          -
          13387
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.If.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.If.apply(org.apache.spark.sql.catalyst.expressions.IsNull.apply(ForceNullable.apply(path)), org.apache.spark.sql.catalyst.expressions.Literal.apply(null), {
  &lt;artifact&gt; val x$7: Map[String,org.apache.spark.sql.catalyst.expressions.Expression] @scala.reflect.internal.annotations.uncheckedBounds = i.setters.map[(String, org.apache.spark.sql.catalyst.expressions.Expression), Map[String,org.apache.spark.sql.catalyst.expressions.Expression]](((p: (String, org.apache.spark.sql.catalyst.expressions.Expression)) =&gt; scala.Tuple2.apply[String, org.apache.spark.sql.catalyst.expressions.Expression](p._1, p._2.transform(({
    @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
      def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
        $anonfun.super.&lt;init&gt;();
        ()
      };
      final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x3: A1, default: A1 =&gt; B1): B1 = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
        case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; com.sparkutils.shim.expressions.GetStructField3.apply(path, o.apply(u.name), com.sparkutils.shim.expressions.GetStructField3.apply$default$3)
        case (defaultCase$ @ _) =&gt; default.apply(x3)
      };
      final def isDefinedAt(x3: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
        case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; true
        case (defaultCase$ @ _) =&gt; false
      }
    };
    new $anonfun()
  }: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression])))))(immutable.this.Map.canBuildFrom[String, org.apache.spark.sql.catalyst.expressions.Expression]);
  &lt;artifact&gt; val x$8: org.apache.spark.sql.catalyst.expressions.Expression = i.copy$default$1;
  i.copy(x$8, x$7)
})
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          7517
        </td>
        <td>
          13049
          -
          13062
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Literal.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Literal.apply(null)
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          7516
        </td>
        <td>
          13020
          -
          13047
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.IsNull.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.IsNull.apply(ForceNullable.apply(path))
        </td>
      </tr><tr>
        <td>
          412
        </td>
        <td>
          7530
        </td>
        <td>
          13082
          -
          13082
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.InitializeJavaBean.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.copy$default$1
        </td>
      </tr><tr>
        <td>
          412
        </td>
        <td>
          7531
        </td>
        <td>
          13080
          -
          13371
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.InitializeJavaBean.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.copy(x$8, x$7)
        </td>
      </tr><tr>
        <td>
          413
        </td>
        <td>
          7529
        </td>
        <td>
          13115
          -
          13353
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.setters.map[(String, org.apache.spark.sql.catalyst.expressions.Expression), Map[String,org.apache.spark.sql.catalyst.expressions.Expression]](((p: (String, org.apache.spark.sql.catalyst.expressions.Expression)) =&gt; scala.Tuple2.apply[String, org.apache.spark.sql.catalyst.expressions.Expression](p._1, p._2.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x3: A1, default: A1 =&gt; B1): B1 = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; com.sparkutils.shim.expressions.GetStructField3.apply(path, o.apply(u.name), com.sparkutils.shim.expressions.GetStructField3.apply$default$3)
      case (defaultCase$ @ _) =&gt; default.apply(x3)
    };
    final def isDefinedAt(x3: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression])))))(immutable.this.Map.canBuildFrom[String, org.apache.spark.sql.catalyst.expressions.Expression])
        </td>
      </tr><tr>
        <td>
          413
        </td>
        <td>
          7528
        </td>
        <td>
          13128
          -
          13128
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.Map.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          immutable.this.Map.canBuildFrom[String, org.apache.spark.sql.catalyst.expressions.Expression]
        </td>
      </tr><tr>
        <td>
          414
        </td>
        <td>
          7526
        </td>
        <td>
          13162
          -
          13332
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.transform
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          p._2.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x3: A1, default: A1 =&gt; B1): B1 = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; com.sparkutils.shim.expressions.GetStructField3.apply(path, o.apply(u.name), com.sparkutils.shim.expressions.GetStructField3.apply$default$3)
      case (defaultCase$ @ _) =&gt; default.apply(x3)
    };
    final def isDefinedAt(x3: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          414
        </td>
        <td>
          7525
        </td>
        <td>
          13177
          -
          13177
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.Encoding.$anon.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          414
        </td>
        <td>
          7527
        </td>
        <td>
          13155
          -
          13333
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[String, org.apache.spark.sql.catalyst.expressions.Expression](p._1, p._2.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x3: A1, default: A1 =&gt; B1): B1 = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; com.sparkutils.shim.expressions.GetStructField3.apply(path, o.apply(u.name), com.sparkutils.shim.expressions.GetStructField3.apply$default$3)
      case (defaultCase$ @ _) =&gt; default.apply(x3)
    };
    final def isDefinedAt(x3: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression])))
        </td>
      </tr><tr>
        <td>
          414
        </td>
        <td>
          7518
        </td>
        <td>
          13156
          -
          13160
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          p._1
        </td>
      </tr><tr>
        <td>
          415
        </td>
        <td>
          7520
        </td>
        <td>
          13232
          -
          13250
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.contains
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          o.contains(u.name)
        </td>
      </tr><tr>
        <td>
          415
        </td>
        <td>
          7519
        </td>
        <td>
          13243
          -
          13249
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          u.name
        </td>
      </tr><tr>
        <td>
          416
        </td>
        <td>
          7521
        </td>
        <td>
          13302
          -
          13308
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          u.name
        </td>
      </tr><tr>
        <td>
          416
        </td>
        <td>
          7524
        </td>
        <td>
          13278
          -
          13310
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.shim.expressions.GetStructField3.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.shim.expressions.GetStructField3.apply(path, o.apply(u.name), com.sparkutils.shim.expressions.GetStructField3.apply$default$3)
        </td>
      </tr><tr>
        <td>
          416
        </td>
        <td>
          7523
        </td>
        <td>
          13278
          -
          13278
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.shim.expressions.GetStructField3.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.shim.expressions.GetStructField3.apply$default$3
        </td>
      </tr><tr>
        <td>
          416
        </td>
        <td>
          7522
        </td>
        <td>
          13300
          -
          13309
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          o.apply(u.name)
        </td>
      </tr><tr>
        <td>
          423
        </td>
        <td>
          7534
        </td>
        <td>
          13479
          -
          13579
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.transformUp(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x4: A1, default: A1 =&gt; B1): B1 = ((x4.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (_: org.apache.spark.sql.catalyst.analysis.GetColumnByOrdinal) =&gt; path
      case (defaultCase$ @ _) =&gt; default.apply(x4)
    };
    final def isDefinedAt(x4: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x4.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (_: org.apache.spark.sql.catalyst.analysis.GetColumnByOrdinal) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          423
        </td>
        <td>
          7533
        </td>
        <td>
          13493
          -
          13493
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.Encoding.$anon.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          427
        </td>
        <td>
          7535
        </td>
        <td>
          13616
          -
          13616
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.Encoding.$anon.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          427
        </td>
        <td>
          7536
        </td>
        <td>
          13602
          -
          13696
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.transformUp(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x5: A1, default: A1 =&gt; B1): B1 = ((x5.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (_: org.apache.spark.sql.catalyst.analysis.GetColumnByOrdinal) =&gt; path
      case (defaultCase$ @ _) =&gt; default.apply(x5)
    };
    final def isDefinedAt(x5: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x5.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (_: org.apache.spark.sql.catalyst.analysis.GetColumnByOrdinal) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          438
        </td>
        <td>
          7537
        </td>
        <td>
          13847
          -
          13863
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.serializer
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          oexpr.serializer
        </td>
      </tr><tr>
        <td>
          439
        </td>
        <td>
          7538
        </td>
        <td>
          13876
          -
          13890
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          se.length.==(1)
        </td>
      </tr><tr>
        <td>
          440
        </td>
        <td>
          7539
        </td>
        <td>
          13902
          -
          13909
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          se.head
        </td>
      </tr><tr>
        <td>
          440
        </td>
        <td>
          7555
        </td>
        <td>
          13902
          -
          14358
        </td>
        <td>
          Match
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          se.head match {
  case (a @ (_: org.apache.spark.sql.catalyst.expressions.Alias)) =&gt; a.child match {
    case (m @ (_: org.apache.spark.sql.catalyst.expressions.objects.MapObjects)) =&gt; a.withNewChildren(scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.objects.MapObjects]({
      &lt;artifact&gt; val x$1: org.apache.spark.sql.catalyst.expressions.Expression = path;
      &lt;artifact&gt; val x$2: org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable = m.copy$default$1;
      &lt;artifact&gt; val x$3: org.apache.spark.sql.catalyst.expressions.Expression = m.copy$default$2;
      &lt;artifact&gt; val x$4: Option[Class[_]] @scala.reflect.internal.annotations.uncheckedBounds = m.copy$default$4;
      m.copy(x$2, x$3, x$1, x$4)
    }))
    case (a @ _) =&gt; a.transformUp(({
      @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
        def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
          $anonfun.super.&lt;init&gt;();
          ()
        };
        final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x1: A1, default: A1 =&gt; B1): B1 = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
          case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; path
          case (defaultCase$ @ _) =&gt; default.apply(x1)
        };
        final def isDefinedAt(x1: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
          case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; true
          case (defaultCase$ @ _) =&gt; false
        }
      };
      new $anonfun()
    }: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
  }
  case (m @ (_: org.apache.spark.sql.catalyst.expressions.objects.MapObjects)) =&gt; {
    &lt;artifact&gt; val x$5: org.apache.spark.sql.catalyst.expressions.Expression = path;
    &lt;artifact&gt; val x$6: org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable = m.copy$default$1;
    &lt;artifact&gt; val x$7: org.apache.spark.sql.catalyst.expressions.Expression = m.copy$default$2;
    &lt;artifact&gt; val x$8: Option[Class[_]] @scala.reflect.internal.annotations.uncheckedBounds = m.copy$default$4;
    m.copy(x$6, x$7, x$5, x$8)
  }
  case (a @ _) =&gt; a.transformUp(({
    @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
      def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
        $anonfun.super.&lt;init&gt;();
        ()
      };
      final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x2: A1, default: A1 =&gt; B1): B1 = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
        case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; path
        case (defaultCase$ @ _) =&gt; default.apply(x2)
      };
      final def isDefinedAt(x2: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
        case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; true
        case (defaultCase$ @ _) =&gt; false
      }
    };
    new $anonfun()
  }: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
}
        </td>
      </tr><tr>
        <td>
          442
        </td>
        <td>
          7540
        </td>
        <td>
          13961
          -
          13968
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Alias.child
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.child
        </td>
      </tr><tr>
        <td>
          443
        </td>
        <td>
          7541
        </td>
        <td>
          14040
          -
          14040
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.MapObjects.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy$default$1
        </td>
      </tr><tr>
        <td>
          443
        </td>
        <td>
          7544
        </td>
        <td>
          14038
          -
          14062
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.MapObjects.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy(x$2, x$3, x$1, x$4)
        </td>
      </tr><tr>
        <td>
          443
        </td>
        <td>
          7546
        </td>
        <td>
          14015
          -
          14065
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.withNewChildren(scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.objects.MapObjects]({
  &lt;artifact&gt; val x$1: org.apache.spark.sql.catalyst.expressions.Expression = path;
  &lt;artifact&gt; val x$2: org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable = m.copy$default$1;
  &lt;artifact&gt; val x$3: org.apache.spark.sql.catalyst.expressions.Expression = m.copy$default$2;
  &lt;artifact&gt; val x$4: Option[Class[_]] @scala.reflect.internal.annotations.uncheckedBounds = m.copy$default$4;
  m.copy(x$2, x$3, x$1, x$4)
}))
        </td>
      </tr><tr>
        <td>
          443
        </td>
        <td>
          7543
        </td>
        <td>
          14040
          -
          14040
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.MapObjects.copy$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy$default$4
        </td>
      </tr><tr>
        <td>
          443
        </td>
        <td>
          7542
        </td>
        <td>
          14040
          -
          14040
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.MapObjects.copy$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy$default$2
        </td>
      </tr><tr>
        <td>
          443
        </td>
        <td>
          7545
        </td>
        <td>
          14033
          -
          14064
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.objects.MapObjects]({
  &lt;artifact&gt; val x$1: org.apache.spark.sql.catalyst.expressions.Expression = path;
  &lt;artifact&gt; val x$2: org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable = m.copy$default$1;
  &lt;artifact&gt; val x$3: org.apache.spark.sql.catalyst.expressions.Expression = m.copy$default$2;
  &lt;artifact&gt; val x$4: Option[Class[_]] @scala.reflect.internal.annotations.uncheckedBounds = m.copy$default$4;
  m.copy(x$2, x$3, x$1, x$4)
})
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          7548
        </td>
        <td>
          14092
          -
          14174
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.transformUp(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x1: A1, default: A1 =&gt; B1): B1 = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; path
      case (defaultCase$ @ _) =&gt; default.apply(x1)
    };
    final def isDefinedAt(x1: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          7547
        </td>
        <td>
          14106
          -
          14106
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.Encoding.$anon.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          448
        </td>
        <td>
          7550
        </td>
        <td>
          14227
          -
          14227
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.MapObjects.copy$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy$default$2
        </td>
      </tr><tr>
        <td>
          448
        </td>
        <td>
          7549
        </td>
        <td>
          14227
          -
          14227
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.MapObjects.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy$default$1
        </td>
      </tr><tr>
        <td>
          448
        </td>
        <td>
          7552
        </td>
        <td>
          14225
          -
          14249
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.MapObjects.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy(x$6, x$7, x$5, x$8)
        </td>
      </tr><tr>
        <td>
          448
        </td>
        <td>
          7551
        </td>
        <td>
          14227
          -
          14227
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.MapObjects.copy$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy$default$4
        </td>
      </tr><tr>
        <td>
          449
        </td>
        <td>
          7553
        </td>
        <td>
          14286
          -
          14286
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.Encoding.$anon.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          449
        </td>
        <td>
          7554
        </td>
        <td>
          14272
          -
          14346
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.transformUp(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x2: A1, default: A1 =&gt; B1): B1 = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; path
      case (defaultCase$ @ _) =&gt; default.apply(x2)
    };
    final def isDefinedAt(x2: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          453
        </td>
        <td>
          7577
        </td>
        <td>
          14372
          -
          14837
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  val o: org.apache.spark.sql.types.StructType = outputType.asInstanceOf[org.apache.spark.sql.types.StructType];
  val dealiased: scala.collection.immutable.Map[String,org.apache.spark.sql.catalyst.expressions.Expression] = se.map[(String, org.apache.spark.sql.catalyst.expressions.Expression), Seq[(String, org.apache.spark.sql.catalyst.expressions.Expression)]](((x0$1: org.apache.spark.sql.catalyst.expressions.NamedExpression) =&gt; x0$1 match {
  case (a @ (_: org.apache.spark.sql.catalyst.expressions.Alias)) =&gt; scala.Predef.ArrowAssoc[String](a.name).-&gt;[org.apache.spark.sql.catalyst.expressions.Expression](a.child.transformUp(({
    @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
      def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
        $anonfun.super.&lt;init&gt;();
        ()
      };
      final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x3: A1, default: A1 =&gt; B1): B1 = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
        case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; path
        case (defaultCase$ @ _) =&gt; default.apply(x3)
      };
      final def isDefinedAt(x3: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
        case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; true
        case (defaultCase$ @ _) =&gt; false
      }
    };
    new $anonfun()
  }: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression])))
}))(collection.this.Seq.canBuildFrom[(String, org.apache.spark.sql.catalyst.expressions.Expression)]).toMap[String, org.apache.spark.sql.catalyst.expressions.Expression](scala.Predef.$conforms[(String, org.apache.spark.sql.catalyst.expressions.Expression)]);
  com.sparkutils.shim.expressions.CreateNamedStruct1.apply(scala.Predef.refArrayOps[(String, org.apache.spark.sql.catalyst.expressions.Expression)](scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](o.fields).map[(String, org.apache.spark.sql.catalyst.expressions.Expression), Array[(String, org.apache.spark.sql.catalyst.expressions.Expression)]](((f: org.apache.spark.sql.types.StructField) =&gt; scala.Predef.ArrowAssoc[String](f.name).-&gt;[org.apache.spark.sql.catalyst.expressions.Expression](dealiased.apply(f.name))))(scala.this.Array.canBuildFrom[(String, org.apache.spark.sql.catalyst.expressions.Expression)]((ClassTag.apply[(String, org.apache.spark.sql.catalyst.expressions.Expression)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(String, org.apache.spark.sql.catalyst.expressions.Expression)])))).flatMap[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](((x0$2: (String, org.apache.spark.sql.catalyst.expressions.Expression)) =&gt; x0$2 match {
    case (_1: String, _2: org.apache.spark.sql.catalyst.expressions.Expression)(String, org.apache.spark.sql.catalyst.expressions.Expression)((name @ _), (e @ _)) =&gt; scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](org.apache.spark.sql.catalyst.expressions.Literal.apply(name), e)
  }))(scala.this.Array.fallbackCanBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression](Predef.this.DummyImplicit.dummyImplicit)))
}
        </td>
      </tr><tr>
        <td>
          454
        </td>
        <td>
          7556
        </td>
        <td>
          14392
          -
          14427
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.asInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          outputType.asInstanceOf[org.apache.spark.sql.types.StructType]
        </td>
      </tr><tr>
        <td>
          456
        </td>
        <td>
          7561
        </td>
        <td>
          14462
          -
          14462
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[(String, org.apache.spark.sql.catalyst.expressions.Expression)]
        </td>
      </tr><tr>
        <td>
          458
        </td>
        <td>
          7557
        </td>
        <td>
          14507
          -
          14513
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Alias.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.name
        </td>
      </tr><tr>
        <td>
          458
        </td>
        <td>
          7559
        </td>
        <td>
          14517
          -
          14601
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.child.transformUp(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x3: A1, default: A1 =&gt; B1): B1 = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; path
      case (defaultCase$ @ _) =&gt; default.apply(x3)
    };
    final def isDefinedAt(x3: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          458
        </td>
        <td>
          7558
        </td>
        <td>
          14537
          -
          14537
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.Encoding.$anon.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          458
        </td>
        <td>
          7560
        </td>
        <td>
          14507
          -
          14601
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Predef.ArrowAssoc.-&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.ArrowAssoc[String](a.name).-&gt;[org.apache.spark.sql.catalyst.expressions.Expression](a.child.transformUp(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x3: A1, default: A1 =&gt; B1): B1 = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; path
      case (defaultCase$ @ _) =&gt; default.apply(x3)
    };
    final def isDefinedAt(x3: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression])))
        </td>
      </tr><tr>
        <td>
          461
        </td>
        <td>
          7562
        </td>
        <td>
          14614
          -
          14614
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Predef.$conforms
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.$conforms[(String, org.apache.spark.sql.catalyst.expressions.Expression)]
        </td>
      </tr><tr>
        <td>
          461
        </td>
        <td>
          7563
        </td>
        <td>
          14455
          -
          14619
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableOnce.toMap
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          se.map[(String, org.apache.spark.sql.catalyst.expressions.Expression), Seq[(String, org.apache.spark.sql.catalyst.expressions.Expression)]](((x0$1: org.apache.spark.sql.catalyst.expressions.NamedExpression) =&gt; x0$1 match {
  case (a @ (_: org.apache.spark.sql.catalyst.expressions.Alias)) =&gt; scala.Predef.ArrowAssoc[String](a.name).-&gt;[org.apache.spark.sql.catalyst.expressions.Expression](a.child.transformUp(({
    @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
      def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
        $anonfun.super.&lt;init&gt;();
        ()
      };
      final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x3: A1, default: A1 =&gt; B1): B1 = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
        case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; path
        case (defaultCase$ @ _) =&gt; default.apply(x3)
      };
      final def isDefinedAt(x3: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
        case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; true
        case (defaultCase$ @ _) =&gt; false
      }
    };
    new $anonfun()
  }: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression])))
}))(collection.this.Seq.canBuildFrom[(String, org.apache.spark.sql.catalyst.expressions.Expression)]).toMap[String, org.apache.spark.sql.catalyst.expressions.Expression](scala.Predef.$conforms[(String, org.apache.spark.sql.catalyst.expressions.Expression)])
        </td>
      </tr><tr>
        <td>
          463
        </td>
        <td>
          7576
        </td>
        <td>
          14631
          -
          14827
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.shim.expressions.CreateNamedStruct1.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.shim.expressions.CreateNamedStruct1.apply(scala.Predef.refArrayOps[(String, org.apache.spark.sql.catalyst.expressions.Expression)](scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](o.fields).map[(String, org.apache.spark.sql.catalyst.expressions.Expression), Array[(String, org.apache.spark.sql.catalyst.expressions.Expression)]](((f: org.apache.spark.sql.types.StructField) =&gt; scala.Predef.ArrowAssoc[String](f.name).-&gt;[org.apache.spark.sql.catalyst.expressions.Expression](dealiased.apply(f.name))))(scala.this.Array.canBuildFrom[(String, org.apache.spark.sql.catalyst.expressions.Expression)]((ClassTag.apply[(String, org.apache.spark.sql.catalyst.expressions.Expression)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(String, org.apache.spark.sql.catalyst.expressions.Expression)])))).flatMap[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](((x0$2: (String, org.apache.spark.sql.catalyst.expressions.Expression)) =&gt; x0$2 match {
  case (_1: String, _2: org.apache.spark.sql.catalyst.expressions.Expression)(String, org.apache.spark.sql.catalyst.expressions.Expression)((name @ _), (e @ _)) =&gt; scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](org.apache.spark.sql.catalyst.expressions.Literal.apply(name), e)
}))(scala.this.Array.fallbackCanBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression](Predef.this.DummyImplicit.dummyImplicit)))
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          7575
        </td>
        <td>
          14663
          -
          14815
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.flatMap
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.refArrayOps[(String, org.apache.spark.sql.catalyst.expressions.Expression)](scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](o.fields).map[(String, org.apache.spark.sql.catalyst.expressions.Expression), Array[(String, org.apache.spark.sql.catalyst.expressions.Expression)]](((f: org.apache.spark.sql.types.StructField) =&gt; scala.Predef.ArrowAssoc[String](f.name).-&gt;[org.apache.spark.sql.catalyst.expressions.Expression](dealiased.apply(f.name))))(scala.this.Array.canBuildFrom[(String, org.apache.spark.sql.catalyst.expressions.Expression)]((ClassTag.apply[(String, org.apache.spark.sql.catalyst.expressions.Expression)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(String, org.apache.spark.sql.catalyst.expressions.Expression)])))).flatMap[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](((x0$2: (String, org.apache.spark.sql.catalyst.expressions.Expression)) =&gt; x0$2 match {
  case (_1: String, _2: org.apache.spark.sql.catalyst.expressions.Expression)(String, org.apache.spark.sql.catalyst.expressions.Expression)((name @ _), (e @ _)) =&gt; scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](org.apache.spark.sql.catalyst.expressions.Literal.apply(name), e)
}))(scala.this.Array.fallbackCanBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression](Predef.this.DummyImplicit.dummyImplicit))
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          7566
        </td>
        <td>
          14701
          -
          14707
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          f.name
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          7565
        </td>
        <td>
          14681
          -
          14687
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          f.name
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          7574
        </td>
        <td>
          14718
          -
          14718
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.FallbackArrayBuilding.fallbackCanBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.this.Array.fallbackCanBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression](Predef.this.DummyImplicit.dummyImplicit)
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          7568
        </td>
        <td>
          14681
          -
          14708
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Predef.ArrowAssoc.-&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.ArrowAssoc[String](f.name).-&gt;[org.apache.spark.sql.catalyst.expressions.Expression](dealiased.apply(f.name))
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          7570
        </td>
        <td>
          14663
          -
          14709
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](o.fields).map[(String, org.apache.spark.sql.catalyst.expressions.Expression), Array[(String, org.apache.spark.sql.catalyst.expressions.Expression)]](((f: org.apache.spark.sql.types.StructField) =&gt; scala.Predef.ArrowAssoc[String](f.name).-&gt;[org.apache.spark.sql.catalyst.expressions.Expression](dealiased.apply(f.name))))(scala.this.Array.canBuildFrom[(String, org.apache.spark.sql.catalyst.expressions.Expression)]((ClassTag.apply[(String, org.apache.spark.sql.catalyst.expressions.Expression)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(String, org.apache.spark.sql.catalyst.expressions.Expression)])))
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          7564
        </td>
        <td>
          14663
          -
          14671
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructType.fields
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          o.fields
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          7573
        </td>
        <td>
          14718
          -
          14718
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Predef.DummyImplicit.dummyImplicit
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          Predef.this.DummyImplicit.dummyImplicit
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          7567
        </td>
        <td>
          14691
          -
          14708
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          dealiased.apply(f.name)
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          7569
        </td>
        <td>
          14675
          -
          14675
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.this.Array.canBuildFrom[(String, org.apache.spark.sql.catalyst.expressions.Expression)]((ClassTag.apply[(String, org.apache.spark.sql.catalyst.expressions.Expression)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(String, org.apache.spark.sql.catalyst.expressions.Expression)]))
        </td>
      </tr><tr>
        <td>
          466
        </td>
        <td>
          7571
        </td>
        <td>
          14784
          -
          14797
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Literal.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Literal.apply(name)
        </td>
      </tr><tr>
        <td>
          466
        </td>
        <td>
          7572
        </td>
        <td>
          14768
          -
          14801
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](org.apache.spark.sql.catalyst.expressions.Literal.apply(name), e)
        </td>
      </tr><tr>
        <td>
          483
        </td>
        <td>
          7579
        </td>
        <td>
          15078
          -
          15100
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.nullable
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SubQueryWrapper.this.children.head.nullable
        </td>
      </tr><tr>
        <td>
          484
        </td>
        <td>
          7580
        </td>
        <td>
          15136
          -
          15141
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          false
        </td>
      </tr><tr>
        <td>
          486
        </td>
        <td>
          7581
        </td>
        <td>
          15241
          -
          15267
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.genCode
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SubQueryWrapper.this.children.head.genCode(ctx)
        </td>
      </tr><tr>
        <td>
          490
        </td>
        <td>
          7582
        </td>
        <td>
          15329
          -
          15354
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.eval
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SubQueryWrapper.this.children.head.eval(input)
        </td>
      </tr><tr>
        <td>
          492
        </td>
        <td>
          7583
        </td>
        <td>
          15392
          -
          15414
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SubQueryWrapper.this.children.head.dataType
        </td>
      </tr><tr>
        <td>
          494
        </td>
        <td>
          7584
        </td>
        <td>
          15507
          -
          15535
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.SubQueryWrapper.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SubQueryWrapper.this.copy(newChildren)
        </td>
      </tr><tr>
        <td>
          499
        </td>
        <td>
          7585
        </td>
        <td>
          15635
          -
          15635
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.SubQueryWrapper.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          501
        </td>
        <td>
          7586
        </td>
        <td>
          15617
          -
          15687
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.isDefined
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          expr.collectFirst[com.sparkutils.quality.impl.util.SubQueryWrapper](({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,com.sparkutils.quality.impl.util.SubQueryWrapper] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; com.sparkutils.quality.impl.util.SubQueryWrapper&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: com.sparkutils.quality.impl.util.SubQueryWrapper](x1: A1, default: A1 =&gt; B1): B1 = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (s @ (_: com.sparkutils.quality.impl.util.SubQueryWrapper)) =&gt; s
      case (defaultCase$ @ _) =&gt; default.apply(x1)
    };
    final def isDefinedAt(x1: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (s @ (_: com.sparkutils.quality.impl.util.SubQueryWrapper)) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,com.sparkutils.quality.impl.util.SubQueryWrapper])).isDefined
        </td>
      </tr>
    </table>
          </div>
        </div>
      </body>
    </html>