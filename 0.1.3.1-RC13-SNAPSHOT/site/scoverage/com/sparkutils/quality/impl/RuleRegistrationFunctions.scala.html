<html>
      <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <title id="title">
          com/sparkutils/quality/impl/RuleRegistrationFunctions.scala.html
        </title>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/css/theme.default.min.css" type="text/css"/><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/js/jquery.tablesorter.min.js"></script><link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css" type="text/css"/><script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script><script type="text/javascript">
        $(document).ready(function() {$(".tablesorter").tablesorter();});
      </script>
        <style>
          table.codegrid { font-family: monospace; font-size: 12px; width: auto!important; }table.statementlist { width: auto!important; font-size: 13px; } table.codegrid td { padding: 0!important; border: 0!important } table td.linenumber { width: 40px!important; } 
        </style>
      </head>
      <body style="font-family: monospace;">
        <ul class="nav nav-tabs">
          <li>
            <a href="#codegrid" data-toggle="tab">Codegrid</a>
          </li>
          <li>
            <a href="#statementlist" data-toggle="tab">Statement List</a>
          </li>
        </ul>
        <div class="tab-content">
          <div class="tab-pane active" id="codegrid">
            <pre style='font-size: 12pt; font-family: courier, monospace;'>1 <span style=''>package com.sparkutils.quality.impl
</span>2 <span style=''>
</span>3 <span style=''>import com.sparkutils.quality.QualityException.qualityException
</span>4 <span style=''>import com.sparkutils.quality.functions._
</span>5 <span style=''>import com.sparkutils.quality.impl.aggregates.AggregateExpressions
</span>6 <span style=''>import com.sparkutils.quality.impl.bloom.{BucketedArrayParquetAggregator, ParquetAggregator}
</span>7 <span style=''>import com.sparkutils.quality.impl.hash.{HashFunctionFactory, HashFunctionsExpression}
</span>8 <span style=''>import com.sparkutils.quality.impl.id.{GenericLongBasedIDExpression, model}
</span>9 <span style=''>import com.sparkutils.quality.impl.longPair.{AsUUID, LongPairExpression}
</span>10 <span style=''>import com.sparkutils.quality.impl.rng.{RandomBytes, RandomLongs}
</span>11 <span style=''>import com.sparkutils.quality.impl.util.{ComparableMapConverter, ComparableMapReverser, InputWrapper, PrintCode}
</span>12 <span style=''>import com.sparkutils.quality.impl.yaml.{YamlDecoderExpr, YamlEncoderExpr}
</span>13 <span style=''>import com.sparkutils.quality.{QualityException, impl}
</span>14 <span style=''>import com.sparkutils.shim
</span>15 <span style=''>import org.apache.commons.rng.simple.RandomSource
</span>16 <span style=''>import org.apache.spark.sql.ShimUtils.{add, column, expression}
</span>17 <span style=''>import org.apache.spark.sql.catalyst.expressions.{Add, AttributeReference, CreateMap, Expression, Literal, UnresolvedNamedLambdaVariable, LambdaFunction =&gt; SLambdaFunction}
</span>18 <span style=''>import org.apache.spark.sql.qualityFunctions.LambdaFunctions.processTopCallFun
</span>19 <span style=''>import org.apache.spark.sql.qualityFunctions._
</span>20 <span style=''>import org.apache.spark.sql.types._
</span>21 <span style=''>import org.apache.spark.sql.{Column, ShimUtils, SparkSession, functions}
</span>22 <span style=''>import org.apache.spark.unsafe.types.UTF8String
</span>23 <span style=''>
</span>24 <span style=''>object RuleRegistrationFunctions {
</span>25 <span style=''>
</span>26 <span style=''>  protected[quality] def literalsNeeded(pos: Int): Nothing =
</span>27 <span style=''>    if (</span><span style='background: #F0ADAD'>pos == -1</span><span style=''>)
</span>28 <span style=''>      </span><span style='background: #F0ADAD'>qualityException(&quot;Cannot setup Quality Expression with non-literals&quot;)</span><span style=''>
</span>29 <span style=''>    else
</span>30 <span style=''>      </span><span style='background: #AEF1AE'>qualityException(s&quot;Quality Expression requires a string literal in (starts with position 0) position $pos&quot;)</span><span style=''>
</span>31 <span style=''>
</span>32 <span style=''>  protected[quality] def literalsNeeded: Nothing = </span><span style='background: #AEF1AE'>literalsNeeded( -1 )</span><span style=''>
</span>33 <span style=''>
</span>34 <span style=''>  protected[quality] def getLong(exp: Expression, pos: Int = -1) =
</span>35 <span style=''>    exp match {
</span>36 <span style=''>      case Literal(seed: Long, LongType) =&gt; seed
</span>37 <span style=''>      case _ =&gt; </span><span style='background: #AEF1AE'>literalsNeeded(pos)</span><span style=''>
</span>38 <span style=''>    }
</span>39 <span style=''>  protected[quality] def getInteger(exp: Expression, pos: Int = -1) =
</span>40 <span style=''>    exp match {
</span>41 <span style=''>      case Literal(seed: Int, IntegerType) =&gt; seed
</span>42 <span style=''>      case _ =&gt; </span><span style='background: #AEF1AE'>literalsNeeded(pos)</span><span style=''>
</span>43 <span style=''>    }
</span>44 <span style=''>  protected[quality] def getString(exp: Expression, pos: Int = -1) =
</span>45 <span style=''>    exp match {
</span>46 <span style=''>      case Literal(str: UTF8String, StringType) =&gt; </span><span style='background: #AEF1AE'>str.toString()</span><span style=''>
</span>47 <span style=''>      case _ =&gt; </span><span style='background: #AEF1AE'>literalsNeeded(pos)</span><span style=''>
</span>48 <span style=''>    }
</span>49 <span style=''>
</span>50 <span style=''>  protected[quality] val mustKeepNames = </span><span style='background: #AEF1AE'>Set(LambdaFunctions.PlaceHolder,
</span>51 <span style=''></span><span style='background: #AEF1AE'>    LambdaFunctions.Lambda, LambdaFunctions.CallFun)</span><span style=''>
</span>52 <span style=''>
</span>53 <span style=''>  val qualityFunctions = {
</span>54 <span style=''>    val withUnderscores = </span><span style='background: #AEF1AE'>Set(&quot;murmur3_ID&quot;,&quot;unique_ID&quot;,&quot;rng_ID&quot;,&quot;provided_ID&quot;,&quot;field_Based_ID&quot;,
</span>55 <span style=''></span><span style='background: #AEF1AE'>      &quot;digest_To_Longs&quot;,&quot;digest_To_Longs_Struct&quot;,&quot;rule_Suite_Result_Details&quot;,&quot;id_Equal&quot;,&quot;long_Pair_Equal&quot;,&quot;big_Bloom&quot;,&quot;small_Bloom&quot;,
</span>56 <span style=''></span><span style='background: #AEF1AE'>      &quot;long_Pair_From_UUID&quot;,&quot;long_Pair&quot;,&quot;rng_UUID&quot;,&quot;rng&quot;,&quot;rng_Bytes&quot;,&quot;return_Sum&quot;,&quot;sum_With&quot;,&quot;results_With&quot;,
</span>57 <span style=''></span><span style='background: #AEF1AE'>      &quot;inc&quot;,&quot;meanF&quot;,&quot;agg_Expr&quot;,&quot;passed&quot;,&quot;failed&quot;,&quot;soft_Failed&quot;,&quot;disabled_Rule&quot;,&quot;pack_Ints&quot;,&quot;unpack&quot;,
</span>58 <span style=''></span><span style='background: #AEF1AE'>      &quot;unpack_Id_Triple&quot;,&quot;soft_Fail&quot;,&quot;probability&quot;,&quot;flatten_Results&quot;,&quot;flatten_Rule_Results&quot;, &quot;flatten_Folder_Results&quot;, &quot;probability_In&quot;,
</span>59 <span style=''></span><span style='background: #AEF1AE'>      &quot;map_Lookup&quot;,&quot;map_Contains&quot;,&quot;hash_With&quot;,&quot;hash_With_Struct&quot;,&quot;za_Hash_With&quot;, &quot;za_Hash_Longs_With&quot;,
</span>60 <span style=''></span><span style='background: #AEF1AE'>      &quot;hash_Field_Based_ID&quot;,&quot;za_Longs_Field_Based_ID&quot;,&quot;za_Hash_Longs_With_Struct&quot;, &quot;za_Hash_With_Struct&quot;, &quot;za_Field_Based_ID&quot;, &quot;prefixed_To_Long_Pair&quot;,
</span>61 <span style=''></span><span style='background: #AEF1AE'>      &quot;coalesce_If_Attributes_Missing&quot;, &quot;coalesce_If_Attributes_Missing_Disable&quot;, &quot;update_Field&quot;, LambdaFunctions.PlaceHolder,
</span>62 <span style=''></span><span style='background: #AEF1AE'>      LambdaFunctions.Lambda, LambdaFunctions.CallFun, &quot;print_Expr&quot;, &quot;print_Code&quot;, &quot;comparable_Maps&quot;, &quot;reverse_Comparable_Maps&quot;, &quot;as_uuid&quot;,
</span>63 <span style=''></span><span style='background: #AEF1AE'>      &quot;id_size&quot;, &quot;id_base64&quot;, &quot;id_from_base64&quot;, &quot;id_raw_type&quot;, &quot;rule_result&quot;, &quot;strip_result_ddl&quot;, &quot;drop_field&quot;,
</span>64 <span style=''></span><span style='background: #AEF1AE'>      &quot;to_yaml&quot;, &quot;from_yaml&quot;
</span>65 <span style=''></span><span style='background: #AEF1AE'>    )</span><span style=''>
</span>66 <span style=''>    </span><span style='background: #AEF1AE'>withUnderscores ++ withUnderscores.map(n =&gt; if (mustKeepNames(n)) n else n.replaceAll(&quot;_&quot;,&quot;&quot;))</span><span style=''>
</span>67 <span style=''>  }
</span>68 <span style=''>
</span>69 <span style=''>  val maxDec = </span><span style='background: #AEF1AE'>DecimalType(DecimalType.MAX_PRECISION, DecimalType.MAX_SCALE)</span><span style=''>
</span>70 <span style=''>
</span>71 <span style=''>  private val noopAdd = (dt: DataType) =&gt; </span><span style='background: #F0ADAD'>None</span><span style=''>
</span>72 <span style=''>
</span>73 <span style=''>  /**
</span>74 <span style=''>   * Provides the default monoidal add for a dataType, used for merging summed results when aggregating
</span>75 <span style=''>   *
</span>76 <span style=''>   * @param dataType
</span>77 <span style=''>   * @return
</span>78 <span style=''>   */
</span>79 <span style=''>  def defaultAdd(dataType: DataType, extension: DataType =&gt; Option[(Expression, Expression) =&gt; Expression] = noopAdd): Option[(Expression, Expression) =&gt; Expression] =
</span>80 <span style=''>    dataType match {
</span>81 <span style=''>      case _: MapType =&gt; </span><span style='background: #AEF1AE'>Some((left, right) =&gt; MapMerge(Seq(left, right), (dataType: DataType) =&gt; defaultAdd(dataType, extension)))</span><span style=''>
</span>82 <span style=''>      case _: IntegerType | LongType | DoubleType =&gt;
</span>83 <span style=''>        </span><span style='background: #AEF1AE'>Some((left, right) =&gt; add(left, right, null))</span><span style=''>
</span>84 <span style=''>      case a: DecimalType =&gt;
</span>85 <span style=''>        </span><span style='background: #AEF1AE'>Some((left, right) =&gt; add(left, right, a))</span><span style=''>
</span>86 <span style=''>      case _ =&gt; </span><span style='background: #F0ADAD'>extension(dataType)</span><span style=''>
</span>87 <span style=''>    }
</span>88 <span style=''>
</span>89 <span style=''>  /**
</span>90 <span style=''>   * Provides the default monoidal Zero for a dataType, used for defaults when aggregating
</span>91 <span style=''>   *
</span>92 <span style=''>   * @param dataType
</span>93 <span style=''>   * @return
</span>94 <span style=''>   */
</span>95 <span style=''>  def defaultZero(dataType: DataType): Option[Any] =
</span>96 <span style=''>    dataType match {
</span>97 <span style=''>      case _: MapType =&gt; </span><span style='background: #AEF1AE'>Some(EmptyMap)</span><span style=''>
</span>98 <span style=''>      case _: IntegerType | LongType =&gt; </span><span style='background: #AEF1AE'>Some(0L)</span><span style=''>
</span>99 <span style=''>      case _: DoubleType =&gt; </span><span style='background: #AEF1AE'>Some(0.0)</span><span style=''>
</span>100 <span style=''>      case d: DecimalType =&gt; </span><span style='background: #AEF1AE'>Some(Decimal.createUnsafe(0, d.precision, d.scale))</span><span style=''>
</span>101 <span style=''>      case _ =&gt; </span><span style='background: #AEF1AE'>None</span><span style=''>
</span>102 <span style=''>    }
</span>103 <span style=''>
</span>104 <span style=''>  /**
</span>105 <span style=''>   * Wrap to provide the default lookup for registerFunctions to change type parsing from DDL based to other or
</span>106 <span style=''>   * when None to add additional lookups should ddl fail
</span>107 <span style=''>   *
</span>108 <span style=''>   * @param string
</span>109 <span style=''>   * @return
</span>110 <span style=''>   */
</span>111 <span style=''>  def defaultParseTypes(string: String): Option[DataType] =
</span>112 <span style=''>    try {
</span>113 <span style=''>      </span><span style='background: #AEF1AE'>Some(
</span>114 <span style=''></span><span style='background: #AEF1AE'>        DataType.fromDDL(string)
</span>115 <span style=''></span><span style='background: #AEF1AE'>      )</span><span style=''>
</span>116 <span style=''>    } catch {
</span>117 <span style=''>      case _: Throwable =&gt; </span><span style='background: #AEF1AE'>None</span><span style=''>
</span>118 <span style=''>    }
</span>119 <span style=''>
</span>120 <span style=''>  val INC_REWRITE_GENEXP_ERR_MSG: String = </span><span style='background: #AEF1AE'>&quot;inc('DDL', generic expression) is not supported in NO_REWRITE mode, use inc(generic expression) without NO_REWRITE mode enabled&quot;</span><span style=''>
</span>121 <span style=''>
</span>122 <span style=''>  // #12 - use underscore names but keep the old camel case approach around for compat
</span>123 <span style=''>  def registerWithChecks(registerFunction: (String, Seq[Expression] =&gt; Expression) =&gt; Unit, name: String, argsf: Seq[Expression] =&gt; Expression, paramNumbers: Set[Int] = Set.empty, minimum: Int = -1) = {
</span>124 <span style=''>    val create =
</span>125 <span style=''>      if (</span><span style='background: #AEF1AE'>paramNumbers.isEmpty &amp;&amp; minimum == -1</span><span style=''> )
</span>126 <span style=''>        </span><span style='background: #AEF1AE'>argsf</span><span style=''>
</span>127 <span style=''>      else
</span>128 <span style=''>        </span><span style='background: #AEF1AE'>(exps: Seq[Expression]) =&gt; {
</span>129 <span style=''></span><span style='background: #AEF1AE'>          if ((paramNumbers.nonEmpty &amp;&amp; !paramNumbers.contains(exps.size)) || (minimum &gt; exps.size)) {
</span>130 <span style=''></span><span style='background: #AEF1AE'>            val sizeerr =
</span>131 <span style=''></span><span style='background: #AEF1AE'>              if (paramNumbers.nonEmpty)
</span>132 <span style=''></span><span style='background: #AEF1AE'>                s&quot;Valid parameter counts are ${paramNumbers.mkString(&quot;, &quot;)}&quot;
</span>133 <span style=''></span><span style='background: #AEF1AE'>              else
</span>134 <span style=''></span><span style='background: #AEF1AE'>                s&quot;A minimum of $minimum parameters is required.&quot;
</span>135 <span style=''></span><span style='background: #AEF1AE'>            throw QualityException(s&quot;Wrong number of arguments provided to Quality function $name. $sizeerr&quot;)
</span>136 <span style=''></span><span style='background: #AEF1AE'>          }
</span>137 <span style=''></span><span style='background: #AEF1AE'>          argsf(exps)
</span>138 <span style=''></span><span style='background: #AEF1AE'>        }</span><span style=''>
</span>139 <span style=''>    </span><span style='background: #AEF1AE'>registerFunction(name, create)</span><span style=''>
</span>140 <span style=''>    if (</span><span style='background: #AEF1AE'>!mustKeepNames(name)</span><span style=''>) {
</span>141 <span style=''>      </span><span style='background: #AEF1AE'>registerFunction(name.replaceAll(&quot;_&quot;,&quot;&quot;), create)</span><span style=''>
</span>142 <span style=''>    }
</span>143 <span style=''>  }
</span>144 <span style=''>
</span>145 <span style=''>  /**
</span>146 <span style=''>   * Must be called before using any functions like Passed, Failed or Probability(X)
</span>147 <span style=''>   * @param parseTypes override type parsing (e.g. DDL, defaults to defaultParseTypes / DataType.fromDDL)
</span>148 <span style=''>   * @param zero override zero creation for aggExpr (defaults to defaultZero)
</span>149 <span style=''>   * @param add override the &quot;add&quot; function for aggExpr types (defaults to defaultAdd(dataType))
</span>150 <span style=''>   * @param writer override the printCode and printExpr print writing function (defaults to println)
</span>151 <span style=''>   * @param registerFunction function to register the sql extensions
</span>152 <span style=''>   */
</span>153 <span style=''>  def registerQualityFunctions(parseTypes: String =&gt; Option[DataType] = defaultParseTypes _,
</span>154 <span style=''>                               zero: DataType =&gt; Option[Any] = defaultZero _,
</span>155 <span style=''>                               add: DataType =&gt; Option[(Expression, Expression) =&gt; Expression] = (dataType: DataType) =&gt; defaultAdd(dataType),
</span>156 <span style=''>                               mapCompare: DataType =&gt; Option[(Any, Any) =&gt; Int] = (dataType: DataType) =&gt; utils.defaultMapCompare(dataType),
</span>157 <span style=''>                               writer: String =&gt; Unit = println(_),
</span>158 <span style=''>                               registerFunction: (String, Seq[Expression] =&gt; Expression) =&gt; Unit =
</span>159 <span style=''>                                 ShimUtils.registerFunction(SparkSession.getActiveSession.get.sessionState.functionRegistry) _
</span>160 <span style=''>                              ) {
</span>161 <span style=''>
</span>162 <span style=''>    def register(name: String, argsf: Seq[Expression] =&gt; Expression, paramNumbers: Set[Int] = Set.empty, minimum: Int = -1) =
</span>163 <span style=''>      </span><span style='background: #AEF1AE'>registerWithChecks(registerFunction, name, argsf, paramNumbers, minimum)</span><span style=''>
</span>164 <span style=''>
</span>165 <span style=''>    def parse(exp: Expression) = {
</span>166 <span style=''>      val Literal(str: UTF8String, StringType) = exp // only accept type as string
</span>167 <span style=''>      </span><span style='background: #AEF1AE'>parseTypes(str.toString).getOrElse(qualityException(s&quot;Could not parse the type $str&quot;))</span><span style=''>
</span>168 <span style=''>    }
</span>169 <span style=''>
</span>170 <span style=''>    def getMap(exp: Expression) = exp match {
</span>171 <span style=''>      case c: CreateMap if </span><span style='background: #AEF1AE'>c.children.grouped(2).forall{
</span>172 <span style=''></span><span style='background: #AEF1AE'>        case Seq(Literal(_: UTF8String, StringType), _: Literal) =&gt;
</span>173 <span style=''></span><span style='background: #AEF1AE'>          true
</span>174 <span style=''></span><span style='background: #AEF1AE'>        case _ =&gt; false
</span>175 <span style=''></span><span style='background: #AEF1AE'>      }</span><span style=''> =&gt;
</span>176 <span style=''>        </span><span style='background: #AEF1AE'>c.children.grouped(2).map{
</span>177 <span style=''></span><span style='background: #AEF1AE'>          case Seq(Literal(str: UTF8String, StringType), value: Literal) =&gt;
</span>178 <span style=''></span><span style='background: #AEF1AE'>            str.toString -&gt; value.value.toString()
</span>179 <span style=''></span><span style='background: #AEF1AE'>        }.toMap</span><span style=''>
</span>180 <span style=''>      case _ =&gt; </span><span style='background: #AEF1AE'>throw QualityException(s&quot;Could not process a literal map with expression $exp&quot;)</span><span style=''>
</span>181 <span style=''>    }
</span>182 <span style=''>
</span>183 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;processor_input_wrapper&quot;, exps =&gt; InputWrapper(exps.head, exps.last), minimum = 2)</span><span style=''>
</span>184 <span style=''>
</span>185 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;to_yaml&quot;, exps =&gt; YamlEncoderExpr(exps.head, if (exps.size == 1) Map.empty else getMap(exps.last)), Set(1, 2))</span><span style=''>
</span>186 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;from_yaml&quot;, exps =&gt; YamlDecoderExpr(exps.head, parse(exps.last)), Set(2))</span><span style=''>
</span>187 <span style=''>
</span>188 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;strip_result_ddl&quot;, exps =&gt; StripResultTypes(exps.head), Set(1))</span><span style=''>
</span>189 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;rule_result&quot;, exps =&gt; RuleResultExpression(Seq(exps(0), exps(1), exps(2), exps(3))), Set(4))</span><span style=''>
</span>190 <span style=''>
</span>191 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;comparable_Maps&quot;, exps =&gt; ComparableMapConverter(exps(0), mapCompare), Set(1))</span><span style=''>
</span>192 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;reverse_Comparable_Maps&quot;, exps =&gt; ComparableMapReverser(exps.head), Set(1))</span><span style=''>
</span>193 <span style=''>
</span>194 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;probability&quot;, exps =&gt; ProbabilityExpr(exps.head), Set(1))</span><span style=''>
</span>195 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;flatten_Results&quot;, exps =&gt; FlattenResultsExpression(exps.head, FlattenStruct.ruleSuiteDeserializer), Set(1))</span><span style=''>
</span>196 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;flatten_Rule_Results&quot;, exps =&gt; FlattenRulesResultsExpression(exps.head, FlattenStruct.ruleSuiteDeserializer), Set(1))</span><span style=''>
</span>197 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;flatten_Folder_Results&quot;, exps =&gt; FlattenFolderResultsExpression(exps.head, FlattenStruct.ruleSuiteDeserializer), Set(1))</span><span style=''>
</span>198 <span style=''>
</span>199 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;passed&quot;, _ =&gt; com.sparkutils.quality.impl.imports.RuleResultsImports.PassedExpr, Set(0))</span><span style=''>
</span>200 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;failed&quot;, _ =&gt; com.sparkutils.quality.impl.imports.RuleResultsImports.FailedExpr, Set(0))</span><span style=''>
</span>201 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;soft_Failed&quot;, _ =&gt; com.sparkutils.quality.impl.imports.RuleResultsImports.SoftFailedExpr, Set(0))</span><span style=''>
</span>202 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;disabled_Rule&quot;, _ =&gt; com.sparkutils.quality.impl.imports.RuleResultsImports.DisabledRuleExpr, Set(0))</span><span style=''>
</span>203 <span style=''>
</span>204 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;pack_Ints&quot;, exps =&gt; Pack(exps(0), exps(1)), Set(2))</span><span style=''>
</span>205 <span style=''>
</span>206 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;unpack&quot;, exps =&gt; UnPack(exps.head), Set(1))</span><span style=''>
</span>207 <span style=''>
</span>208 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;unpack_Id_Triple&quot;, exps =&gt; UnPackIdTriple(exps.head), Set(1))</span><span style=''>
</span>209 <span style=''>
</span>210 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;soft_Fail&quot;, exps =&gt; SoftFailExpr(exps.head), Set(1))</span><span style=''>
</span>211 <span style=''>
</span>212 <span style=''>    def strType(exp: Expression) = {
</span>213 <span style=''>      val Literal(str: UTF8String, StringType) = exp // only accept type as string
</span>214 <span style=''>      </span><span style='background: #AEF1AE'>str.toString</span><span style=''>
</span>215 <span style=''>    }
</span>216 <span style=''>
</span>217 <span style=''>    </span><span style='background: #AEF1AE'>register(LambdaFunctions.PlaceHolder, {
</span>218 <span style=''></span><span style='background: #AEF1AE'>      case Seq(e, Literal(bol: Boolean, BooleanType)) =&gt;
</span>219 <span style=''></span><span style='background: #AEF1AE'>        PlaceHolderExpression(parse(e), bol)
</span>220 <span style=''></span><span style='background: #AEF1AE'>      case Seq(e) =&gt;
</span>221 <span style=''></span><span style='background: #AEF1AE'>        PlaceHolderExpression(parse(e))
</span>222 <span style=''></span><span style='background: #AEF1AE'>      case _ =&gt;
</span>223 <span style=''></span><span style='background: #AEF1AE'>        PlaceHolderExpression(LongType)
</span>224 <span style=''></span><span style='background: #AEF1AE'>    }, Set(0, 1, 2))</span><span style=''>
</span>225 <span style=''>
</span>226 <span style=''>    /* Note - both Lambda and CallFun are only called in top level expressions,
</span>227 <span style=''>          nested calls are handled within the lambda expression
</span>228 <span style=''>          creation that &quot;calls&quot; this. */
</span>229 <span style=''>
</span>230 <span style=''>    </span><span style='background: #AEF1AE'>register(LambdaFunctions.Lambda, {
</span>231 <span style=''></span><span style='background: #AEF1AE'>      case Seq(fun: FunForward) =&gt;
</span>232 <span style=''></span><span style='background: #AEF1AE'>        val res = FunCall(fun)
</span>233 <span style=''></span><span style='background: #AEF1AE'>        res
</span>234 <span style=''></span><span style='background: #AEF1AE'>      case Seq(fun: FunN) =&gt;
</span>235 <span style=''></span><span style='background: #AEF1AE'>        // placeholders that are 1:1
</span>236 <span style=''></span><span style='background: #AEF1AE'>        val res = fun.function
</span>237 <span style=''></span><span style='background: #AEF1AE'>        res
</span>238 <span style=''></span><span style='background: #AEF1AE'>    }, Set(1))</span><span style=''>
</span>239 <span style=''>
</span>240 <span style=''>    </span><span style='background: #AEF1AE'>register(LambdaFunctions.CallFun, {
</span>241 <span style=''></span><span style='background: #AEF1AE'>      case (fun@ FunN(_, l@ SLambdaFunction(ff : FunForward, _, _), _, _, _, _)) +: args =&gt;
</span>242 <span style=''></span><span style='background: #AEF1AE'>        processTopCallFun(fun, l, ff, args)
</span>243 <span style=''></span><span style='background: #AEF1AE'>      case t =&gt; qualityException(s&quot;${LambdaFunctions.CallFun} should only be used to process partially applied functions returned by a user lambda, got $t instead&quot;)
</span>244 <span style=''></span><span style='background: #AEF1AE'>    }, minimum = 1)</span><span style=''>
</span>245 <span style=''>
</span>246 <span style=''>    val afx = (exps: Seq[Expression]) =&gt; {
</span>247 <span style=''>      val (sumType, filter, sum, count) =
</span>248 <span style=''>        exps.size match {
</span>249 <span style=''>          case 3 =&gt;
</span>250 <span style=''>            // attempt to take a look at exps1 to identify if it's a FunN or mapWith
</span>251 <span style=''>            val typ =
</span>252 <span style=''>              exps(1) match {
</span>253 <span style=''>                case FunN(Seq(RefExpression(dataType, _, _)), _, _, _, _, _) =&gt; dataType // would default to long anyway
</span>254 <span style=''>                case MapTransform(RefExpression(t: MapType, _, _), _, _, _) =&gt; t
</span>255 <span style=''>                case _ =&gt; LongType
</span>256 <span style=''>              }
</span>257 <span style=''>            (typ, exps(0), exps(1), exps(2))
</span>258 <span style=''>          case 4 =&gt;
</span>259 <span style=''>            val Literal(str: UTF8String, StringType) = exps(0) // only accept type as string
</span>260 <span style=''>            if (str.toString == &quot;NO_REWRITE&quot;)
</span>261 <span style=''>            // signal not to replace types
</span>262 <span style=''>            (null, exps(1), exps(2), exps(3))
</span>263 <span style=''>              else
</span>264 <span style=''>              (parse(exps(0)), exps(1), exps(2), exps(3))
</span>265 <span style=''>        }
</span>266 <span style=''>      </span><span style='background: #AEF1AE'>AggregateExpressions(sumType, filter, sum, count, zero, add)</span><span style=''>
</span>267 <span style=''>    }
</span>268 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;agg_Expr&quot;, afx, Set(3, 4))</span><span style=''>
</span>269 <span style=''>
</span>270 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;sum_With&quot;, (exps: Seq[Expression]) =&gt; {
</span>271 <span style=''></span><span style='background: #AEF1AE'>      val (dataType, origExp) = exps.size match {
</span>272 <span style=''></span><span style='background: #AEF1AE'>        case 1 =&gt; (LongType, exps(0))
</span>273 <span style=''></span><span style='background: #AEF1AE'>        // backwards compat
</span>274 <span style=''></span><span style='background: #AEF1AE'>        case 2 =&gt; (parse(exps(0)), exps(1))
</span>275 <span style=''></span><span style='background: #AEF1AE'>      }
</span>276 <span style=''></span><span style='background: #AEF1AE'>      FunN(Seq(RefExpression(dataType)), origExp, Some(&quot;sum_With&quot;))
</span>277 <span style=''></span><span style='background: #AEF1AE'>    }, Set(1, 2))</span><span style=''>
</span>278 <span style=''>
</span>279 <span style=''>    val ff2 = (exps: Seq[Expression]) =&gt; {
</span>280 <span style=''>      // real type for param1 is changed by aggrExpr, but last works for all compat as well
</span>281 <span style=''>      val (sumType, exp) =
</span>282 <span style=''>        exps.size match {
</span>283 <span style=''>          case 1 =&gt; (LongType, exps(0))
</span>284 <span style=''>          case 2 =&gt; (parse(exps(0)), exps(1)) // support the NO_REWRITE override case
</span>285 <span style=''>        }
</span>286 <span style=''>
</span>287 <span style=''>      </span><span style='background: #AEF1AE'>FunN(Seq(RefExpression(sumType), RefExpression(LongType)), exp, Some(&quot;results_With&quot;))</span><span style=''>
</span>288 <span style=''>    }
</span>289 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;results_With&quot;, ff2, Set(1, 2))</span><span style=''>
</span>290 <span style=''>
</span>291 <span style=''>    val mapFX = (exps: Seq[Expression]) =&gt; </span><span style='background: #AEF1AE'>exps.size</span><span style=''> match {
</span>292 <span style=''>      case 3 =&gt;
</span>293 <span style=''>        // parse it for old sql to support backwards aggExpr
</span>294 <span style=''>        </span><span style='background: #AEF1AE'>MapTransform.create(RefExpression(parse(exps(0))), exps(1), exps(2), zero)</span><span style=''>
</span>295 <span style=''>      case 2 =&gt;
</span>296 <span style=''>        // default to LongType, aggrExpr must fix, 2nd param is key, third the func manipulating the key
</span>297 <span style=''>        </span><span style='background: #AEF1AE'>MapTransform.create(RefExpression(MapType(LongType, LongType)), exps(0), exps(1), zero)</span><span style=''>
</span>298 <span style=''>    }
</span>299 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;map_With&quot;, mapFX, Set(2, 3))</span><span style=''>
</span>300 <span style=''>
</span>301 <span style=''>    def aggFWith(fun: String) = (what: String) =&gt; (exps: Seq[Expression]) =&gt; </span><span style='background: #AEF1AE'>expression(
</span>302 <span style=''></span><span style='background: #AEF1AE'>      if (exps.size == 0)
</span>303 <span style=''></span><span style='background: #AEF1AE'>        functions.expr(s&quot;$fun( $what )&quot;)
</span>304 <span style=''></span><span style='background: #AEF1AE'>      else
</span>305 <span style=''></span><span style='background: #AEF1AE'>        functions.expr(s&quot;$fun('${strType(exps(0))}', $what )&quot;)
</span>306 <span style=''></span><span style='background: #AEF1AE'>      )</span><span style=''>
</span>307 <span style=''>
</span>308 <span style=''>    val retWith = </span><span style='background: #AEF1AE'>aggFWith(&quot;results_With&quot;)</span><span style=''>
</span>309 <span style=''>
</span>310 <span style=''>    // common cases
</span>311 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;meanF&quot;, retWith(&quot;(sum, count) -&gt; sum / count&quot;), Set(0, 1))</span><span style=''>
</span>312 <span style=''>
</span>313 <span style=''>    val sumWith = </span><span style='background: #AEF1AE'>aggFWith(&quot;sum_With&quot;)</span><span style=''>
</span>314 <span style=''>
</span>315 <span style=''>    val incX = (exps: Seq[Expression]) =&gt; exps match {
</span>316 <span style=''>      case Seq(x: AttributeReference) =&gt;
</span>317 <span style=''>        val name = </span><span style='background: #AEF1AE'>x.qualifier.mkString(&quot;.&quot;) + x.name</span><span style=''> // that is bad code man should be option
</span>318 <span style=''>        </span><span style='background: #AEF1AE'>sumWith(s&quot;sum -&gt; sum + $name&quot;)(Seq())</span><span style=''>
</span>319 <span style=''>      case Seq(Literal(str: UTF8String, StringType)) =&gt;
</span>320 <span style=''>        // case for type passing
</span>321 <span style=''>        </span><span style='background: #AEF1AE'>sumWith(&quot;sum -&gt; sum + 1&quot;)(exps)</span><span style=''>
</span>322 <span style=''>      case Seq(Literal(str: UTF8String, StringType), x: AttributeReference) =&gt;
</span>323 <span style=''>        val name = </span><span style='background: #AEF1AE'>x.qualifier.mkString(&quot;.&quot;) + x.name</span><span style=''>
</span>324 <span style=''>        </span><span style='background: #AEF1AE'>sumWith(s&quot;sum -&gt; sum + $name&quot;)(Seq(exps(0)))</span><span style=''> // keep the type, drop the attr
</span>325 <span style=''>      case Seq(Literal(str: UTF8String, StringType), y) =&gt;
</span>326 <span style=''>        </span><span style='background: #AEF1AE'>qualityException(INC_REWRITE_GENEXP_ERR_MSG)</span><span style=''>
</span>327 <span style=''>      case Seq( y ) =&gt;
</span>328 <span style=''>        val SLambdaFunction(a: Add, Seq(sum: UnresolvedNamedLambdaVariable), hidden ) = expression(functions.expr(&quot;sumWith(sum -&gt; sum + 1)&quot;)).children(0)
</span>329 <span style=''>        import ShimUtils.{add =&gt; addf}
</span>330 <span style=''>        // could be a cast around x or three attributes plusing each other or....
</span>331 <span style=''>        </span><span style='background: #AEF1AE'>FunN(Seq(RefExpression(LongType)),
</span>332 <span style=''></span><span style='background: #AEF1AE'>          SLambdaFunction(addf(a.left, y, LongType), Seq(sum), hidden )
</span>333 <span style=''></span><span style='background: #AEF1AE'>          , Some(&quot;inc&quot;))</span><span style=''> // keep the type
</span>334 <span style=''>      case Seq() =&gt; </span><span style='background: #AEF1AE'>expression(functions.expr(s&quot;sumWith(sum -&gt; sum + 1)&quot;))</span><span style=''>
</span>335 <span style=''>    }
</span>336 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;inc&quot;, incX, Set(1, 0, 2))</span><span style=''>
</span>337 <span style=''>
</span>338 <span style=''>    // return sum
</span>339 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;return_Sum&quot;, retWith(&quot;(sum, count) -&gt; sum&quot;), Set(0, 1))</span><span style=''>
</span>340 <span style=''>    def getRandom(exp: Expression, pos: Int) = {
</span>341 <span style=''>      val str = </span><span style='background: #AEF1AE'>getString(exp, pos)</span><span style=''>
</span>342 <span style=''>      </span><span style='background: #AEF1AE'>RandomSource.valueOf(str)</span><span style=''>
</span>343 <span style=''>    }
</span>344 <span style=''>
</span>345 <span style=''>    // random generators
</span>346 <span style=''>    val brf = (exps: Seq[Expression]) =&gt; {
</span>347 <span style=''>      //numBytes: Int, randomSource: RandomSource, seed: Long constructor but needs to use random, seed, numbytes
</span>348 <span style=''>      val (numBytes: Int, randomSource, seed: Long) =
</span>349 <span style=''>        exps.size match {
</span>350 <span style=''>          case 0 =&gt; (16, RandomSource.XO_RO_SHI_RO_128_PP, 0L)
</span>351 <span style=''>          case 1 =&gt; (16, getRandom(exps(0), 0), 0L)
</span>352 <span style=''>          case 2 =&gt; (16, getRandom(exps(0), 0), getLong(exps(1), 1))
</span>353 <span style=''>          case 3 =&gt; (getLong(exps(2)).toInt, getRandom(exps(0), 0), getLong(exps(1), 1))
</span>354 <span style=''>          case _ =&gt; literalsNeeded
</span>355 <span style=''>        }
</span>356 <span style=''>
</span>357 <span style=''>      </span><span style='background: #AEF1AE'>RandomBytes(numBytes, randomSource, seed)</span><span style=''>
</span>358 <span style=''>    }
</span>359 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;rng_Bytes&quot;, brf, Set(0,1,2,3))</span><span style=''>
</span>360 <span style=''>
</span>361 <span style=''>    // random generators
</span>362 <span style=''>    val lrf = (exps: Seq[Expression]) =&gt; {
</span>363 <span style=''>
</span>364 <span style=''>      //randomSource: RandomSource, seed: Long constructor but needs to use random, seed, numbytes
</span>365 <span style=''>      val (randomSource, seed: Long) =
</span>366 <span style=''>        exps.size match {
</span>367 <span style=''>          case 0 =&gt; (RandomSource.XO_RO_SHI_RO_128_PP, 0L)
</span>368 <span style=''>          case 1 =&gt; (getRandom(exps(0), 0), 0L)
</span>369 <span style=''>          case 2 =&gt; (getRandom(exps(0), 0), getLong(exps(1), 1))
</span>370 <span style=''>          case _ =&gt; literalsNeeded
</span>371 <span style=''>        }
</span>372 <span style=''>
</span>373 <span style=''>      </span><span style='background: #AEF1AE'>RandomLongs.create(randomSource, seed)</span><span style=''>
</span>374 <span style=''>    }
</span>375 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;rng&quot;, lrf, Set(0,1,2))</span><span style=''>
</span>376 <span style=''>
</span>377 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;rng_UUID&quot;, exps =&gt;
</span>378 <span style=''></span><span style='background: #AEF1AE'>      RngUUIDExpression(exps.head),
</span>379 <span style=''></span><span style='background: #AEF1AE'>      Set(1))</span><span style=''>
</span>380 <span style=''>
</span>381 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;long_Pair&quot;, exps =&gt; LongPairExpression(exps(0), exps(1)), Set(2))</span><span style=''>
</span>382 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;long_Pair_From_UUID&quot;, exps =&gt; UUIDToLongsExpression(exps.head), Set(1))</span><span style=''>
</span>383 <span style=''>
</span>384 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;small_Bloom&quot;, exps =&gt; ParquetAggregator(exps(0), exps(1), exps(2)), Set(3))</span><span style=''>
</span>385 <span style=''>
</span>386 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;big_Bloom&quot;, exps =&gt; exps.size match {
</span>387 <span style=''></span><span style='background: #AEF1AE'>      case 4 =&gt;
</span>388 <span style=''></span><span style='background: #AEF1AE'>        BucketedArrayParquetAggregator(exps(0), exps(1), exps(2), exps(3))
</span>389 <span style=''></span><span style='background: #AEF1AE'>      case 3 =&gt;
</span>390 <span style=''></span><span style='background: #AEF1AE'>        BucketedArrayParquetAggregator(exps(0), exps(1), exps(2), Literal(java.util.UUID.randomUUID().toString))
</span>391 <span style=''></span><span style='background: #AEF1AE'>    }, Set(3,4))</span><span style=''>
</span>392 <span style=''>
</span>393 <span style=''>
</span>394 <span style=''>    val longPairEqual = (exps: Seq[Expression]) =&gt; {
</span>395 <span style=''>      val Seq(Literal(a, StringType), Literal(b, StringType)) = exps
</span>396 <span style=''>      </span><span style='background: #AEF1AE'>expression(long_pair_equal(a.toString, b.toString))</span><span style=''>
</span>397 <span style=''>    }
</span>398 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;long_Pair_Equal&quot;, longPairEqual, Set(2))</span><span style=''>
</span>399 <span style=''>
</span>400 <span style=''>    val idEqual = (exps: Seq[Expression]) =&gt; {
</span>401 <span style=''>      val Seq(Literal(a, StringType), Literal(b, StringType)) = exps
</span>402 <span style=''>      </span><span style='background: #AEF1AE'>expression(id_equal(a.toString, b.toString))</span><span style=''>
</span>403 <span style=''>    }
</span>404 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;idEqual&quot;, idEqual, Set(2))</span><span style=''>
</span>405 <span style=''>
</span>406 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;as_uuid&quot;, exps =&gt; AsUUID(exps(0), exps(1)), Set(2))</span><span style=''>
</span>407 <span style=''>
</span>408 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;rule_Suite_Result_Details&quot;, exps =&gt; impl.RuleSuiteResultDetailsExpr(exps(0)), Set(1))</span><span style=''>
</span>409 <span style=''>
</span>410 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;digest_To_Longs_Struct&quot;,  exps =&gt; expression(digest_to_longs_struct(getString(exps.head, 0), exps.tail.map(column(_)): _*)), minimum = 2)</span><span style=''>
</span>411 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;digest_To_Longs&quot;, exps =&gt; expression(digest_to_longs(getString(exps.head, 0), exps.tail.map(column(_)): _*)), minimum = 2)</span><span style=''>
</span>412 <span style=''>
</span>413 <span style=''>    def fieldBasedID(func: (String, String, Seq[Column]) =&gt; Column) = (exps: Seq[Expression]) =&gt;
</span>414 <span style=''>      </span><span style='background: #AEF1AE'>exps.size</span><span style=''> match {
</span>415 <span style=''>        case a if </span><span style='background: #AEF1AE'>a &gt; 3</span><span style=''> =&gt;
</span>416 <span style=''>          val digestImpl = </span><span style='background: #AEF1AE'>getString(exps(1), 1)</span><span style=''>
</span>417 <span style=''>          val prefix = </span><span style='background: #AEF1AE'>getString(exps.head, 0)</span><span style=''>
</span>418 <span style=''>          </span><span style='background: #AEF1AE'>expression(func(prefix, digestImpl, exps.drop(2).map(column(_)) ))</span><span style=''>
</span>419 <span style=''>
</span>420 <span style=''>        case _ =&gt; </span><span style='background: #AEF1AE'>literalsNeeded</span><span style=''>
</span>421 <span style=''>      }
</span>422 <span style=''>
</span>423 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;field_Based_ID&quot;, fieldBasedID(field_based_id _), minimum = 3)</span><span style=''>
</span>424 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;za_Longs_Field_Based_ID&quot;, fieldBasedID(za_longs_field_based_id _), minimum = 3)</span><span style=''>
</span>425 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;za_Field_Based_ID&quot;, fieldBasedID(za_field_based_id _), minimum = 3)</span><span style=''>
</span>426 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;hash_Field_Based_ID&quot;, fieldBasedID(hash_field_based_id _), minimum = 3)</span><span style=''>
</span>427 <span style=''>
</span>428 <span style=''>    val providedID = (exps: Seq[Expression]) =&gt;
</span>429 <span style=''>      </span><span style='background: #AEF1AE'>exps.size</span><span style=''> match {
</span>430 <span style=''>        case 2 =&gt;
</span>431 <span style=''>          </span><span style='background: #AEF1AE'>GenericLongBasedIDExpression(model.ProvidedID,
</span>432 <span style=''></span><span style='background: #AEF1AE'>            exps(1), getString(exps.head, 0))</span><span style=''>
</span>433 <span style=''>
</span>434 <span style=''>        case _ =&gt; </span><span style='background: #AEF1AE'>literalsNeeded</span><span style=''>
</span>435 <span style=''>      }
</span>436 <span style=''>
</span>437 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;provided_ID&quot;, providedID, Set(2))</span><span style=''>
</span>438 <span style=''>
</span>439 <span style=''>    val prefixedToLongPair = (exps: Seq[Expression]) =&gt;
</span>440 <span style=''>      </span><span style='background: #AEF1AE'>exps.size</span><span style=''> match {
</span>441 <span style=''>        case 2 =&gt;
</span>442 <span style=''>          </span><span style='background: #AEF1AE'>expression(prefixed_to_long_pair(column(exps(1)), getString(exps.head, 0)))</span><span style=''>
</span>443 <span style=''>
</span>444 <span style=''>        case _ =&gt; </span><span style='background: #AEF1AE'>literalsNeeded</span><span style=''>
</span>445 <span style=''>      }
</span>446 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;prefixed_To_Long_Pair&quot;, prefixedToLongPair, Set(2))</span><span style=''>
</span>447 <span style=''>
</span>448 <span style=''>    val rngID = (exps: Seq[Expression]) =&gt; {
</span>449 <span style=''>      val (randomSource, seed: Long, prefix) =
</span>450 <span style=''>        exps.size match {
</span>451 <span style=''>          case 1 =&gt; ( RandomSource.XO_RO_SHI_RO_128_PP, 0L, getString(exps.head, 0))
</span>452 <span style=''>          case 2 =&gt; ( getRandom(exps(1), 1), 0L,  getString(exps.head, 0))
</span>453 <span style=''>          case 3 =&gt; ( getRandom(exps(1), 1), getLong(exps(2), 1),  getString(exps.head, 0))
</span>454 <span style=''>          case _ =&gt; literalsNeeded
</span>455 <span style=''>        }
</span>456 <span style=''>
</span>457 <span style=''>      </span><span style='background: #AEF1AE'>expression(rng_id(prefix, randomSource, seed))</span><span style=''>
</span>458 <span style=''>    }
</span>459 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;rng_ID&quot;, rngID, Set(1,2,3))</span><span style=''>
</span>460 <span style=''>
</span>461 <span style=''>    val uniqueID = (exps: Seq[Expression]) =&gt; {
</span>462 <span style=''>      val (prefix) =
</span>463 <span style=''>        </span><span style='background: #AEF1AE'>exps.size</span><span style=''> match {
</span>464 <span style=''>          case 1 =&gt; </span><span style='background: #AEF1AE'>getString(exps.head, 0)</span><span style=''>
</span>465 <span style=''>          case _ =&gt; </span><span style='background: #AEF1AE'>literalsNeeded</span><span style=''>
</span>466 <span style=''>        }
</span>467 <span style=''>
</span>468 <span style=''>      </span><span style='background: #AEF1AE'>expression(unique_id(prefix))</span><span style=''>
</span>469 <span style=''>    }
</span>470 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;unique_ID&quot;, uniqueID, Set(1))</span><span style=''>
</span>471 <span style=''>
</span>472 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;id_size&quot;, exps =&gt; expression(id_size(column(exps.head))), Set(1))</span><span style=''>
</span>473 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;id_base64&quot;, exps =&gt; expression(id_base64(exps.map(column(_)): _*)), minimum = 1)</span><span style=''>
</span>474 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;id_from_base64&quot;, {
</span>475 <span style=''></span><span style='background: #AEF1AE'>      case Seq(e) =&gt; expression(id_from_base64(column(e)))
</span>476 <span style=''></span><span style='background: #AEF1AE'>      case Seq(e, s) =&gt; expression(id_from_base64(column(e), getInteger(s)))
</span>477 <span style=''></span><span style='background: #AEF1AE'>    }, Set(1,2))</span><span style=''>
</span>478 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;id_raw_type&quot;, exps =&gt; expression(id_raw_type(column(exps.head))), Set(1))</span><span style=''>
</span>479 <span style=''>
</span>480 <span style=''>    val Murmur3_128_64 = (exps: Seq[Expression]) =&gt; {
</span>481 <span style=''>      val (prefix) =
</span>482 <span style=''>        </span><span style='background: #AEF1AE'>exps.size</span><span style=''> match {
</span>483 <span style=''>          case a if </span><span style='background: #AEF1AE'>a &lt; 2</span><span style=''> =&gt; </span><span style='background: #AEF1AE'>literalsNeeded</span><span style=''>
</span>484 <span style=''>          case _ =&gt; </span><span style='background: #AEF1AE'>getString(exps.head, 0)</span><span style=''>
</span>485 <span style=''>        }
</span>486 <span style=''>      </span><span style='background: #AEF1AE'>GenericLongBasedIDExpression(model.FieldBasedID,
</span>487 <span style=''></span><span style='background: #AEF1AE'>        HashFunctionsExpression(exps.tail, &quot;IGNORED&quot;, true, HashFunctionFactory(&quot;IGNORED&quot;)), prefix)</span><span style=''>
</span>488 <span style=''>    }
</span>489 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;murmur3_ID&quot;, Murmur3_128_64, minimum = 2)</span><span style=''>
</span>490 <span style=''>
</span>491 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;hash_With&quot;, exps =&gt; expression(hash_with(getString(exps.head, 0), exps.tail.map(column(_)) :_*)), minimum = 2)</span><span style=''>
</span>492 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;hash_With_Struct&quot;, exps =&gt; expression(hash_with_struct(getString(exps.head, 0), exps.tail.map(column(_)) :_*)), minimum = 2)</span><span style=''>
</span>493 <span style=''>
</span>494 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;za_Hash_With&quot;, exps =&gt; expression(za_hash_with(getString(exps.head, 0), exps.tail.map(column(_)) :_*)), minimum = 2)</span><span style=''> // 64bit only, not a great id choice
</span>495 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;za_Hash_With_Struct&quot;, exps =&gt; expression(za_hash_with_struct(getString(exps.head,0 ), exps.tail.map(column(_)) :_*)), minimum = 2)</span><span style=''> // 64bit only, not a great id choice
</span>496 <span style=''>
</span>497 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;za_Hash_Longs_With&quot;, exps =&gt; expression(za_hash_longs_with(getString(exps.head, 0), exps.tail.map(column(_)) :_*)), minimum = 2)</span><span style=''>
</span>498 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;za_Hash_Longs_With_Struct&quot;, exps =&gt; expression(za_hash_longs_with_struct(getString(exps.head, 0), exps.tail.map(column(_)) :_*)), minimum = 2)</span><span style=''>
</span>499 <span style=''>
</span>500 <span style=''>    // here to stop these functions being used and allow validation
</span>501 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;coalesce_If_Attributes_Missing&quot;, _ =&gt; qualityException(&quot;coalesceIf functions cannot be created&quot;) )</span><span style=''>
</span>502 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;coalesce_If_Attributes_Missing_Disable&quot;, _ =&gt; qualityException(&quot;coalesceIf functions cannot be created&quot;) )</span><span style=''>
</span>503 <span style=''>
</span>504 <span style=''>    // 3.0.1 adds this #37 drops 3.0.0 and we can remove the c+p from 3.4.1 needed due to #36
</span>505 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;update_field&quot;, exps =&gt; {
</span>506 <span style=''></span><span style='background: #AEF1AE'>      expression(update_field(column(exps.head), ( exps.tail.grouped(2).map(p =&gt; getString(p.head, 0) -&gt; column(p.last)).toSeq): _*))
</span>507 <span style=''></span><span style='background: #AEF1AE'>    }, minimum = 3)</span><span style=''>
</span>508 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;drop_field&quot;, exps =&gt; {
</span>509 <span style=''></span><span style='background: #AEF1AE'>      expression(drop_field(column(exps.head), exps.tail.zipWithIndex.map{case (p, i) =&gt; getString(p, i+1)} : _*))
</span>510 <span style=''></span><span style='background: #AEF1AE'>    }, minimum = 2)</span><span style=''>
</span>511 <span style=''>
</span>512 <span style=''>    def msgAndExpr(msgDefault: String, exps: Seq[Expression]) = exps match {
</span>513 <span style=''>      case Seq(Literal(str: UTF8String, StringType), e: Expression) =&gt;
</span>514 <span style=''>        </span><span style='background: #AEF1AE'>(str.toString, e)</span><span style=''>
</span>515 <span style=''>      case Seq(e: Expression) =&gt;
</span>516 <span style=''>        </span><span style='background: #AEF1AE'>(msgDefault, e)</span><span style=''>
</span>517 <span style=''>    }
</span>518 <span style=''>
</span>519 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;print_Code&quot;, (exps: Seq[Expression]) =&gt; {
</span>520 <span style=''></span><span style='background: #AEF1AE'>      val (msg, exp) = msgAndExpr(PrintCode(exps(0)).msg, exps)
</span>521 <span style=''></span><span style='background: #AEF1AE'>      PrintCode(exp, msg, writer)
</span>522 <span style=''></span><span style='background: #AEF1AE'>    }, Set(1, 2))</span><span style=''>
</span>523 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;print_Expr&quot;, (exps: Seq[Expression]) =&gt; {
</span>524 <span style=''></span><span style='background: #AEF1AE'>      val (msg, exp) = msgAndExpr(&quot;Expression toStr is -&gt;&quot;, exps)
</span>525 <span style=''></span><span style='background: #AEF1AE'>      writer(s&quot;$msg $exp .  Sql is ${exp.sql}&quot;)
</span>526 <span style=''></span><span style='background: #AEF1AE'>      exp
</span>527 <span style=''></span><span style='background: #AEF1AE'>    }, Set(1,2))</span><span style=''>
</span>528 <span style=''>  }
</span>529 <span style=''>
</span>530 <span style=''>}
</span></pre>
          </div>
          <div class="tab-pane" id="statementlist">
            <table cellspacing="0" cellpadding="0" class="table statementlist">
      <tr>
        <th>Line</th>
        <th>Stmt Id</th>
        <th>Pos</th>
        <th>Tree</th>
        <th>Symbol</th>
        <th>Tests</th>
        <th>Code</th>
      </tr><tr>
        <td>
          27
        </td>
        <td>
          2334
        </td>
        <td>
          1551
          -
          1560
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.==
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          pos.==(-1)
        </td>
      </tr><tr>
        <td>
          28
        </td>
        <td>
          2335
        </td>
        <td>
          1585
          -
          1636
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;Cannot setup Quality Expression with non-literals&quot;
        </td>
      </tr><tr>
        <td>
          28
        </td>
        <td>
          2338
        </td>
        <td>
          1568
          -
          1637
        </td>
        <td>
          Block
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.sparkutils.quality.QualityException.qualityException(&quot;Cannot setup Quality Expression with non-literals&quot;, com.sparkutils.quality.QualityException.qualityException$default$2)
        </td>
      </tr><tr>
        <td>
          28
        </td>
        <td>
          2337
        </td>
        <td>
          1568
          -
          1637
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.QualityException.qualityException(&quot;Cannot setup Quality Expression with non-literals&quot;, com.sparkutils.quality.QualityException.qualityException$default$2)
        </td>
      </tr><tr>
        <td>
          28
        </td>
        <td>
          2336
        </td>
        <td>
          1568
          -
          1568
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
      </tr><tr>
        <td>
          30
        </td>
        <td>
          2341
        </td>
        <td>
          1653
          -
          1760
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.QualityException.qualityException(scala.StringContext.apply(&quot;Quality Expression requires a string literal in (starts with position 0) position &quot;, &quot;&quot;).s(pos), com.sparkutils.quality.QualityException.qualityException$default$2)
        </td>
      </tr><tr>
        <td>
          30
        </td>
        <td>
          2340
        </td>
        <td>
          1653
          -
          1653
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
      </tr><tr>
        <td>
          30
        </td>
        <td>
          2339
        </td>
        <td>
          1670
          -
          1759
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;Quality Expression requires a string literal in (starts with position 0) position &quot;, &quot;&quot;).s(pos)
        </td>
      </tr><tr>
        <td>
          30
        </td>
        <td>
          2342
        </td>
        <td>
          1653
          -
          1760
        </td>
        <td>
          Block
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.QualityException.qualityException(scala.StringContext.apply(&quot;Quality Expression requires a string literal in (starts with position 0) position &quot;, &quot;&quot;).s(pos), com.sparkutils.quality.QualityException.qualityException$default$2)
        </td>
      </tr><tr>
        <td>
          32
        </td>
        <td>
          2343
        </td>
        <td>
          1813
          -
          1833
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.literalsNeeded
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.literalsNeeded(-1)
        </td>
      </tr><tr>
        <td>
          37
        </td>
        <td>
          2344
        </td>
        <td>
          1983
          -
          2002
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.literalsNeeded
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.literalsNeeded(pos)
        </td>
      </tr><tr>
        <td>
          42
        </td>
        <td>
          2345
        </td>
        <td>
          2162
          -
          2181
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.literalsNeeded
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.literalsNeeded(pos)
        </td>
      </tr><tr>
        <td>
          46
        </td>
        <td>
          2346
        </td>
        <td>
          2324
          -
          2338
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.unsafe.types.UTF8String.toString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          str.toString()
        </td>
      </tr><tr>
        <td>
          47
        </td>
        <td>
          2347
        </td>
        <td>
          2355
          -
          2374
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.literalsNeeded
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.literalsNeeded(pos)
        </td>
      </tr><tr>
        <td>
          50
        </td>
        <td>
          2348
        </td>
        <td>
          2427
          -
          2454
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.PlaceHolder
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.PlaceHolder
        </td>
      </tr><tr>
        <td>
          50
        </td>
        <td>
          2351
        </td>
        <td>
          2423
          -
          2508
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[String](org.apache.spark.sql.qualityFunctions.LambdaFunctions.PlaceHolder, org.apache.spark.sql.qualityFunctions.LambdaFunctions.Lambda, org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun)
        </td>
      </tr><tr>
        <td>
          51
        </td>
        <td>
          2350
        </td>
        <td>
          2484
          -
          2507
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun
        </td>
      </tr><tr>
        <td>
          51
        </td>
        <td>
          2349
        </td>
        <td>
          2460
          -
          2482
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.Lambda
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.Lambda
        </td>
      </tr><tr>
        <td>
          54
        </td>
        <td>
          2353
        </td>
        <td>
          2580
          -
          2591
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;unique_ID&quot;
        </td>
      </tr><tr>
        <td>
          54
        </td>
        <td>
          2356
        </td>
        <td>
          2615
          -
          2631
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;field_Based_ID&quot;
        </td>
      </tr><tr>
        <td>
          54
        </td>
        <td>
          2352
        </td>
        <td>
          2567
          -
          2579
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;murmur3_ID&quot;
        </td>
      </tr><tr>
        <td>
          54
        </td>
        <td>
          2355
        </td>
        <td>
          2601
          -
          2614
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;provided_ID&quot;
        </td>
      </tr><tr>
        <td>
          54
        </td>
        <td>
          2354
        </td>
        <td>
          2592
          -
          2600
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;rng_ID&quot;
        </td>
      </tr><tr>
        <td>
          54
        </td>
        <td>
          2420
        </td>
        <td>
          2563
          -
          3781
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[String](&quot;murmur3_ID&quot;, &quot;unique_ID&quot;, &quot;rng_ID&quot;, &quot;provided_ID&quot;, &quot;field_Based_ID&quot;, &quot;digest_To_Longs&quot;, &quot;digest_To_Longs_Struct&quot;, &quot;rule_Suite_Result_Details&quot;, &quot;id_Equal&quot;, &quot;long_Pair_Equal&quot;, &quot;big_Bloom&quot;, &quot;small_Bloom&quot;, &quot;long_Pair_From_UUID&quot;, &quot;long_Pair&quot;, &quot;rng_UUID&quot;, &quot;rng&quot;, &quot;rng_Bytes&quot;, &quot;return_Sum&quot;, &quot;sum_With&quot;, &quot;results_With&quot;, &quot;inc&quot;, &quot;meanF&quot;, &quot;agg_Expr&quot;, &quot;passed&quot;, &quot;failed&quot;, &quot;soft_Failed&quot;, &quot;disabled_Rule&quot;, &quot;pack_Ints&quot;, &quot;unpack&quot;, &quot;unpack_Id_Triple&quot;, &quot;soft_Fail&quot;, &quot;probability&quot;, &quot;flatten_Results&quot;, &quot;flatten_Rule_Results&quot;, &quot;flatten_Folder_Results&quot;, &quot;probability_In&quot;, &quot;map_Lookup&quot;, &quot;map_Contains&quot;, &quot;hash_With&quot;, &quot;hash_With_Struct&quot;, &quot;za_Hash_With&quot;, &quot;za_Hash_Longs_With&quot;, &quot;hash_Field_Based_ID&quot;, &quot;za_Longs_Field_Based_ID&quot;, &quot;za_Hash_Longs_With_Struct&quot;, &quot;za_Hash_With_Struct&quot;, &quot;za_Field_Based_ID&quot;, &quot;prefixed_To_Long_Pair&quot;, &quot;coalesce_If_Attributes_Missing&quot;, &quot;coalesce_If_Attributes_Missing_Disable&quot;, &quot;update_Field&quot;, org.apache.spark.sql.qualityFunctions.LambdaFunctions.PlaceHolder, org.apache.spark.sql.qualityFunctions.LambdaFunctions.Lambda, org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun, &quot;print_Expr&quot;, &quot;print_Code&quot;, &quot;comparable_Maps&quot;, &quot;reverse_Comparable_Maps&quot;, &quot;as_uuid&quot;, &quot;id_size&quot;, &quot;id_base64&quot;, &quot;id_from_base64&quot;, &quot;id_raw_type&quot;, &quot;rule_result&quot;, &quot;strip_result_ddl&quot;, &quot;drop_field&quot;, &quot;to_yaml&quot;, &quot;from_yaml&quot;)
        </td>
      </tr><tr>
        <td>
          55
        </td>
        <td>
          2362
        </td>
        <td>
          2739
          -
          2750
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;big_Bloom&quot;
        </td>
      </tr><tr>
        <td>
          55
        </td>
        <td>
          2359
        </td>
        <td>
          2682
          -
          2709
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;rule_Suite_Result_Details&quot;
        </td>
      </tr><tr>
        <td>
          55
        </td>
        <td>
          2358
        </td>
        <td>
          2657
          -
          2681
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;digest_To_Longs_Struct&quot;
        </td>
      </tr><tr>
        <td>
          55
        </td>
        <td>
          2361
        </td>
        <td>
          2721
          -
          2738
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;long_Pair_Equal&quot;
        </td>
      </tr><tr>
        <td>
          55
        </td>
        <td>
          2363
        </td>
        <td>
          2751
          -
          2764
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;small_Bloom&quot;
        </td>
      </tr><tr>
        <td>
          55
        </td>
        <td>
          2357
        </td>
        <td>
          2639
          -
          2656
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;digest_To_Longs&quot;
        </td>
      </tr><tr>
        <td>
          55
        </td>
        <td>
          2360
        </td>
        <td>
          2710
          -
          2720
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;id_Equal&quot;
        </td>
      </tr><tr>
        <td>
          56
        </td>
        <td>
          2371
        </td>
        <td>
          2859
          -
          2873
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;results_With&quot;
        </td>
      </tr><tr>
        <td>
          56
        </td>
        <td>
          2365
        </td>
        <td>
          2794
          -
          2805
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;long_Pair&quot;
        </td>
      </tr><tr>
        <td>
          56
        </td>
        <td>
          2368
        </td>
        <td>
          2823
          -
          2834
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;rng_Bytes&quot;
        </td>
      </tr><tr>
        <td>
          56
        </td>
        <td>
          2367
        </td>
        <td>
          2817
          -
          2822
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;rng&quot;
        </td>
      </tr><tr>
        <td>
          56
        </td>
        <td>
          2370
        </td>
        <td>
          2848
          -
          2858
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;sum_With&quot;
        </td>
      </tr><tr>
        <td>
          56
        </td>
        <td>
          2364
        </td>
        <td>
          2772
          -
          2793
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;long_Pair_From_UUID&quot;
        </td>
      </tr><tr>
        <td>
          56
        </td>
        <td>
          2366
        </td>
        <td>
          2806
          -
          2816
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;rng_UUID&quot;
        </td>
      </tr><tr>
        <td>
          56
        </td>
        <td>
          2369
        </td>
        <td>
          2835
          -
          2847
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;return_Sum&quot;
        </td>
      </tr><tr>
        <td>
          57
        </td>
        <td>
          2380
        </td>
        <td>
          2966
          -
          2974
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;unpack&quot;
        </td>
      </tr><tr>
        <td>
          57
        </td>
        <td>
          2374
        </td>
        <td>
          2895
          -
          2905
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;agg_Expr&quot;
        </td>
      </tr><tr>
        <td>
          57
        </td>
        <td>
          2376
        </td>
        <td>
          2915
          -
          2923
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;failed&quot;
        </td>
      </tr><tr>
        <td>
          57
        </td>
        <td>
          2379
        </td>
        <td>
          2954
          -
          2965
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;pack_Ints&quot;
        </td>
      </tr><tr>
        <td>
          57
        </td>
        <td>
          2373
        </td>
        <td>
          2887
          -
          2894
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;meanF&quot;
        </td>
      </tr><tr>
        <td>
          57
        </td>
        <td>
          2372
        </td>
        <td>
          2881
          -
          2886
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;inc&quot;
        </td>
      </tr><tr>
        <td>
          57
        </td>
        <td>
          2375
        </td>
        <td>
          2906
          -
          2914
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;passed&quot;
        </td>
      </tr><tr>
        <td>
          57
        </td>
        <td>
          2378
        </td>
        <td>
          2938
          -
          2953
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;disabled_Rule&quot;
        </td>
      </tr><tr>
        <td>
          57
        </td>
        <td>
          2377
        </td>
        <td>
          2924
          -
          2937
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;soft_Failed&quot;
        </td>
      </tr><tr>
        <td>
          58
        </td>
        <td>
          2383
        </td>
        <td>
          3013
          -
          3026
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;probability&quot;
        </td>
      </tr><tr>
        <td>
          58
        </td>
        <td>
          2385
        </td>
        <td>
          3045
          -
          3067
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;flatten_Rule_Results&quot;
        </td>
      </tr><tr>
        <td>
          58
        </td>
        <td>
          2382
        </td>
        <td>
          3001
          -
          3012
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;soft_Fail&quot;
        </td>
      </tr><tr>
        <td>
          58
        </td>
        <td>
          2381
        </td>
        <td>
          2982
          -
          3000
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;unpack_Id_Triple&quot;
        </td>
      </tr><tr>
        <td>
          58
        </td>
        <td>
          2384
        </td>
        <td>
          3027
          -
          3044
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;flatten_Results&quot;
        </td>
      </tr><tr>
        <td>
          58
        </td>
        <td>
          2387
        </td>
        <td>
          3095
          -
          3111
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;probability_In&quot;
        </td>
      </tr><tr>
        <td>
          58
        </td>
        <td>
          2386
        </td>
        <td>
          3069
          -
          3093
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;flatten_Folder_Results&quot;
        </td>
      </tr><tr>
        <td>
          59
        </td>
        <td>
          2389
        </td>
        <td>
          3132
          -
          3146
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;map_Contains&quot;
        </td>
      </tr><tr>
        <td>
          59
        </td>
        <td>
          2392
        </td>
        <td>
          3178
          -
          3192
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;za_Hash_With&quot;
        </td>
      </tr><tr>
        <td>
          59
        </td>
        <td>
          2391
        </td>
        <td>
          3159
          -
          3177
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;hash_With_Struct&quot;
        </td>
      </tr><tr>
        <td>
          59
        </td>
        <td>
          2388
        </td>
        <td>
          3119
          -
          3131
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;map_Lookup&quot;
        </td>
      </tr><tr>
        <td>
          59
        </td>
        <td>
          2390
        </td>
        <td>
          3147
          -
          3158
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;hash_With&quot;
        </td>
      </tr><tr>
        <td>
          59
        </td>
        <td>
          2393
        </td>
        <td>
          3194
          -
          3214
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;za_Hash_Longs_With&quot;
        </td>
      </tr><tr>
        <td>
          60
        </td>
        <td>
          2398
        </td>
        <td>
          3322
          -
          3341
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;za_Field_Based_ID&quot;
        </td>
      </tr><tr>
        <td>
          60
        </td>
        <td>
          2394
        </td>
        <td>
          3222
          -
          3243
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;hash_Field_Based_ID&quot;
        </td>
      </tr><tr>
        <td>
          60
        </td>
        <td>
          2397
        </td>
        <td>
          3299
          -
          3320
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;za_Hash_With_Struct&quot;
        </td>
      </tr><tr>
        <td>
          60
        </td>
        <td>
          2399
        </td>
        <td>
          3343
          -
          3366
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;prefixed_To_Long_Pair&quot;
        </td>
      </tr><tr>
        <td>
          60
        </td>
        <td>
          2396
        </td>
        <td>
          3270
          -
          3297
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;za_Hash_Longs_With_Struct&quot;
        </td>
      </tr><tr>
        <td>
          60
        </td>
        <td>
          2395
        </td>
        <td>
          3244
          -
          3269
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;za_Longs_Field_Based_ID&quot;
        </td>
      </tr><tr>
        <td>
          61
        </td>
        <td>
          2401
        </td>
        <td>
          3408
          -
          3448
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;coalesce_If_Attributes_Missing_Disable&quot;
        </td>
      </tr><tr>
        <td>
          61
        </td>
        <td>
          2400
        </td>
        <td>
          3374
          -
          3406
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;coalesce_If_Attributes_Missing&quot;
        </td>
      </tr><tr>
        <td>
          61
        </td>
        <td>
          2403
        </td>
        <td>
          3466
          -
          3493
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.PlaceHolder
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.PlaceHolder
        </td>
      </tr><tr>
        <td>
          61
        </td>
        <td>
          2402
        </td>
        <td>
          3450
          -
          3464
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;update_Field&quot;
        </td>
      </tr><tr>
        <td>
          62
        </td>
        <td>
          2407
        </td>
        <td>
          3564
          -
          3576
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;print_Code&quot;
        </td>
      </tr><tr>
        <td>
          62
        </td>
        <td>
          2410
        </td>
        <td>
          3624
          -
          3633
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;as_uuid&quot;
        </td>
      </tr><tr>
        <td>
          62
        </td>
        <td>
          2409
        </td>
        <td>
          3597
          -
          3622
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;reverse_Comparable_Maps&quot;
        </td>
      </tr><tr>
        <td>
          62
        </td>
        <td>
          2406
        </td>
        <td>
          3550
          -
          3562
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;print_Expr&quot;
        </td>
      </tr><tr>
        <td>
          62
        </td>
        <td>
          2408
        </td>
        <td>
          3578
          -
          3595
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;comparable_Maps&quot;
        </td>
      </tr><tr>
        <td>
          62
        </td>
        <td>
          2405
        </td>
        <td>
          3525
          -
          3548
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun
        </td>
      </tr><tr>
        <td>
          62
        </td>
        <td>
          2404
        </td>
        <td>
          3501
          -
          3523
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.Lambda
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.Lambda
        </td>
      </tr><tr>
        <td>
          63
        </td>
        <td>
          2416
        </td>
        <td>
          3713
          -
          3731
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;strip_result_ddl&quot;
        </td>
      </tr><tr>
        <td>
          63
        </td>
        <td>
          2412
        </td>
        <td>
          3652
          -
          3663
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;id_base64&quot;
        </td>
      </tr><tr>
        <td>
          63
        </td>
        <td>
          2415
        </td>
        <td>
          3698
          -
          3711
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;rule_result&quot;
        </td>
      </tr><tr>
        <td>
          63
        </td>
        <td>
          2414
        </td>
        <td>
          3683
          -
          3696
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;id_raw_type&quot;
        </td>
      </tr><tr>
        <td>
          63
        </td>
        <td>
          2417
        </td>
        <td>
          3733
          -
          3745
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;drop_field&quot;
        </td>
      </tr><tr>
        <td>
          63
        </td>
        <td>
          2411
        </td>
        <td>
          3641
          -
          3650
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;id_size&quot;
        </td>
      </tr><tr>
        <td>
          63
        </td>
        <td>
          2413
        </td>
        <td>
          3665
          -
          3681
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;id_from_base64&quot;
        </td>
      </tr><tr>
        <td>
          64
        </td>
        <td>
          2419
        </td>
        <td>
          3764
          -
          3775
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;from_yaml&quot;
        </td>
      </tr><tr>
        <td>
          64
        </td>
        <td>
          2418
        </td>
        <td>
          3753
          -
          3762
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;to_yaml&quot;
        </td>
      </tr><tr>
        <td>
          66
        </td>
        <td>
          2425
        </td>
        <td>
          3824
          -
          3824
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.Set.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          immutable.this.Set.canBuildFrom[String]
        </td>
      </tr><tr>
        <td>
          66
        </td>
        <td>
          2427
        </td>
        <td>
          3786
          -
          3880
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SetLike.++
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          withUnderscores.++(withUnderscores.map[String, scala.collection.immutable.Set[String]](((n: String) =&gt; if (RuleRegistrationFunctions.this.mustKeepNames.apply(n))
  n
else
  n.replaceAll(&quot;_&quot;, &quot;&quot;)))(immutable.this.Set.canBuildFrom[String]))
        </td>
      </tr><tr>
        <td>
          66
        </td>
        <td>
          2421
        </td>
        <td>
          3834
          -
          3850
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.GenSetLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.mustKeepNames.apply(n)
        </td>
      </tr><tr>
        <td>
          66
        </td>
        <td>
          2424
        </td>
        <td>
          3859
          -
          3879
        </td>
        <td>
          Block
        </td>
        <td>
          java.lang.String.replaceAll
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          n.replaceAll(&quot;_&quot;, &quot;&quot;)
        </td>
      </tr><tr>
        <td>
          66
        </td>
        <td>
          2423
        </td>
        <td>
          3859
          -
          3879
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.replaceAll
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          n.replaceAll(&quot;_&quot;, &quot;&quot;)
        </td>
      </tr><tr>
        <td>
          66
        </td>
        <td>
          2426
        </td>
        <td>
          3805
          -
          3880
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.SetLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          withUnderscores.map[String, scala.collection.immutable.Set[String]](((n: String) =&gt; if (RuleRegistrationFunctions.this.mustKeepNames.apply(n))
  n
else
  n.replaceAll(&quot;_&quot;, &quot;&quot;)))(immutable.this.Set.canBuildFrom[String])
        </td>
      </tr><tr>
        <td>
          66
        </td>
        <td>
          2422
        </td>
        <td>
          3852
          -
          3853
        </td>
        <td>
          Ident
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.n
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          n
        </td>
      </tr><tr>
        <td>
          69
        </td>
        <td>
          2428
        </td>
        <td>
          3913
          -
          3938
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.MAX_PRECISION
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.DecimalType.MAX_PRECISION
        </td>
      </tr><tr>
        <td>
          69
        </td>
        <td>
          2430
        </td>
        <td>
          3901
          -
          3962
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.DecimalType.apply(org.apache.spark.sql.types.DecimalType.MAX_PRECISION, org.apache.spark.sql.types.DecimalType.MAX_SCALE)
        </td>
      </tr><tr>
        <td>
          69
        </td>
        <td>
          2429
        </td>
        <td>
          3940
          -
          3961
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.MAX_SCALE
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.DecimalType.MAX_SCALE
        </td>
      </tr><tr>
        <td>
          71
        </td>
        <td>
          2431
        </td>
        <td>
          4006
          -
          4010
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.None
        </td>
      </tr><tr>
        <td>
          81
        </td>
        <td>
          2434
        </td>
        <td>
          4403
          -
          4486
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.MapMerge.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.MapMerge.apply(scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](left, right), ((dataType: org.apache.spark.sql.types.DataType) =&gt; RuleRegistrationFunctions.this.defaultAdd(dataType, extension)))
        </td>
      </tr><tr>
        <td>
          81
        </td>
        <td>
          2433
        </td>
        <td>
          4454
          -
          4485
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.defaultAdd
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.defaultAdd(dataType, extension)
        </td>
      </tr><tr>
        <td>
          81
        </td>
        <td>
          2432
        </td>
        <td>
          4412
          -
          4428
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](left, right)
        </td>
      </tr><tr>
        <td>
          81
        </td>
        <td>
          2435
        </td>
        <td>
          4381
          -
          4487
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[(org.apache.spark.sql.catalyst.expressions.Expression, org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.qualityFunctions.MapMerge](((left: org.apache.spark.sql.catalyst.expressions.Expression, right: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.qualityFunctions.MapMerge.apply(scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](left, right), ((dataType: org.apache.spark.sql.types.DataType) =&gt; RuleRegistrationFunctions.this.defaultAdd(dataType, extension)))))
        </td>
      </tr><tr>
        <td>
          83
        </td>
        <td>
          2436
        </td>
        <td>
          4571
          -
          4593
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.add
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.add(left, right, null)
        </td>
      </tr><tr>
        <td>
          83
        </td>
        <td>
          2437
        </td>
        <td>
          4549
          -
          4594
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[(org.apache.spark.sql.catalyst.expressions.Expression, org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.catalyst.expressions.Expression](((left: org.apache.spark.sql.catalyst.expressions.Expression, right: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.add(left, right, null)))
        </td>
      </tr><tr>
        <td>
          85
        </td>
        <td>
          2439
        </td>
        <td>
          4632
          -
          4674
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[(org.apache.spark.sql.catalyst.expressions.Expression, org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.catalyst.expressions.Expression](((left: org.apache.spark.sql.catalyst.expressions.Expression, right: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.add(left, right, a)))
        </td>
      </tr><tr>
        <td>
          85
        </td>
        <td>
          2438
        </td>
        <td>
          4654
          -
          4673
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.add
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.add(left, right, a)
        </td>
      </tr><tr>
        <td>
          86
        </td>
        <td>
          2440
        </td>
        <td>
          4691
          -
          4710
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function1.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          extension.apply(dataType)
        </td>
      </tr><tr>
        <td>
          97
        </td>
        <td>
          2441
        </td>
        <td>
          4959
          -
          4973
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[com.sparkutils.quality.impl.EmptyMap.type](EmptyMap)
        </td>
      </tr><tr>
        <td>
          98
        </td>
        <td>
          2442
        </td>
        <td>
          5014
          -
          5022
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[Long](0L)
        </td>
      </tr><tr>
        <td>
          99
        </td>
        <td>
          2443
        </td>
        <td>
          5051
          -
          5060
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[Double](0.0)
        </td>
      </tr><tr>
        <td>
          100
        </td>
        <td>
          2445
        </td>
        <td>
          5119
          -
          5130
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.precision
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          d.precision
        </td>
      </tr><tr>
        <td>
          100
        </td>
        <td>
          2448
        </td>
        <td>
          5090
          -
          5141
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[org.apache.spark.sql.types.Decimal](org.apache.spark.sql.types.Decimal.createUnsafe(0L, d.precision, d.scale))
        </td>
      </tr><tr>
        <td>
          100
        </td>
        <td>
          2444
        </td>
        <td>
          5116
          -
          5117
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0L
        </td>
      </tr><tr>
        <td>
          100
        </td>
        <td>
          2447
        </td>
        <td>
          5095
          -
          5140
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.Decimal.createUnsafe
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.Decimal.createUnsafe(0L, d.precision, d.scale)
        </td>
      </tr><tr>
        <td>
          100
        </td>
        <td>
          2446
        </td>
        <td>
          5132
          -
          5139
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.scale
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          d.scale
        </td>
      </tr><tr>
        <td>
          101
        </td>
        <td>
          2449
        </td>
        <td>
          5158
          -
          5162
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.None
        </td>
      </tr><tr>
        <td>
          113
        </td>
        <td>
          2452
        </td>
        <td>
          5464
          -
          5510
        </td>
        <td>
          Block
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[org.apache.spark.sql.types.DataType](org.apache.spark.sql.types.DataType.fromDDL(string))
        </td>
      </tr><tr>
        <td>
          113
        </td>
        <td>
          2451
        </td>
        <td>
          5464
          -
          5510
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[org.apache.spark.sql.types.DataType](org.apache.spark.sql.types.DataType.fromDDL(string))
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          2450
        </td>
        <td>
          5478
          -
          5502
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.DataType.fromDDL
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.DataType.fromDDL(string)
        </td>
      </tr><tr>
        <td>
          117
        </td>
        <td>
          2453
        </td>
        <td>
          5552
          -
          5556
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.None
        </td>
      </tr><tr>
        <td>
          120
        </td>
        <td>
          2454
        </td>
        <td>
          5607
          -
          5736
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;inc(\'DDL\', generic expression) is not supported in NO_REWRITE mode, use inc(generic expression) without NO_REWRITE mode enabled&quot;
        </td>
      </tr><tr>
        <td>
          125
        </td>
        <td>
          2456
        </td>
        <td>
          6055
          -
          6092
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          paramNumbers.isEmpty.&amp;&amp;(minimum.==(-1))
        </td>
      </tr><tr>
        <td>
          125
        </td>
        <td>
          2455
        </td>
        <td>
          6079
          -
          6092
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          minimum.==(-1)
        </td>
      </tr><tr>
        <td>
          126
        </td>
        <td>
          2457
        </td>
        <td>
          6103
          -
          6108
        </td>
        <td>
          Ident
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.argsf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          argsf
        </td>
      </tr><tr>
        <td>
          128
        </td>
        <td>
          2476
        </td>
        <td>
          6128
          -
          6641
        </td>
        <td>
          Function
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.$anonfun
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; {
  if (paramNumbers.nonEmpty.&amp;&amp;(paramNumbers.contains(exps.size).unary_!).||(minimum.&gt;(exps.size)))
    {
      val sizeerr: String = if (paramNumbers.nonEmpty)
        scala.StringContext.apply(&quot;Valid parameter counts are &quot;, &quot;&quot;).s(paramNumbers.mkString(&quot;, &quot;))
      else
        scala.StringContext.apply(&quot;A minimum of &quot;, &quot; parameters is required.&quot;).s(minimum);
      throw com.sparkutils.quality.QualityException.apply(scala.StringContext.apply(&quot;Wrong number of arguments provided to Quality function &quot;, &quot;. &quot;, &quot;&quot;).s(name, sizeerr), com.sparkutils.quality.QualityException.apply$default$2)
    }
  else
    ();
  argsf.apply(exps)
})
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          2461
        </td>
        <td>
          6236
          -
          6255
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          minimum.&gt;(exps.size)
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          2460
        </td>
        <td>
          6246
          -
          6255
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          2472
        </td>
        <td>
          6258
          -
          6609
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  val sizeerr: String = if (paramNumbers.nonEmpty)
    scala.StringContext.apply(&quot;Valid parameter counts are &quot;, &quot;&quot;).s(paramNumbers.mkString(&quot;, &quot;))
  else
    scala.StringContext.apply(&quot;A minimum of &quot;, &quot; parameters is required.&quot;).s(minimum);
  throw com.sparkutils.quality.QualityException.apply(scala.StringContext.apply(&quot;Wrong number of arguments provided to Quality function &quot;, &quot;. &quot;, &quot;&quot;).s(name, sizeerr), com.sparkutils.quality.QualityException.apply$default$2)
}
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          2474
        </td>
        <td>
          6167
          -
          6167
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          2459
        </td>
        <td>
          6197
          -
          6230
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          paramNumbers.contains(exps.size).unary_!
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          2462
        </td>
        <td>
          6171
          -
          6256
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.||
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          paramNumbers.nonEmpty.&amp;&amp;(paramNumbers.contains(exps.size).unary_!).||(minimum.&gt;(exps.size))
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          2473
        </td>
        <td>
          6167
          -
          6167
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          2458
        </td>
        <td>
          6220
          -
          6229
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size
        </td>
      </tr><tr>
        <td>
          131
        </td>
        <td>
          2463
        </td>
        <td>
          6304
          -
          6325
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableOnce.nonEmpty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          paramNumbers.nonEmpty
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          2466
        </td>
        <td>
          6374
          -
          6401
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.mkString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          paramNumbers.mkString(&quot;, &quot;)
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          2468
        </td>
        <td>
          6343
          -
          6403
        </td>
        <td>
          Block
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;Valid parameter counts are &quot;, &quot;&quot;).s(paramNumbers.mkString(&quot;, &quot;))
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          2465
        </td>
        <td>
          6402
          -
          6403
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          2464
        </td>
        <td>
          6345
          -
          6373
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;Valid parameter counts are &quot;
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          2467
        </td>
        <td>
          6343
          -
          6403
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;Valid parameter counts are &quot;, &quot;&quot;).s(paramNumbers.mkString(&quot;, &quot;))
        </td>
      </tr><tr>
        <td>
          134
        </td>
        <td>
          2470
        </td>
        <td>
          6439
          -
          6487
        </td>
        <td>
          Block
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;A minimum of &quot;, &quot; parameters is required.&quot;).s(minimum)
        </td>
      </tr><tr>
        <td>
          134
        </td>
        <td>
          2469
        </td>
        <td>
          6439
          -
          6487
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;A minimum of &quot;, &quot; parameters is required.&quot;).s(minimum)
        </td>
      </tr><tr>
        <td>
          135
        </td>
        <td>
          2471
        </td>
        <td>
          6500
          -
          6597
        </td>
        <td>
          Throw
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          throw com.sparkutils.quality.QualityException.apply(scala.StringContext.apply(&quot;Wrong number of arguments provided to Quality function &quot;, &quot;. &quot;, &quot;&quot;).s(name, sizeerr), com.sparkutils.quality.QualityException.apply$default$2)
        </td>
      </tr><tr>
        <td>
          137
        </td>
        <td>
          2475
        </td>
        <td>
          6620
          -
          6631
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function1.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          argsf.apply(exps)
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          2477
        </td>
        <td>
          6646
          -
          6676
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          registerFunction.apply(name, create)
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          2478
        </td>
        <td>
          6685
          -
          6705
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.mustKeepNames.apply(name).unary_!
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          2483
        </td>
        <td>
          6681
          -
          6681
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          2482
        </td>
        <td>
          6681
          -
          6681
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          141
        </td>
        <td>
          2479
        </td>
        <td>
          6732
          -
          6755
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.replaceAll
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          name.replaceAll(&quot;_&quot;, &quot;&quot;)
        </td>
      </tr><tr>
        <td>
          141
        </td>
        <td>
          2481
        </td>
        <td>
          6715
          -
          6764
        </td>
        <td>
          Block
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          registerFunction.apply(name.replaceAll(&quot;_&quot;, &quot;&quot;), create)
        </td>
      </tr><tr>
        <td>
          141
        </td>
        <td>
          2480
        </td>
        <td>
          6715
          -
          6764
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          registerFunction.apply(name.replaceAll(&quot;_&quot;, &quot;&quot;), create)
        </td>
      </tr><tr>
        <td>
          163
        </td>
        <td>
          2484
        </td>
        <td>
          8241
          -
          8313
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.registerWithChecks
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.registerWithChecks(registerFunction, name, argsf, paramNumbers, minimum)
        </td>
      </tr><tr>
        <td>
          167
        </td>
        <td>
          2488
        </td>
        <td>
          8474
          -
          8524
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.QualityException.qualityException(scala.StringContext.apply(&quot;Could not parse the type &quot;, &quot;&quot;).s(str), com.sparkutils.quality.QualityException.qualityException$default$2)
        </td>
      </tr><tr>
        <td>
          167
        </td>
        <td>
          2487
        </td>
        <td>
          8474
          -
          8474
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
      </tr><tr>
        <td>
          167
        </td>
        <td>
          2486
        </td>
        <td>
          8491
          -
          8523
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;Could not parse the type &quot;, &quot;&quot;).s(str)
        </td>
      </tr><tr>
        <td>
          167
        </td>
        <td>
          2489
        </td>
        <td>
          8439
          -
          8525
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Option.getOrElse
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          parseTypes.apply(str.toString()).getOrElse[org.apache.spark.sql.types.DataType](com.sparkutils.quality.QualityException.qualityException(scala.StringContext.apply(&quot;Could not parse the type &quot;, &quot;&quot;).s(str), com.sparkutils.quality.QualityException.qualityException$default$2))
        </td>
      </tr><tr>
        <td>
          167
        </td>
        <td>
          2485
        </td>
        <td>
          8450
          -
          8462
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.unsafe.types.UTF8String.toString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          str.toString()
        </td>
      </tr><tr>
        <td>
          171
        </td>
        <td>
          2490
        </td>
        <td>
          8625
          -
          8626
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          2
        </td>
      </tr><tr>
        <td>
          171
        </td>
        <td>
          2493
        </td>
        <td>
          8606
          -
          8750
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.Iterator.forall
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          c.children.grouped(2).forall(((x0$1: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; x0$1 match {
  case scala.collection.Seq.unapplySeq[org.apache.spark.sql.catalyst.expressions.Expression](&lt;unapply-selector&gt;) &lt;unapply&gt; ((value: Any, dataType: org.apache.spark.sql.types.DataType)org.apache.spark.sql.catalyst.expressions.Literal((_: org.apache.spark.unsafe.types.UTF8String), org.apache.spark.sql.types.StringType), (_: org.apache.spark.sql.catalyst.expressions.Literal)) =&gt; true
  case _ =&gt; false
}))
        </td>
      </tr><tr>
        <td>
          173
        </td>
        <td>
          2491
        </td>
        <td>
          8714
          -
          8718
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          174
        </td>
        <td>
          2492
        </td>
        <td>
          8737
          -
          8742
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          false
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          2494
        </td>
        <td>
          8781
          -
          8782
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          2
        </td>
      </tr><tr>
        <td>
          178
        </td>
        <td>
          2496
        </td>
        <td>
          8893
          -
          8915
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.toString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          value.value.toString()
        </td>
      </tr><tr>
        <td>
          178
        </td>
        <td>
          2495
        </td>
        <td>
          8877
          -
          8889
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.unsafe.types.UTF8String.toString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          str.toString()
        </td>
      </tr><tr>
        <td>
          178
        </td>
        <td>
          2497
        </td>
        <td>
          8877
          -
          8915
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Predef.ArrowAssoc.-&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.ArrowAssoc[String](str.toString()).-&gt;[String](value.value.toString())
        </td>
      </tr><tr>
        <td>
          179
        </td>
        <td>
          2499
        </td>
        <td>
          8762
          -
          8931
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableOnce.toMap
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          c.children.grouped(2).map[(String, String)](((x0$2: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; x0$2 match {
  case scala.collection.Seq.unapplySeq[org.apache.spark.sql.catalyst.expressions.Expression](&lt;unapply-selector&gt;) &lt;unapply&gt; ((value: Any, dataType: org.apache.spark.sql.types.DataType)org.apache.spark.sql.catalyst.expressions.Literal((str @ (_: org.apache.spark.unsafe.types.UTF8String)), org.apache.spark.sql.types.StringType), (value @ (_: org.apache.spark.sql.catalyst.expressions.Literal))) =&gt; scala.Predef.ArrowAssoc[String](str.toString()).-&gt;[String](value.value.toString())
})).toMap[String, String](scala.Predef.$conforms[(String, String)])
        </td>
      </tr><tr>
        <td>
          179
        </td>
        <td>
          2498
        </td>
        <td>
          8926
          -
          8926
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Predef.$conforms
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.$conforms[(String, String)]
        </td>
      </tr><tr>
        <td>
          180
        </td>
        <td>
          2500
        </td>
        <td>
          8948
          -
          9027
        </td>
        <td>
          Throw
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          throw com.sparkutils.quality.QualityException.apply(scala.StringContext.apply(&quot;Could not process a literal map with expression &quot;, &quot;&quot;).s(exp), com.sparkutils.quality.QualityException.apply$default$2)
        </td>
      </tr><tr>
        <td>
          183
        </td>
        <td>
          2505
        </td>
        <td>
          9129
          -
          9130
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          2
        </td>
      </tr><tr>
        <td>
          183
        </td>
        <td>
          2502
        </td>
        <td>
          9096
          -
          9105
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          183
        </td>
        <td>
          2501
        </td>
        <td>
          9048
          -
          9073
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;processor_input_wrapper&quot;
        </td>
      </tr><tr>
        <td>
          183
        </td>
        <td>
          2504
        </td>
        <td>
          9083
          -
          9117
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.InputWrapper.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.util.InputWrapper.apply(exps.head, exps.last)
        </td>
      </tr><tr>
        <td>
          183
        </td>
        <td>
          2506
        </td>
        <td>
          9039
          -
          9131
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(x$1, x$2, x$4, x$3)
        </td>
      </tr><tr>
        <td>
          183
        </td>
        <td>
          2503
        </td>
        <td>
          9107
          -
          9116
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableLike.last
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.last
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          2511
        </td>
        <td>
          9212
          -
          9221
        </td>
        <td>
          Block
        </td>
        <td>
          scala.collection.immutable.Map.empty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Map.empty[String, Nothing]
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          2514
        </td>
        <td>
          9227
          -
          9244
        </td>
        <td>
          Block
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getMap
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          getMap(exps.last)
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          2508
        </td>
        <td>
          9181
          -
          9190
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          2517
        </td>
        <td>
          9137
          -
          9257
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;to_yaml&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.impl.yaml.YamlEncoderExpr.apply(exps.head, if (exps.size.==(1))
  scala.Predef.Map.empty[String, Nothing]
else
  getMap(exps.last))), scala.Predef.Set.apply[Int](1, 2), register$default$4)
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          2510
        </td>
        <td>
          9212
          -
          9221
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.Map.empty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Map.empty[String, Nothing]
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          2513
        </td>
        <td>
          9227
          -
          9244
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getMap
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          getMap(exps.last)
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          2516
        </td>
        <td>
          9247
          -
          9256
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1, 2)
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          2507
        </td>
        <td>
          9146
          -
          9155
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;to_yaml&quot;
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          2515
        </td>
        <td>
          9165
          -
          9245
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.yaml.YamlEncoderExpr.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.yaml.YamlEncoderExpr.apply(exps.head, if (exps.size.==(1))
  scala.Predef.Map.empty[String, Nothing]
else
  getMap(exps.last))
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          2509
        </td>
        <td>
          9196
          -
          9210
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size.==(1)
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          2512
        </td>
        <td>
          9234
          -
          9243
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableLike.last
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.last
        </td>
      </tr><tr>
        <td>
          186
        </td>
        <td>
          2520
        </td>
        <td>
          9325
          -
          9334
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableLike.last
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.last
        </td>
      </tr><tr>
        <td>
          186
        </td>
        <td>
          2523
        </td>
        <td>
          9338
          -
          9344
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](2)
        </td>
      </tr><tr>
        <td>
          186
        </td>
        <td>
          2519
        </td>
        <td>
          9308
          -
          9317
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          186
        </td>
        <td>
          2522
        </td>
        <td>
          9292
          -
          9336
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.yaml.YamlDecoderExpr.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.yaml.YamlDecoderExpr.apply(exps.head, parse(exps.last))
        </td>
      </tr><tr>
        <td>
          186
        </td>
        <td>
          2524
        </td>
        <td>
          9262
          -
          9345
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;from_yaml&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.impl.yaml.YamlDecoderExpr.apply(exps.head, parse(exps.last))), scala.Predef.Set.apply[Int](2), register$default$4)
        </td>
      </tr><tr>
        <td>
          186
        </td>
        <td>
          2518
        </td>
        <td>
          9271
          -
          9282
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;from_yaml&quot;
        </td>
      </tr><tr>
        <td>
          186
        </td>
        <td>
          2521
        </td>
        <td>
          9319
          -
          9335
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.parse
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          parse(exps.last)
        </td>
      </tr><tr>
        <td>
          188
        </td>
        <td>
          2529
        </td>
        <td>
          9351
          -
          9424
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;strip_result_ddl&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; StripResultTypes.apply(exps.head)), scala.Predef.Set.apply[Int](1), register$default$4)
        </td>
      </tr><tr>
        <td>
          188
        </td>
        <td>
          2526
        </td>
        <td>
          9405
          -
          9414
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          188
        </td>
        <td>
          2528
        </td>
        <td>
          9417
          -
          9423
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1)
        </td>
      </tr><tr>
        <td>
          188
        </td>
        <td>
          2525
        </td>
        <td>
          9360
          -
          9378
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;strip_result_ddl&quot;
        </td>
      </tr><tr>
        <td>
          188
        </td>
        <td>
          2527
        </td>
        <td>
          9388
          -
          9415
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.StripResultTypes.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          StripResultTypes.apply(exps.head)
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          2538
        </td>
        <td>
          9429
          -
          9531
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;rule_result&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; RuleResultExpression.apply(scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](exps.apply(0), exps.apply(1), exps.apply(2), exps.apply(3)))), scala.Predef.Set.apply[Int](4), register$default$4)
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          2532
        </td>
        <td>
          9495
          -
          9502
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          2535
        </td>
        <td>
          9482
          -
          9521
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](exps.apply(0), exps.apply(1), exps.apply(2), exps.apply(3))
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          2534
        </td>
        <td>
          9513
          -
          9520
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(3)
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          2537
        </td>
        <td>
          9524
          -
          9530
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](4)
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          2531
        </td>
        <td>
          9486
          -
          9493
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          2533
        </td>
        <td>
          9504
          -
          9511
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(2)
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          2536
        </td>
        <td>
          9461
          -
          9522
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleResultExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleResultExpression.apply(scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](exps.apply(0), exps.apply(1), exps.apply(2), exps.apply(3)))
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          2530
        </td>
        <td>
          9438
          -
          9451
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;rule_result&quot;
        </td>
      </tr><tr>
        <td>
          191
        </td>
        <td>
          2541
        </td>
        <td>
          9573
          -
          9616
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ComparableMapConverter.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.util.ComparableMapConverter.apply(exps.apply(0), mapCompare)
        </td>
      </tr><tr>
        <td>
          191
        </td>
        <td>
          2543
        </td>
        <td>
          9537
          -
          9625
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;comparable_Maps&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.impl.util.ComparableMapConverter.apply(exps.apply(0), mapCompare)), scala.Predef.Set.apply[Int](1), register$default$4)
        </td>
      </tr><tr>
        <td>
          191
        </td>
        <td>
          2540
        </td>
        <td>
          9596
          -
          9603
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          191
        </td>
        <td>
          2542
        </td>
        <td>
          9618
          -
          9624
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1)
        </td>
      </tr><tr>
        <td>
          191
        </td>
        <td>
          2539
        </td>
        <td>
          9546
          -
          9563
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;comparable_Maps&quot;
        </td>
      </tr><tr>
        <td>
          192
        </td>
        <td>
          2547
        </td>
        <td>
          9708
          -
          9714
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1)
        </td>
      </tr><tr>
        <td>
          192
        </td>
        <td>
          2544
        </td>
        <td>
          9639
          -
          9664
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;reverse_Comparable_Maps&quot;
        </td>
      </tr><tr>
        <td>
          192
        </td>
        <td>
          2546
        </td>
        <td>
          9674
          -
          9706
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ComparableMapReverser.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.util.ComparableMapReverser.apply(exps.head)
        </td>
      </tr><tr>
        <td>
          192
        </td>
        <td>
          2545
        </td>
        <td>
          9696
          -
          9705
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          192
        </td>
        <td>
          2548
        </td>
        <td>
          9630
          -
          9715
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;reverse_Comparable_Maps&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.impl.util.ComparableMapReverser.apply(exps.head)), scala.Predef.Set.apply[Int](1), register$default$4)
        </td>
      </tr><tr>
        <td>
          194
        </td>
        <td>
          2550
        </td>
        <td>
          9769
          -
          9778
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          194
        </td>
        <td>
          2553
        </td>
        <td>
          9721
          -
          9788
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;probability&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; ProbabilityExpr.apply(exps.head)), scala.Predef.Set.apply[Int](1), register$default$4)
        </td>
      </tr><tr>
        <td>
          194
        </td>
        <td>
          2552
        </td>
        <td>
          9781
          -
          9787
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1)
        </td>
      </tr><tr>
        <td>
          194
        </td>
        <td>
          2549
        </td>
        <td>
          9730
          -
          9743
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;probability&quot;
        </td>
      </tr><tr>
        <td>
          194
        </td>
        <td>
          2551
        </td>
        <td>
          9753
          -
          9779
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.ProbabilityExpr.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ProbabilityExpr.apply(exps.head)
        </td>
      </tr><tr>
        <td>
          195
        </td>
        <td>
          2556
        </td>
        <td>
          9865
          -
          9900
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.FlattenStruct.ruleSuiteDeserializer
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          FlattenStruct.ruleSuiteDeserializer
        </td>
      </tr><tr>
        <td>
          195
        </td>
        <td>
          2559
        </td>
        <td>
          9793
          -
          9910
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;flatten_Results&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; FlattenResultsExpression.apply(exps.head, FlattenStruct.ruleSuiteDeserializer)), scala.Predef.Set.apply[Int](1), register$default$4)
        </td>
      </tr><tr>
        <td>
          195
        </td>
        <td>
          2555
        </td>
        <td>
          9854
          -
          9863
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          195
        </td>
        <td>
          2558
        </td>
        <td>
          9903
          -
          9909
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1)
        </td>
      </tr><tr>
        <td>
          195
        </td>
        <td>
          2557
        </td>
        <td>
          9829
          -
          9901
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.FlattenResultsExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          FlattenResultsExpression.apply(exps.head, FlattenStruct.ruleSuiteDeserializer)
        </td>
      </tr><tr>
        <td>
          195
        </td>
        <td>
          2554
        </td>
        <td>
          9802
          -
          9819
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;flatten_Results&quot;
        </td>
      </tr><tr>
        <td>
          196
        </td>
        <td>
          2565
        </td>
        <td>
          9915
          -
          10042
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;flatten_Rule_Results&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; FlattenRulesResultsExpression.apply(exps.head, FlattenStruct.ruleSuiteDeserializer)), scala.Predef.Set.apply[Int](1), register$default$4)
        </td>
      </tr><tr>
        <td>
          196
        </td>
        <td>
          2562
        </td>
        <td>
          9997
          -
          10032
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.FlattenStruct.ruleSuiteDeserializer
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          FlattenStruct.ruleSuiteDeserializer
        </td>
      </tr><tr>
        <td>
          196
        </td>
        <td>
          2561
        </td>
        <td>
          9986
          -
          9995
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          196
        </td>
        <td>
          2564
        </td>
        <td>
          10035
          -
          10041
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1)
        </td>
      </tr><tr>
        <td>
          196
        </td>
        <td>
          2560
        </td>
        <td>
          9924
          -
          9946
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;flatten_Rule_Results&quot;
        </td>
      </tr><tr>
        <td>
          196
        </td>
        <td>
          2563
        </td>
        <td>
          9956
          -
          10033
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.FlattenRulesResultsExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          FlattenRulesResultsExpression.apply(exps.head, FlattenStruct.ruleSuiteDeserializer)
        </td>
      </tr><tr>
        <td>
          197
        </td>
        <td>
          2571
        </td>
        <td>
          10047
          -
          10177
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;flatten_Folder_Results&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; FlattenFolderResultsExpression.apply(exps.head, FlattenStruct.ruleSuiteDeserializer)), scala.Predef.Set.apply[Int](1), register$default$4)
        </td>
      </tr><tr>
        <td>
          197
        </td>
        <td>
          2568
        </td>
        <td>
          10132
          -
          10167
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.FlattenStruct.ruleSuiteDeserializer
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          FlattenStruct.ruleSuiteDeserializer
        </td>
      </tr><tr>
        <td>
          197
        </td>
        <td>
          2570
        </td>
        <td>
          10170
          -
          10176
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1)
        </td>
      </tr><tr>
        <td>
          197
        </td>
        <td>
          2567
        </td>
        <td>
          10121
          -
          10130
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          197
        </td>
        <td>
          2566
        </td>
        <td>
          10056
          -
          10080
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;flatten_Folder_Results&quot;
        </td>
      </tr><tr>
        <td>
          197
        </td>
        <td>
          2569
        </td>
        <td>
          10090
          -
          10168
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.FlattenFolderResultsExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          FlattenFolderResultsExpression.apply(exps.head, FlattenStruct.ruleSuiteDeserializer)
        </td>
      </tr><tr>
        <td>
          199
        </td>
        <td>
          2574
        </td>
        <td>
          10274
          -
          10280
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](0)
        </td>
      </tr><tr>
        <td>
          199
        </td>
        <td>
          2573
        </td>
        <td>
          10207
          -
          10272
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.imports.RuleResultsImports.PassedExpr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.imports.RuleResultsImports.PassedExpr
        </td>
      </tr><tr>
        <td>
          199
        </td>
        <td>
          2575
        </td>
        <td>
          10183
          -
          10281
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;passed&quot;, ((x$2: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.impl.imports.RuleResultsImports.PassedExpr), scala.Predef.Set.apply[Int](0), register$default$4)
        </td>
      </tr><tr>
        <td>
          199
        </td>
        <td>
          2572
        </td>
        <td>
          10192
          -
          10200
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;passed&quot;
        </td>
      </tr><tr>
        <td>
          200
        </td>
        <td>
          2577
        </td>
        <td>
          10310
          -
          10375
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.imports.RuleResultsImports.FailedExpr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.imports.RuleResultsImports.FailedExpr
        </td>
      </tr><tr>
        <td>
          200
        </td>
        <td>
          2579
        </td>
        <td>
          10286
          -
          10384
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;failed&quot;, ((x$3: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.impl.imports.RuleResultsImports.FailedExpr), scala.Predef.Set.apply[Int](0), register$default$4)
        </td>
      </tr><tr>
        <td>
          200
        </td>
        <td>
          2576
        </td>
        <td>
          10295
          -
          10303
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;failed&quot;
        </td>
      </tr><tr>
        <td>
          200
        </td>
        <td>
          2578
        </td>
        <td>
          10377
          -
          10383
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](0)
        </td>
      </tr><tr>
        <td>
          201
        </td>
        <td>
          2580
        </td>
        <td>
          10398
          -
          10411
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;soft_Failed&quot;
        </td>
      </tr><tr>
        <td>
          201
        </td>
        <td>
          2583
        </td>
        <td>
          10389
          -
          10496
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;soft_Failed&quot;, ((x$4: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.impl.imports.RuleResultsImports.SoftFailedExpr), scala.Predef.Set.apply[Int](0), register$default$4)
        </td>
      </tr><tr>
        <td>
          201
        </td>
        <td>
          2582
        </td>
        <td>
          10489
          -
          10495
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](0)
        </td>
      </tr><tr>
        <td>
          201
        </td>
        <td>
          2581
        </td>
        <td>
          10418
          -
          10487
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.imports.RuleResultsImports.SoftFailedExpr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.imports.RuleResultsImports.SoftFailedExpr
        </td>
      </tr><tr>
        <td>
          202
        </td>
        <td>
          2586
        </td>
        <td>
          10605
          -
          10611
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](0)
        </td>
      </tr><tr>
        <td>
          202
        </td>
        <td>
          2585
        </td>
        <td>
          10532
          -
          10603
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.imports.RuleResultsImports.DisabledRuleExpr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.imports.RuleResultsImports.DisabledRuleExpr
        </td>
      </tr><tr>
        <td>
          202
        </td>
        <td>
          2584
        </td>
        <td>
          10510
          -
          10525
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;disabled_Rule&quot;
        </td>
      </tr><tr>
        <td>
          202
        </td>
        <td>
          2587
        </td>
        <td>
          10501
          -
          10612
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;disabled_Rule&quot;, ((x$5: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.impl.imports.RuleResultsImports.DisabledRuleExpr), scala.Predef.Set.apply[Int](0), register$default$4)
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          2589
        </td>
        <td>
          10653
          -
          10660
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          2592
        </td>
        <td>
          10672
          -
          10678
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](2)
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          2588
        </td>
        <td>
          10627
          -
          10638
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;pack_Ints&quot;
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          2591
        </td>
        <td>
          10648
          -
          10670
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.Pack.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          Pack.apply(exps.apply(0), exps.apply(1))
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          2593
        </td>
        <td>
          10618
          -
          10679
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;pack_Ints&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; Pack.apply(exps.apply(0), exps.apply(1))), scala.Predef.Set.apply[Int](2), register$default$4)
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          2590
        </td>
        <td>
          10662
          -
          10669
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          206
        </td>
        <td>
          2598
        </td>
        <td>
          10685
          -
          10738
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;unpack&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; UnPack.apply(exps.head)), scala.Predef.Set.apply[Int](1), register$default$4)
        </td>
      </tr><tr>
        <td>
          206
        </td>
        <td>
          2595
        </td>
        <td>
          10719
          -
          10728
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          206
        </td>
        <td>
          2594
        </td>
        <td>
          10694
          -
          10702
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;unpack&quot;
        </td>
      </tr><tr>
        <td>
          206
        </td>
        <td>
          2597
        </td>
        <td>
          10731
          -
          10737
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1)
        </td>
      </tr><tr>
        <td>
          206
        </td>
        <td>
          2596
        </td>
        <td>
          10712
          -
          10729
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.UnPack.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          UnPack.apply(exps.head)
        </td>
      </tr><tr>
        <td>
          208
        </td>
        <td>
          2601
        </td>
        <td>
          10781
          -
          10806
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.UnPackIdTriple.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          UnPackIdTriple.apply(exps.head)
        </td>
      </tr><tr>
        <td>
          208
        </td>
        <td>
          2603
        </td>
        <td>
          10744
          -
          10815
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;unpack_Id_Triple&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; UnPackIdTriple.apply(exps.head)), scala.Predef.Set.apply[Int](1), register$default$4)
        </td>
      </tr><tr>
        <td>
          208
        </td>
        <td>
          2600
        </td>
        <td>
          10796
          -
          10805
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          208
        </td>
        <td>
          2602
        </td>
        <td>
          10808
          -
          10814
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1)
        </td>
      </tr><tr>
        <td>
          208
        </td>
        <td>
          2599
        </td>
        <td>
          10753
          -
          10771
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;unpack_Id_Triple&quot;
        </td>
      </tr><tr>
        <td>
          210
        </td>
        <td>
          2607
        </td>
        <td>
          10876
          -
          10882
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1)
        </td>
      </tr><tr>
        <td>
          210
        </td>
        <td>
          2604
        </td>
        <td>
          10830
          -
          10841
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;soft_Fail&quot;
        </td>
      </tr><tr>
        <td>
          210
        </td>
        <td>
          2606
        </td>
        <td>
          10851
          -
          10874
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.SoftFailExpr.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SoftFailExpr.apply(exps.head)
        </td>
      </tr><tr>
        <td>
          210
        </td>
        <td>
          2605
        </td>
        <td>
          10864
          -
          10873
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          210
        </td>
        <td>
          2608
        </td>
        <td>
          10821
          -
          10883
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;soft_Fail&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; SoftFailExpr.apply(exps.head)), scala.Predef.Set.apply[Int](1), register$default$4)
        </td>
      </tr><tr>
        <td>
          214
        </td>
        <td>
          2609
        </td>
        <td>
          11011
          -
          11023
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.unsafe.types.UTF8String.toString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          str.toString()
        </td>
      </tr><tr>
        <td>
          217
        </td>
        <td>
          2610
        </td>
        <td>
          11044
          -
          11071
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.PlaceHolder
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.PlaceHolder
        </td>
      </tr><tr>
        <td>
          217
        </td>
        <td>
          2620
        </td>
        <td>
          11035
          -
          11314
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(org.apache.spark.sql.qualityFunctions.LambdaFunctions.PlaceHolder, ((x0$1: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; x0$1 match {
  case scala.collection.Seq.unapplySeq[org.apache.spark.sql.catalyst.expressions.Expression](&lt;unapply-selector&gt;) &lt;unapply&gt; ((e @ _), (value: Any, dataType: org.apache.spark.sql.types.DataType)org.apache.spark.sql.catalyst.expressions.Literal((bol @ (_: Boolean)), org.apache.spark.sql.types.BooleanType)) =&gt; org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply(parse(e), bol)
  case scala.collection.Seq.unapplySeq[org.apache.spark.sql.catalyst.expressions.Expression](&lt;unapply-selector&gt;) &lt;unapply&gt; ((e @ _)) =&gt; org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply(parse(e), org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply$default$2)
  case _ =&gt; org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply$default$2)
}), scala.Predef.Set.apply[Int](0, 1, 2), register$default$4)
        </td>
      </tr><tr>
        <td>
          219
        </td>
        <td>
          2612
        </td>
        <td>
          11140
          -
          11176
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply(parse(e), bol)
        </td>
      </tr><tr>
        <td>
          219
        </td>
        <td>
          2611
        </td>
        <td>
          11162
          -
          11170
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.parse
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          parse(e)
        </td>
      </tr><tr>
        <td>
          221
        </td>
        <td>
          2613
        </td>
        <td>
          11228
          -
          11236
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.parse
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          parse(e)
        </td>
      </tr><tr>
        <td>
          221
        </td>
        <td>
          2615
        </td>
        <td>
          11206
          -
          11237
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply(parse(e), org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply$default$2)
        </td>
      </tr><tr>
        <td>
          221
        </td>
        <td>
          2614
        </td>
        <td>
          11206
          -
          11206
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply$default$2
        </td>
      </tr><tr>
        <td>
          223
        </td>
        <td>
          2616
        </td>
        <td>
          11284
          -
          11292
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          223
        </td>
        <td>
          2618
        </td>
        <td>
          11262
          -
          11293
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply$default$2)
        </td>
      </tr><tr>
        <td>
          223
        </td>
        <td>
          2617
        </td>
        <td>
          11262
          -
          11262
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply$default$2
        </td>
      </tr><tr>
        <td>
          224
        </td>
        <td>
          2619
        </td>
        <td>
          11301
          -
          11313
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](0, 1, 2)
        </td>
      </tr><tr>
        <td>
          230
        </td>
        <td>
          2625
        </td>
        <td>
          11506
          -
          11742
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(org.apache.spark.sql.qualityFunctions.LambdaFunctions.Lambda, ((x0$2: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; x0$2 match {
  case scala.collection.Seq.unapplySeq[org.apache.spark.sql.catalyst.expressions.Expression](&lt;unapply-selector&gt;) &lt;unapply&gt; ((fun @ (_: org.apache.spark.sql.qualityFunctions.FunForward))) =&gt; {
    val res: org.apache.spark.sql.catalyst.expressions.Expression = org.apache.spark.sql.qualityFunctions.FunCall.apply(fun);
    res
  }
  case scala.collection.Seq.unapplySeq[org.apache.spark.sql.catalyst.expressions.Expression](&lt;unapply-selector&gt;) &lt;unapply&gt; ((fun @ (_: org.apache.spark.sql.qualityFunctions.FunN))) =&gt; {
    val res: org.apache.spark.sql.catalyst.expressions.Expression = fun.function;
    res
  }
}), scala.Predef.Set.apply[Int](1), register$default$4)
        </td>
      </tr><tr>
        <td>
          230
        </td>
        <td>
          2621
        </td>
        <td>
          11515
          -
          11537
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.Lambda
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.Lambda
        </td>
      </tr><tr>
        <td>
          232
        </td>
        <td>
          2622
        </td>
        <td>
          11594
          -
          11606
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunCall.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunCall.apply(fun)
        </td>
      </tr><tr>
        <td>
          236
        </td>
        <td>
          2623
        </td>
        <td>
          11703
          -
          11715
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.function
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          fun.function
        </td>
      </tr><tr>
        <td>
          238
        </td>
        <td>
          2624
        </td>
        <td>
          11735
          -
          11741
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1)
        </td>
      </tr><tr>
        <td>
          240
        </td>
        <td>
          2636
        </td>
        <td>
          11748
          -
          12104
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(x$5, x$6, x$8, x$7)
        </td>
      </tr><tr>
        <td>
          240
        </td>
        <td>
          2626
        </td>
        <td>
          11757
          -
          11780
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun
        </td>
      </tr><tr>
        <td>
          242
        </td>
        <td>
          2627
        </td>
        <td>
          11884
          -
          11919
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.processTopCallFun
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.processTopCallFun(fun, l, ff, args)
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          2631
        </td>
        <td>
          11957
          -
          11980
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          2634
        </td>
        <td>
          11936
          -
          12084
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.QualityException.qualityException(scala.StringContext.apply(&quot;&quot;, &quot; should only be used to process partially applied functions returned by a user lambda, got &quot;, &quot; instead&quot;).s(org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun, t), com.sparkutils.quality.QualityException.qualityException$default$2)
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          2628
        </td>
        <td>
          11955
          -
          11956
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          2630
        </td>
        <td>
          12074
          -
          12083
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot; instead&quot;
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          2633
        </td>
        <td>
          11936
          -
          11936
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          2629
        </td>
        <td>
          11981
          -
          12073
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot; should only be used to process partially applied functions returned by a user lambda, got &quot;
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          2632
        </td>
        <td>
          11953
          -
          12083
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;&quot;, &quot; should only be used to process partially applied functions returned by a user lambda, got &quot;, &quot; instead&quot;).s(org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun, t)
        </td>
      </tr><tr>
        <td>
          244
        </td>
        <td>
          2635
        </td>
        <td>
          12102
          -
          12103
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          1
        </td>
      </tr><tr>
        <td>
          247
        </td>
        <td>
          2640
        </td>
        <td>
          12182
          -
          12182
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple4._4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$6._4
        </td>
      </tr><tr>
        <td>
          247
        </td>
        <td>
          2637
        </td>
        <td>
          12160
          -
          12160
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple4._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$6._1
        </td>
      </tr><tr>
        <td>
          247
        </td>
        <td>
          2639
        </td>
        <td>
          12177
          -
          12177
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple4._3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$6._3
        </td>
      </tr><tr>
        <td>
          247
        </td>
        <td>
          2638
        </td>
        <td>
          12169
          -
          12169
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple4._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$6._2
        </td>
      </tr><tr>
        <td>
          266
        </td>
        <td>
          2642
        </td>
        <td>
          13013
          -
          13073
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.apply(sumType, filter, sum, count, zero, add, com.sparkutils.quality.impl.aggregates.AggregateExpressions.apply$default$7)
        </td>
      </tr><tr>
        <td>
          266
        </td>
        <td>
          2641
        </td>
        <td>
          13013
          -
          13013
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.apply$default$7
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.apply$default$7
        </td>
      </tr><tr>
        <td>
          268
        </td>
        <td>
          2643
        </td>
        <td>
          13093
          -
          13103
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;agg_Expr&quot;
        </td>
      </tr><tr>
        <td>
          268
        </td>
        <td>
          2645
        </td>
        <td>
          13084
          -
          13120
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;agg_Expr&quot;, afx, scala.Predef.Set.apply[Int](3, 4), register$default$4)
        </td>
      </tr><tr>
        <td>
          268
        </td>
        <td>
          2644
        </td>
        <td>
          13110
          -
          13119
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](3, 4)
        </td>
      </tr><tr>
        <td>
          270
        </td>
        <td>
          2646
        </td>
        <td>
          13135
          -
          13145
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;sum_With&quot;
        </td>
      </tr><tr>
        <td>
          270
        </td>
        <td>
          2659
        </td>
        <td>
          13126
          -
          13429
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;sum_With&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; {
  &lt;synthetic&gt; &lt;artifact&gt; private[this] val x$7: (org.apache.spark.sql.types.DataType, org.apache.spark.sql.catalyst.expressions.Expression) = (exps.size match {
    case 1 =&gt; scala.Tuple2.apply[org.apache.spark.sql.types.LongType.type, org.apache.spark.sql.catalyst.expressions.Expression](org.apache.spark.sql.types.LongType, exps.apply(0))
    case 2 =&gt; scala.Tuple2.apply[org.apache.spark.sql.types.DataType, org.apache.spark.sql.catalyst.expressions.Expression](parse(exps.apply(0)), exps.apply(1))
  }: (org.apache.spark.sql.types.DataType, org.apache.spark.sql.catalyst.expressions.Expression) @unchecked) match {
    case (_1: org.apache.spark.sql.types.DataType, _2: org.apache.spark.sql.catalyst.expressions.Expression)(org.apache.spark.sql.types.DataType, org.apache.spark.sql.catalyst.expressions.Expression)((dataType @ _), (origExp @ _)) =&gt; scala.Tuple2.apply[org.apache.spark.sql.types.DataType, org.apache.spark.sql.catalyst.expressions.Expression](dataType, origExp)
  };
  val dataType: org.apache.spark.sql.types.DataType = x$7._1;
  val origExp: org.apache.spark.sql.catalyst.expressions.Expression = x$7._2;
  org.apache.spark.sql.qualityFunctions.FunN.apply(scala.collection.Seq.apply[org.apache.spark.sql.qualityFunctions.RefExpression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(dataType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)), origExp, scala.Some.apply[String](&quot;sum_With&quot;), org.apache.spark.sql.qualityFunctions.FunN.apply$default$4, org.apache.spark.sql.qualityFunctions.FunN.apply$default$5, org.apache.spark.sql.qualityFunctions.FunN.apply$default$6)
}), scala.Predef.Set.apply[Int](1, 2), register$default$4)
        </td>
      </tr><tr>
        <td>
          271
        </td>
        <td>
          2648
        </td>
        <td>
          13197
          -
          13197
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$7._2
        </td>
      </tr><tr>
        <td>
          271
        </td>
        <td>
          2647
        </td>
        <td>
          13187
          -
          13187
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$7._1
        </td>
      </tr><tr>
        <td>
          276
        </td>
        <td>
          2649
        </td>
        <td>
          13359
          -
          13359
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
      </tr><tr>
        <td>
          276
        </td>
        <td>
          2652
        </td>
        <td>
          13355
          -
          13383
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.qualityFunctions.RefExpression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(dataType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3))
        </td>
      </tr><tr>
        <td>
          276
        </td>
        <td>
          2655
        </td>
        <td>
          13350
          -
          13350
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$5
        </td>
      </tr><tr>
        <td>
          276
        </td>
        <td>
          2654
        </td>
        <td>
          13350
          -
          13350
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$4
        </td>
      </tr><tr>
        <td>
          276
        </td>
        <td>
          2657
        </td>
        <td>
          13350
          -
          13411
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply(scala.collection.Seq.apply[org.apache.spark.sql.qualityFunctions.RefExpression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(dataType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)), origExp, scala.Some.apply[String](&quot;sum_With&quot;), org.apache.spark.sql.qualityFunctions.FunN.apply$default$4, org.apache.spark.sql.qualityFunctions.FunN.apply$default$5, org.apache.spark.sql.qualityFunctions.FunN.apply$default$6)
        </td>
      </tr><tr>
        <td>
          276
        </td>
        <td>
          2651
        </td>
        <td>
          13359
          -
          13382
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply(dataType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)
        </td>
      </tr><tr>
        <td>
          276
        </td>
        <td>
          2653
        </td>
        <td>
          13394
          -
          13410
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[String](&quot;sum_With&quot;)
        </td>
      </tr><tr>
        <td>
          276
        </td>
        <td>
          2656
        </td>
        <td>
          13350
          -
          13350
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$6
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$6
        </td>
      </tr><tr>
        <td>
          276
        </td>
        <td>
          2650
        </td>
        <td>
          13359
          -
          13359
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          2658
        </td>
        <td>
          13419
          -
          13428
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1, 2)
        </td>
      </tr><tr>
        <td>
          281
        </td>
        <td>
          2661
        </td>
        <td>
          13586
          -
          13586
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$8._2
        </td>
      </tr><tr>
        <td>
          281
        </td>
        <td>
          2660
        </td>
        <td>
          13577
          -
          13577
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$8._1
        </td>
      </tr><tr>
        <td>
          287
        </td>
        <td>
          2667
        </td>
        <td>
          13795
          -
          13795
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
      </tr><tr>
        <td>
          287
        </td>
        <td>
          2670
        </td>
        <td>
          13826
          -
          13846
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[String](&quot;results_With&quot;)
        </td>
      </tr><tr>
        <td>
          287
        </td>
        <td>
          2673
        </td>
        <td>
          13762
          -
          13762
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$6
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$6
        </td>
      </tr><tr>
        <td>
          287
        </td>
        <td>
          2664
        </td>
        <td>
          13771
          -
          13793
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply(sumType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)
        </td>
      </tr><tr>
        <td>
          287
        </td>
        <td>
          2663
        </td>
        <td>
          13771
          -
          13771
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
      </tr><tr>
        <td>
          287
        </td>
        <td>
          2672
        </td>
        <td>
          13762
          -
          13762
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$5
        </td>
      </tr><tr>
        <td>
          287
        </td>
        <td>
          2666
        </td>
        <td>
          13795
          -
          13795
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
      </tr><tr>
        <td>
          287
        </td>
        <td>
          2669
        </td>
        <td>
          13767
          -
          13819
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.qualityFunctions.RefExpression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(sumType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3), org.apache.spark.sql.qualityFunctions.RefExpression.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3))
        </td>
      </tr><tr>
        <td>
          287
        </td>
        <td>
          2668
        </td>
        <td>
          13795
          -
          13818
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)
        </td>
      </tr><tr>
        <td>
          287
        </td>
        <td>
          2662
        </td>
        <td>
          13771
          -
          13771
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
      </tr><tr>
        <td>
          287
        </td>
        <td>
          2671
        </td>
        <td>
          13762
          -
          13762
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$4
        </td>
      </tr><tr>
        <td>
          287
        </td>
        <td>
          2674
        </td>
        <td>
          13762
          -
          13847
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply(scala.collection.Seq.apply[org.apache.spark.sql.qualityFunctions.RefExpression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(sumType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3), org.apache.spark.sql.qualityFunctions.RefExpression.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)), exp, scala.Some.apply[String](&quot;results_With&quot;), org.apache.spark.sql.qualityFunctions.FunN.apply$default$4, org.apache.spark.sql.qualityFunctions.FunN.apply$default$5, org.apache.spark.sql.qualityFunctions.FunN.apply$default$6)
        </td>
      </tr><tr>
        <td>
          287
        </td>
        <td>
          2665
        </td>
        <td>
          13809
          -
          13817
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          289
        </td>
        <td>
          2676
        </td>
        <td>
          13888
          -
          13897
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1, 2)
        </td>
      </tr><tr>
        <td>
          289
        </td>
        <td>
          2675
        </td>
        <td>
          13867
          -
          13881
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;results_With&quot;
        </td>
      </tr><tr>
        <td>
          289
        </td>
        <td>
          2677
        </td>
        <td>
          13858
          -
          13898
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;results_With&quot;, ff2, scala.Predef.Set.apply[Int](1, 2), register$default$4)
        </td>
      </tr><tr>
        <td>
          291
        </td>
        <td>
          2678
        </td>
        <td>
          13943
          -
          13952
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size
        </td>
      </tr><tr>
        <td>
          294
        </td>
        <td>
          2685
        </td>
        <td>
          14106
          -
          14113
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(2)
        </td>
      </tr><tr>
        <td>
          294
        </td>
        <td>
          2679
        </td>
        <td>
          14086
          -
          14093
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          294
        </td>
        <td>
          2682
        </td>
        <td>
          14066
          -
          14066
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
      </tr><tr>
        <td>
          294
        </td>
        <td>
          2681
        </td>
        <td>
          14066
          -
          14066
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
      </tr><tr>
        <td>
          294
        </td>
        <td>
          2684
        </td>
        <td>
          14097
          -
          14104
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          294
        </td>
        <td>
          2686
        </td>
        <td>
          14046
          -
          14120
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.MapTransform.create
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.MapTransform.create(org.apache.spark.sql.qualityFunctions.RefExpression.apply(parse(exps.apply(0)), org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3), exps.apply(1), exps.apply(2), zero)
        </td>
      </tr><tr>
        <td>
          294
        </td>
        <td>
          2680
        </td>
        <td>
          14080
          -
          14094
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.parse
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          parse(exps.apply(0))
        </td>
      </tr><tr>
        <td>
          294
        </td>
        <td>
          2683
        </td>
        <td>
          14066
          -
          14095
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply(parse(exps.apply(0)), org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)
        </td>
      </tr><tr>
        <td>
          297
        </td>
        <td>
          2694
        </td>
        <td>
          14323
          -
          14330
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          297
        </td>
        <td>
          2688
        </td>
        <td>
          14302
          -
          14310
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          297
        </td>
        <td>
          2690
        </td>
        <td>
          14270
          -
          14270
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
      </tr><tr>
        <td>
          297
        </td>
        <td>
          2693
        </td>
        <td>
          14314
          -
          14321
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          297
        </td>
        <td>
          2687
        </td>
        <td>
          14292
          -
          14300
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          297
        </td>
        <td>
          2695
        </td>
        <td>
          14250
          -
          14337
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.MapTransform.create
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.MapTransform.create(org.apache.spark.sql.qualityFunctions.RefExpression.apply(org.apache.spark.sql.types.MapType.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.types.LongType), org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3), exps.apply(0), exps.apply(1), zero)
        </td>
      </tr><tr>
        <td>
          297
        </td>
        <td>
          2689
        </td>
        <td>
          14284
          -
          14311
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.MapType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.MapType.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.types.LongType)
        </td>
      </tr><tr>
        <td>
          297
        </td>
        <td>
          2692
        </td>
        <td>
          14270
          -
          14312
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply(org.apache.spark.sql.types.MapType.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.types.LongType), org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)
        </td>
      </tr><tr>
        <td>
          297
        </td>
        <td>
          2691
        </td>
        <td>
          14270
          -
          14270
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
      </tr><tr>
        <td>
          299
        </td>
        <td>
          2697
        </td>
        <td>
          14376
          -
          14385
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](2, 3)
        </td>
      </tr><tr>
        <td>
          299
        </td>
        <td>
          2696
        </td>
        <td>
          14357
          -
          14367
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;map_With&quot;
        </td>
      </tr><tr>
        <td>
          299
        </td>
        <td>
          2698
        </td>
        <td>
          14348
          -
          14386
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;map_With&quot;, mapFX, scala.Predef.Set.apply[Int](2, 3), register$default$4)
        </td>
      </tr><tr>
        <td>
          301
        </td>
        <td>
          2712
        </td>
        <td>
          14465
          -
          14625
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(if (exps.size.==(0))
  org.apache.spark.sql.functions.expr(scala.StringContext.apply(&quot;&quot;, &quot;( &quot;, &quot; )&quot;).s(fun, what))
else
  org.apache.spark.sql.functions.expr(scala.StringContext.apply(&quot;&quot;, &quot;(\'&quot;, &quot;\', &quot;, &quot; )&quot;).s(fun, strType(exps.apply(0)), what)))
        </td>
      </tr><tr>
        <td>
          302
        </td>
        <td>
          2699
        </td>
        <td>
          14487
          -
          14501
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size.==(0)
        </td>
      </tr><tr>
        <td>
          303
        </td>
        <td>
          2702
        </td>
        <td>
          14511
          -
          14543
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.functions.expr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.functions.expr(scala.StringContext.apply(&quot;&quot;, &quot;( &quot;, &quot; )&quot;).s(fun, what))
        </td>
      </tr><tr>
        <td>
          303
        </td>
        <td>
          2701
        </td>
        <td>
          14511
          -
          14543
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.functions.expr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.functions.expr(scala.StringContext.apply(&quot;&quot;, &quot;( &quot;, &quot; )&quot;).s(fun, what))
        </td>
      </tr><tr>
        <td>
          303
        </td>
        <td>
          2700
        </td>
        <td>
          14526
          -
          14542
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;&quot;, &quot;( &quot;, &quot; )&quot;).s(fun, what)
        </td>
      </tr><tr>
        <td>
          305
        </td>
        <td>
          2703
        </td>
        <td>
          14580
          -
          14581
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          305
        </td>
        <td>
          2706
        </td>
        <td>
          14613
          -
          14616
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot; )&quot;
        </td>
      </tr><tr>
        <td>
          305
        </td>
        <td>
          2708
        </td>
        <td>
          14588
          -
          14604
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.strType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          strType(exps.apply(0))
        </td>
      </tr><tr>
        <td>
          305
        </td>
        <td>
          2711
        </td>
        <td>
          14563
          -
          14617
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.functions.expr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.functions.expr(scala.StringContext.apply(&quot;&quot;, &quot;(\'&quot;, &quot;\', &quot;, &quot; )&quot;).s(fun, strType(exps.apply(0)), what))
        </td>
      </tr><tr>
        <td>
          305
        </td>
        <td>
          2705
        </td>
        <td>
          14605
          -
          14609
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;\', &quot;
        </td>
      </tr><tr>
        <td>
          305
        </td>
        <td>
          2704
        </td>
        <td>
          14584
          -
          14587
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;(\'&quot;
        </td>
      </tr><tr>
        <td>
          305
        </td>
        <td>
          2707
        </td>
        <td>
          14596
          -
          14603
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          305
        </td>
        <td>
          2710
        </td>
        <td>
          14563
          -
          14617
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.functions.expr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.functions.expr(scala.StringContext.apply(&quot;&quot;, &quot;(\'&quot;, &quot;\', &quot;, &quot; )&quot;).s(fun, strType(exps.apply(0)), what))
        </td>
      </tr><tr>
        <td>
          305
        </td>
        <td>
          2709
        </td>
        <td>
          14578
          -
          14616
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;&quot;, &quot;(\'&quot;, &quot;\', &quot;, &quot; )&quot;).s(fun, strType(exps.apply(0)), what)
        </td>
      </tr><tr>
        <td>
          308
        </td>
        <td>
          2713
        </td>
        <td>
          14645
          -
          14669
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.aggFWith
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          aggFWith(&quot;results_With&quot;)
        </td>
      </tr><tr>
        <td>
          311
        </td>
        <td>
          2715
        </td>
        <td>
          14713
          -
          14751
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function1.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          retWith.apply(&quot;(sum, count) -&gt; sum / count&quot;)
        </td>
      </tr><tr>
        <td>
          311
        </td>
        <td>
          2714
        </td>
        <td>
          14704
          -
          14711
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;meanF&quot;
        </td>
      </tr><tr>
        <td>
          311
        </td>
        <td>
          2717
        </td>
        <td>
          14695
          -
          14763
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;meanF&quot;, retWith.apply(&quot;(sum, count) -&gt; sum / count&quot;), scala.Predef.Set.apply[Int](0, 1), register$default$4)
        </td>
      </tr><tr>
        <td>
          311
        </td>
        <td>
          2716
        </td>
        <td>
          14753
          -
          14762
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](0, 1)
        </td>
      </tr><tr>
        <td>
          313
        </td>
        <td>
          2718
        </td>
        <td>
          14783
          -
          14803
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.aggFWith
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          aggFWith(&quot;sum_With&quot;)
        </td>
      </tr><tr>
        <td>
          317
        </td>
        <td>
          2721
        </td>
        <td>
          14920
          -
          14954
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x.qualifier.mkString(&quot;.&quot;).+(x.name)
        </td>
      </tr><tr>
        <td>
          317
        </td>
        <td>
          2720
        </td>
        <td>
          14948
          -
          14954
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.AttributeReference.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x.name
        </td>
      </tr><tr>
        <td>
          317
        </td>
        <td>
          2719
        </td>
        <td>
          14941
          -
          14944
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;.&quot;
        </td>
      </tr><tr>
        <td>
          318
        </td>
        <td>
          2724
        </td>
        <td>
          15004
          -
          15041
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function1.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          sumWith.apply(scala.StringContext.apply(&quot;sum -&gt; sum + &quot;, &quot;&quot;).s(name)).apply(scala.collection.Seq.apply[Nothing]())
        </td>
      </tr><tr>
        <td>
          318
        </td>
        <td>
          2723
        </td>
        <td>
          15035
          -
          15040
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[Nothing]()
        </td>
      </tr><tr>
        <td>
          318
        </td>
        <td>
          2722
        </td>
        <td>
          15012
          -
          15033
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;sum -&gt; sum + &quot;, &quot;&quot;).s(name)
        </td>
      </tr><tr>
        <td>
          321
        </td>
        <td>
          2725
        </td>
        <td>
          15139
          -
          15170
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function1.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          sumWith.apply(&quot;sum -&gt; sum + 1&quot;).apply(exps)
        </td>
      </tr><tr>
        <td>
          323
        </td>
        <td>
          2726
        </td>
        <td>
          15290
          -
          15293
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;.&quot;
        </td>
      </tr><tr>
        <td>
          323
        </td>
        <td>
          2728
        </td>
        <td>
          15269
          -
          15303
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x.qualifier.mkString(&quot;.&quot;).+(x.name)
        </td>
      </tr><tr>
        <td>
          323
        </td>
        <td>
          2727
        </td>
        <td>
          15297
          -
          15303
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.AttributeReference.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x.name
        </td>
      </tr><tr>
        <td>
          324
        </td>
        <td>
          2730
        </td>
        <td>
          15347
          -
          15354
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          324
        </td>
        <td>
          2732
        </td>
        <td>
          15312
          -
          15356
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function1.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          sumWith.apply(scala.StringContext.apply(&quot;sum -&gt; sum + &quot;, &quot;&quot;).s(name)).apply(scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](exps.apply(0)))
        </td>
      </tr><tr>
        <td>
          324
        </td>
        <td>
          2729
        </td>
        <td>
          15320
          -
          15341
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;sum -&gt; sum + &quot;, &quot;&quot;).s(name)
        </td>
      </tr><tr>
        <td>
          324
        </td>
        <td>
          2731
        </td>
        <td>
          15343
          -
          15355
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](exps.apply(0))
        </td>
      </tr><tr>
        <td>
          326
        </td>
        <td>
          2733
        </td>
        <td>
          15473
          -
          15499
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.INC_REWRITE_GENEXP_ERR_MSG
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.INC_REWRITE_GENEXP_ERR_MSG
        </td>
      </tr><tr>
        <td>
          326
        </td>
        <td>
          2735
        </td>
        <td>
          15456
          -
          15500
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.QualityException.qualityException(RuleRegistrationFunctions.this.INC_REWRITE_GENEXP_ERR_MSG, com.sparkutils.quality.QualityException.qualityException$default$2)
        </td>
      </tr><tr>
        <td>
          326
        </td>
        <td>
          2734
        </td>
        <td>
          15456
          -
          15456
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
      </tr><tr>
        <td>
          328
        </td>
        <td>
          2738
        </td>
        <td>
          15601
          -
          15601
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$9._3
        </td>
      </tr><tr>
        <td>
          328
        </td>
        <td>
          2737
        </td>
        <td>
          15564
          -
          15564
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$9._2
        </td>
      </tr><tr>
        <td>
          328
        </td>
        <td>
          2736
        </td>
        <td>
          15552
          -
          15552
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$9._1
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          2739
        </td>
        <td>
          15830
          -
          15838
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          2742
        </td>
        <td>
          15816
          -
          15839
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          2741
        </td>
        <td>
          15816
          -
          15816
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          2750
        </td>
        <td>
          15807
          -
          15807
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$4
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          2753
        </td>
        <td>
          15807
          -
          15938
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply(scala.collection.Seq.apply[org.apache.spark.sql.qualityFunctions.RefExpression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)), org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply(org.apache.spark.sql.ShimUtils.add(a.left, y, org.apache.spark.sql.types.LongType), scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.UnresolvedNamedLambdaVariable](sum), hidden), scala.Some.apply[String](&quot;inc&quot;), org.apache.spark.sql.qualityFunctions.FunN.apply$default$4, org.apache.spark.sql.qualityFunctions.FunN.apply$default$5, org.apache.spark.sql.qualityFunctions.FunN.apply$default$6)
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          2740
        </td>
        <td>
          15816
          -
          15816
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          2752
        </td>
        <td>
          15807
          -
          15807
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$6
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$6
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          2743
        </td>
        <td>
          15812
          -
          15840
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.qualityFunctions.RefExpression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3))
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          2751
        </td>
        <td>
          15807
          -
          15807
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$5
        </td>
      </tr><tr>
        <td>
          332
        </td>
        <td>
          2748
        </td>
        <td>
          15852
          -
          15913
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply(org.apache.spark.sql.ShimUtils.add(a.left, y, org.apache.spark.sql.types.LongType), scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.UnresolvedNamedLambdaVariable](sum), hidden)
        </td>
      </tr><tr>
        <td>
          332
        </td>
        <td>
          2744
        </td>
        <td>
          15873
          -
          15879
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Add.left
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.left
        </td>
      </tr><tr>
        <td>
          332
        </td>
        <td>
          2747
        </td>
        <td>
          15895
          -
          15903
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.UnresolvedNamedLambdaVariable](sum)
        </td>
      </tr><tr>
        <td>
          332
        </td>
        <td>
          2746
        </td>
        <td>
          15868
          -
          15893
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.add
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.add(a.left, y, org.apache.spark.sql.types.LongType)
        </td>
      </tr><tr>
        <td>
          332
        </td>
        <td>
          2745
        </td>
        <td>
          15884
          -
          15892
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          333
        </td>
        <td>
          2749
        </td>
        <td>
          15926
          -
          15937
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[String](&quot;inc&quot;)
        </td>
      </tr><tr>
        <td>
          334
        </td>
        <td>
          2756
        </td>
        <td>
          15976
          -
          16030
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(org.apache.spark.sql.functions.expr(scala.StringContext.apply(&quot;sumWith(sum -&gt; sum + 1)&quot;).s()))
        </td>
      </tr><tr>
        <td>
          334
        </td>
        <td>
          2755
        </td>
        <td>
          15987
          -
          16029
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.functions.expr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.functions.expr(scala.StringContext.apply(&quot;sumWith(sum -&gt; sum + 1)&quot;).s())
        </td>
      </tr><tr>
        <td>
          334
        </td>
        <td>
          2754
        </td>
        <td>
          16002
          -
          16028
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;sumWith(sum -&gt; sum + 1)&quot;).s()
        </td>
      </tr><tr>
        <td>
          336
        </td>
        <td>
          2757
        </td>
        <td>
          16050
          -
          16055
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;inc&quot;
        </td>
      </tr><tr>
        <td>
          336
        </td>
        <td>
          2759
        </td>
        <td>
          16041
          -
          16076
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;inc&quot;, incX, scala.Predef.Set.apply[Int](1, 0, 2), register$default$4)
        </td>
      </tr><tr>
        <td>
          336
        </td>
        <td>
          2758
        </td>
        <td>
          16063
          -
          16075
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1, 0, 2)
        </td>
      </tr><tr>
        <td>
          339
        </td>
        <td>
          2762
        </td>
        <td>
          16155
          -
          16164
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](0, 1)
        </td>
      </tr><tr>
        <td>
          339
        </td>
        <td>
          2761
        </td>
        <td>
          16123
          -
          16153
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function1.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          retWith.apply(&quot;(sum, count) -&gt; sum&quot;)
        </td>
      </tr><tr>
        <td>
          339
        </td>
        <td>
          2760
        </td>
        <td>
          16109
          -
          16121
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;return_Sum&quot;
        </td>
      </tr><tr>
        <td>
          339
        </td>
        <td>
          2763
        </td>
        <td>
          16100
          -
          16165
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;return_Sum&quot;, retWith.apply(&quot;(sum, count) -&gt; sum&quot;), scala.Predef.Set.apply[Int](0, 1), register$default$4)
        </td>
      </tr><tr>
        <td>
          341
        </td>
        <td>
          2764
        </td>
        <td>
          16231
          -
          16250
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.getString(exp, pos)
        </td>
      </tr><tr>
        <td>
          342
        </td>
        <td>
          2765
        </td>
        <td>
          16257
          -
          16282
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.commons.rng.simple.RandomSource.valueOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.commons.rng.simple.RandomSource.valueOf(str)
        </td>
      </tr><tr>
        <td>
          348
        </td>
        <td>
          2766
        </td>
        <td>
          16483
          -
          16483
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$10._1
        </td>
      </tr><tr>
        <td>
          348
        </td>
        <td>
          2768
        </td>
        <td>
          16512
          -
          16512
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$10._3
        </td>
      </tr><tr>
        <td>
          348
        </td>
        <td>
          2767
        </td>
        <td>
          16498
          -
          16498
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$10._2
        </td>
      </tr><tr>
        <td>
          357
        </td>
        <td>
          2769
        </td>
        <td>
          16877
          -
          16918
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.rng.RandomBytes.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.rng.RandomBytes.apply(numBytes, randomSource, seed)
        </td>
      </tr><tr>
        <td>
          359
        </td>
        <td>
          2771
        </td>
        <td>
          16956
          -
          16968
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](0, 1, 2, 3)
        </td>
      </tr><tr>
        <td>
          359
        </td>
        <td>
          2770
        </td>
        <td>
          16938
          -
          16949
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;rng_Bytes&quot;
        </td>
      </tr><tr>
        <td>
          359
        </td>
        <td>
          2772
        </td>
        <td>
          16929
          -
          16969
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;rng_Bytes&quot;, brf, scala.Predef.Set.apply[Int](0, 1, 2, 3), register$default$4)
        </td>
      </tr><tr>
        <td>
          365
        </td>
        <td>
          2774
        </td>
        <td>
          17164
          -
          17164
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$11._2
        </td>
      </tr><tr>
        <td>
          365
        </td>
        <td>
          2773
        </td>
        <td>
          17150
          -
          17150
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$11._1
        </td>
      </tr><tr>
        <td>
          373
        </td>
        <td>
          2775
        </td>
        <td>
          17428
          -
          17466
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.rng.RandomLongs.create
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.rng.RandomLongs.create(randomSource, seed)
        </td>
      </tr><tr>
        <td>
          375
        </td>
        <td>
          2777
        </td>
        <td>
          17498
          -
          17508
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](0, 1, 2)
        </td>
      </tr><tr>
        <td>
          375
        </td>
        <td>
          2776
        </td>
        <td>
          17486
          -
          17491
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;rng&quot;
        </td>
      </tr><tr>
        <td>
          375
        </td>
        <td>
          2778
        </td>
        <td>
          17477
          -
          17509
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;rng&quot;, lrf, scala.Predef.Set.apply[Int](0, 1, 2), register$default$4)
        </td>
      </tr><tr>
        <td>
          377
        </td>
        <td>
          2783
        </td>
        <td>
          17515
          -
          17593
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;rng_UUID&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; RngUUIDExpression.apply(exps.head)), scala.Predef.Set.apply[Int](1), register$default$4)
        </td>
      </tr><tr>
        <td>
          377
        </td>
        <td>
          2779
        </td>
        <td>
          17524
          -
          17534
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;rng_UUID&quot;
        </td>
      </tr><tr>
        <td>
          378
        </td>
        <td>
          2780
        </td>
        <td>
          17568
          -
          17577
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          378
        </td>
        <td>
          2781
        </td>
        <td>
          17550
          -
          17578
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RngUUIDExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RngUUIDExpression.apply(exps.head)
        </td>
      </tr><tr>
        <td>
          379
        </td>
        <td>
          2782
        </td>
        <td>
          17586
          -
          17592
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1)
        </td>
      </tr><tr>
        <td>
          381
        </td>
        <td>
          2784
        </td>
        <td>
          17608
          -
          17619
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;long_Pair&quot;
        </td>
      </tr><tr>
        <td>
          381
        </td>
        <td>
          2786
        </td>
        <td>
          17657
          -
          17664
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          381
        </td>
        <td>
          2789
        </td>
        <td>
          17599
          -
          17674
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;long_Pair&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.impl.longPair.LongPairExpression.apply(exps.apply(0), exps.apply(1))), scala.Predef.Set.apply[Int](2), register$default$4)
        </td>
      </tr><tr>
        <td>
          381
        </td>
        <td>
          2788
        </td>
        <td>
          17667
          -
          17673
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](2)
        </td>
      </tr><tr>
        <td>
          381
        </td>
        <td>
          2785
        </td>
        <td>
          17648
          -
          17655
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          381
        </td>
        <td>
          2787
        </td>
        <td>
          17629
          -
          17665
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.longPair.LongPairExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.longPair.LongPairExpression.apply(exps.apply(0), exps.apply(1))
        </td>
      </tr><tr>
        <td>
          382
        </td>
        <td>
          2793
        </td>
        <td>
          17753
          -
          17759
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1)
        </td>
      </tr><tr>
        <td>
          382
        </td>
        <td>
          2792
        </td>
        <td>
          17719
          -
          17751
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.UUIDToLongsExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          UUIDToLongsExpression.apply(exps.head)
        </td>
      </tr><tr>
        <td>
          382
        </td>
        <td>
          2791
        </td>
        <td>
          17741
          -
          17750
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          382
        </td>
        <td>
          2794
        </td>
        <td>
          17679
          -
          17760
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;long_Pair_From_UUID&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; UUIDToLongsExpression.apply(exps.head)), scala.Predef.Set.apply[Int](1), register$default$4)
        </td>
      </tr><tr>
        <td>
          382
        </td>
        <td>
          2790
        </td>
        <td>
          17688
          -
          17709
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;long_Pair_From_UUID&quot;
        </td>
      </tr><tr>
        <td>
          384
        </td>
        <td>
          2802
        </td>
        <td>
          17844
          -
          17850
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](3)
        </td>
      </tr><tr>
        <td>
          384
        </td>
        <td>
          2801
        </td>
        <td>
          17798
          -
          17842
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.bloom.ParquetAggregator.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.bloom.ParquetAggregator.apply(exps.apply(0), exps.apply(1), exps.apply(2), com.sparkutils.quality.impl.bloom.ParquetAggregator.apply$default$4, com.sparkutils.quality.impl.bloom.ParquetAggregator.apply$default$5)
        </td>
      </tr><tr>
        <td>
          384
        </td>
        <td>
          2795
        </td>
        <td>
          17775
          -
          17788
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;small_Bloom&quot;
        </td>
      </tr><tr>
        <td>
          384
        </td>
        <td>
          2798
        </td>
        <td>
          17834
          -
          17841
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(2)
        </td>
      </tr><tr>
        <td>
          384
        </td>
        <td>
          2797
        </td>
        <td>
          17825
          -
          17832
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          384
        </td>
        <td>
          2800
        </td>
        <td>
          17798
          -
          17798
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.bloom.ParquetAggregator.apply$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.bloom.ParquetAggregator.apply$default$5
        </td>
      </tr><tr>
        <td>
          384
        </td>
        <td>
          2803
        </td>
        <td>
          17766
          -
          17851
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;small_Bloom&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.impl.bloom.ParquetAggregator.apply(exps.apply(0), exps.apply(1), exps.apply(2), com.sparkutils.quality.impl.bloom.ParquetAggregator.apply$default$4, com.sparkutils.quality.impl.bloom.ParquetAggregator.apply$default$5)), scala.Predef.Set.apply[Int](3), register$default$4)
        </td>
      </tr><tr>
        <td>
          384
        </td>
        <td>
          2796
        </td>
        <td>
          17816
          -
          17823
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          384
        </td>
        <td>
          2799
        </td>
        <td>
          17798
          -
          17798
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.bloom.ParquetAggregator.apply$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.bloom.ParquetAggregator.apply$default$4
        </td>
      </tr><tr>
        <td>
          386
        </td>
        <td>
          2804
        </td>
        <td>
          17866
          -
          17877
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;big_Bloom&quot;
        </td>
      </tr><tr>
        <td>
          386
        </td>
        <td>
          2824
        </td>
        <td>
          17857
          -
          18141
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;big_Bloom&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; exps.size match {
  case 4 =&gt; com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply(exps.apply(0), exps.apply(1), exps.apply(2), exps.apply(3), com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$5, com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$6, com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$7)
  case 3 =&gt; com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply(exps.apply(0), exps.apply(1), exps.apply(2), org.apache.spark.sql.catalyst.expressions.Literal.apply(java.util.UUID.randomUUID().toString()), com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$5, com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$6, com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$7)
}), scala.Predef.Set.apply[Int](3, 4), register$default$4)
        </td>
      </tr><tr>
        <td>
          386
        </td>
        <td>
          2805
        </td>
        <td>
          17887
          -
          17896
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size
        </td>
      </tr><tr>
        <td>
          388
        </td>
        <td>
          2810
        </td>
        <td>
          17929
          -
          17929
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$5
        </td>
      </tr><tr>
        <td>
          388
        </td>
        <td>
          2813
        </td>
        <td>
          17929
          -
          17995
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply(exps.apply(0), exps.apply(1), exps.apply(2), exps.apply(3), com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$5, com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$6, com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$7)
        </td>
      </tr><tr>
        <td>
          388
        </td>
        <td>
          2807
        </td>
        <td>
          17969
          -
          17976
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          388
        </td>
        <td>
          2806
        </td>
        <td>
          17960
          -
          17967
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          388
        </td>
        <td>
          2809
        </td>
        <td>
          17987
          -
          17994
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(3)
        </td>
      </tr><tr>
        <td>
          388
        </td>
        <td>
          2812
        </td>
        <td>
          17929
          -
          17929
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$7
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$7
        </td>
      </tr><tr>
        <td>
          388
        </td>
        <td>
          2811
        </td>
        <td>
          17929
          -
          17929
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$6
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$6
        </td>
      </tr><tr>
        <td>
          388
        </td>
        <td>
          2808
        </td>
        <td>
          17978
          -
          17985
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(2)
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          2819
        </td>
        <td>
          18020
          -
          18020
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$5
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          2822
        </td>
        <td>
          18020
          -
          18124
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply(exps.apply(0), exps.apply(1), exps.apply(2), org.apache.spark.sql.catalyst.expressions.Literal.apply(java.util.UUID.randomUUID().toString()), com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$5, com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$6, com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$7)
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          2816
        </td>
        <td>
          18069
          -
          18076
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(2)
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          2815
        </td>
        <td>
          18060
          -
          18067
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          2818
        </td>
        <td>
          18078
          -
          18123
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Literal.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Literal.apply(java.util.UUID.randomUUID().toString())
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          2821
        </td>
        <td>
          18020
          -
          18020
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$7
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$7
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          2820
        </td>
        <td>
          18020
          -
          18020
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$6
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$6
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          2814
        </td>
        <td>
          18051
          -
          18058
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          2817
        </td>
        <td>
          18086
          -
          18122
        </td>
        <td>
          Apply
        </td>
        <td>
          java.util.UUID.toString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          java.util.UUID.randomUUID().toString()
        </td>
      </tr><tr>
        <td>
          391
        </td>
        <td>
          2823
        </td>
        <td>
          18132
          -
          18140
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](3, 4)
        </td>
      </tr><tr>
        <td>
          395
        </td>
        <td>
          2825
        </td>
        <td>
          18219
          -
          18219
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$12._1
        </td>
      </tr><tr>
        <td>
          395
        </td>
        <td>
          2826
        </td>
        <td>
          18243
          -
          18243
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$12._2
        </td>
      </tr><tr>
        <td>
          396
        </td>
        <td>
          2828
        </td>
        <td>
          18311
          -
          18321
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.toString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          b.toString()
        </td>
      </tr><tr>
        <td>
          396
        </td>
        <td>
          2827
        </td>
        <td>
          18299
          -
          18309
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.toString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.toString()
        </td>
      </tr><tr>
        <td>
          396
        </td>
        <td>
          2830
        </td>
        <td>
          18272
          -
          18323
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.long_pair_equal(a.toString(), b.toString()))
        </td>
      </tr><tr>
        <td>
          396
        </td>
        <td>
          2829
        </td>
        <td>
          18283
          -
          18322
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.functions.long_pair_equal
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.long_pair_equal(a.toString(), b.toString())
        </td>
      </tr><tr>
        <td>
          398
        </td>
        <td>
          2831
        </td>
        <td>
          18343
          -
          18360
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;long_Pair_Equal&quot;
        </td>
      </tr><tr>
        <td>
          398
        </td>
        <td>
          2833
        </td>
        <td>
          18334
          -
          18384
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;long_Pair_Equal&quot;, longPairEqual, scala.Predef.Set.apply[Int](2), register$default$4)
        </td>
      </tr><tr>
        <td>
          398
        </td>
        <td>
          2832
        </td>
        <td>
          18377
          -
          18383
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](2)
        </td>
      </tr><tr>
        <td>
          401
        </td>
        <td>
          2834
        </td>
        <td>
          18455
          -
          18455
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$13._1
        </td>
      </tr><tr>
        <td>
          401
        </td>
        <td>
          2835
        </td>
        <td>
          18479
          -
          18479
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$13._2
        </td>
      </tr><tr>
        <td>
          402
        </td>
        <td>
          2837
        </td>
        <td>
          18540
          -
          18550
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.toString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          b.toString()
        </td>
      </tr><tr>
        <td>
          402
        </td>
        <td>
          2836
        </td>
        <td>
          18528
          -
          18538
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.toString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.toString()
        </td>
      </tr><tr>
        <td>
          402
        </td>
        <td>
          2839
        </td>
        <td>
          18508
          -
          18552
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.id_equal(a.toString(), b.toString()))
        </td>
      </tr><tr>
        <td>
          402
        </td>
        <td>
          2838
        </td>
        <td>
          18519
          -
          18551
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.functions.id_equal
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.id_equal(a.toString(), b.toString())
        </td>
      </tr><tr>
        <td>
          404
        </td>
        <td>
          2840
        </td>
        <td>
          18572
          -
          18581
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;idEqual&quot;
        </td>
      </tr><tr>
        <td>
          404
        </td>
        <td>
          2842
        </td>
        <td>
          18563
          -
          18599
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;idEqual&quot;, idEqual, scala.Predef.Set.apply[Int](2), register$default$4)
        </td>
      </tr><tr>
        <td>
          404
        </td>
        <td>
          2841
        </td>
        <td>
          18592
          -
          18598
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](2)
        </td>
      </tr><tr>
        <td>
          406
        </td>
        <td>
          2843
        </td>
        <td>
          18614
          -
          18623
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;as_uuid&quot;
        </td>
      </tr><tr>
        <td>
          406
        </td>
        <td>
          2846
        </td>
        <td>
          18633
          -
          18657
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.longPair.AsUUID.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.longPair.AsUUID.apply(exps.apply(0), exps.apply(1))
        </td>
      </tr><tr>
        <td>
          406
        </td>
        <td>
          2848
        </td>
        <td>
          18605
          -
          18666
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;as_uuid&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.impl.longPair.AsUUID.apply(exps.apply(0), exps.apply(1))), scala.Predef.Set.apply[Int](2), register$default$4)
        </td>
      </tr><tr>
        <td>
          406
        </td>
        <td>
          2845
        </td>
        <td>
          18649
          -
          18656
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          406
        </td>
        <td>
          2847
        </td>
        <td>
          18659
          -
          18665
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](2)
        </td>
      </tr><tr>
        <td>
          406
        </td>
        <td>
          2844
        </td>
        <td>
          18640
          -
          18647
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          408
        </td>
        <td>
          2852
        </td>
        <td>
          18760
          -
          18766
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1)
        </td>
      </tr><tr>
        <td>
          408
        </td>
        <td>
          2849
        </td>
        <td>
          18681
          -
          18708
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;rule_Suite_Result_Details&quot;
        </td>
      </tr><tr>
        <td>
          408
        </td>
        <td>
          2851
        </td>
        <td>
          18718
          -
          18758
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleSuiteResultDetailsExpr.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.RuleSuiteResultDetailsExpr.apply(exps.apply(0))
        </td>
      </tr><tr>
        <td>
          408
        </td>
        <td>
          2850
        </td>
        <td>
          18750
          -
          18757
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          408
        </td>
        <td>
          2853
        </td>
        <td>
          18672
          -
          18767
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;rule_Suite_Result_Details&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.impl.RuleSuiteResultDetailsExpr.apply(exps.apply(0))), scala.Predef.Set.apply[Int](1), register$default$4)
        </td>
      </tr><tr>
        <td>
          410
        </td>
        <td>
          2861
        </td>
        <td>
          18828
          -
          18905
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashRelatedFunctionImports.digest_to_longs_struct
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.digest_to_longs_struct(RuleRegistrationFunctions.this.getString(exps.head, 0), (exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$14: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$14)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]): _*))
        </td>
      </tr><tr>
        <td>
          410
        </td>
        <td>
          2855
        </td>
        <td>
          18861
          -
          18870
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          410
        </td>
        <td>
          2864
        </td>
        <td>
          18773
          -
          18920
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(x$9, x$10, x$12, x$11)
        </td>
      </tr><tr>
        <td>
          410
        </td>
        <td>
          2858
        </td>
        <td>
          18890
          -
          18899
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.column
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.column(x$14)
        </td>
      </tr><tr>
        <td>
          410
        </td>
        <td>
          2857
        </td>
        <td>
          18851
          -
          18874
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.getString(exps.head, 0)
        </td>
      </tr><tr>
        <td>
          410
        </td>
        <td>
          2860
        </td>
        <td>
          18876
          -
          18900
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$14: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$14)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column])
        </td>
      </tr><tr>
        <td>
          410
        </td>
        <td>
          2854
        </td>
        <td>
          18782
          -
          18806
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;digest_To_Longs_Struct&quot;
        </td>
      </tr><tr>
        <td>
          410
        </td>
        <td>
          2863
        </td>
        <td>
          18918
          -
          18919
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          2
        </td>
      </tr><tr>
        <td>
          410
        </td>
        <td>
          2856
        </td>
        <td>
          18872
          -
          18873
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          410
        </td>
        <td>
          2859
        </td>
        <td>
          18889
          -
          18889
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]
        </td>
      </tr><tr>
        <td>
          410
        </td>
        <td>
          2862
        </td>
        <td>
          18817
          -
          18906
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.digest_to_longs_struct(RuleRegistrationFunctions.this.getString(exps.head, 0), (exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$14: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$14)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]): _*)))
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          2870
        </td>
        <td>
          19026
          -
          19026
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          2873
        </td>
        <td>
          18961
          -
          19043
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.digest_to_longs(RuleRegistrationFunctions.this.getString(exps.head, 0), (exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$15: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$15)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]): _*)))
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          2867
        </td>
        <td>
          19009
          -
          19010
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          2875
        </td>
        <td>
          18925
          -
          19057
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(x$13, x$14, x$16, x$15)
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          2866
        </td>
        <td>
          18998
          -
          19007
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          2869
        </td>
        <td>
          19027
          -
          19036
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.column
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.column(x$15)
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          2872
        </td>
        <td>
          18972
          -
          19042
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashRelatedFunctionImports.digest_to_longs
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.digest_to_longs(RuleRegistrationFunctions.this.getString(exps.head, 0), (exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$15: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$15)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]): _*))
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          2871
        </td>
        <td>
          19013
          -
          19037
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$15: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$15)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column])
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          2865
        </td>
        <td>
          18934
          -
          18951
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;digest_To_Longs&quot;
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          2874
        </td>
        <td>
          19055
          -
          19056
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          2
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          2868
        </td>
        <td>
          18988
          -
          19011
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.getString(exps.head, 0)
        </td>
      </tr><tr>
        <td>
          414
        </td>
        <td>
          2876
        </td>
        <td>
          19162
          -
          19171
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size
        </td>
      </tr><tr>
        <td>
          415
        </td>
        <td>
          2877
        </td>
        <td>
          19198
          -
          19203
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.&gt;(3)
        </td>
      </tr><tr>
        <td>
          416
        </td>
        <td>
          2879
        </td>
        <td>
          19253
          -
          19254
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          1
        </td>
      </tr><tr>
        <td>
          416
        </td>
        <td>
          2878
        </td>
        <td>
          19244
          -
          19251
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          416
        </td>
        <td>
          2880
        </td>
        <td>
          19234
          -
          19255
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.getString(exps.apply(1), 1)
        </td>
      </tr><tr>
        <td>
          417
        </td>
        <td>
          2882
        </td>
        <td>
          19300
          -
          19301
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          417
        </td>
        <td>
          2881
        </td>
        <td>
          19289
          -
          19298
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          417
        </td>
        <td>
          2883
        </td>
        <td>
          19279
          -
          19302
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.getString(exps.head, 0)
        </td>
      </tr><tr>
        <td>
          418
        </td>
        <td>
          2888
        </td>
        <td>
          19324
          -
          19378
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function3.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          func.apply(prefix, digestImpl, exps.drop(2).map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$16: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$16)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]))
        </td>
      </tr><tr>
        <td>
          418
        </td>
        <td>
          2885
        </td>
        <td>
          19366
          -
          19375
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.column
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.column(x$16)
        </td>
      </tr><tr>
        <td>
          418
        </td>
        <td>
          2884
        </td>
        <td>
          19359
          -
          19360
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          2
        </td>
      </tr><tr>
        <td>
          418
        </td>
        <td>
          2887
        </td>
        <td>
          19349
          -
          19376
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.drop(2).map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$16: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$16)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column])
        </td>
      </tr><tr>
        <td>
          418
        </td>
        <td>
          2889
        </td>
        <td>
          19313
          -
          19379
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(func.apply(prefix, digestImpl, exps.drop(2).map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$16: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$16)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column])))
        </td>
      </tr><tr>
        <td>
          418
        </td>
        <td>
          2886
        </td>
        <td>
          19365
          -
          19365
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]
        </td>
      </tr><tr>
        <td>
          420
        </td>
        <td>
          2890
        </td>
        <td>
          19399
          -
          19413
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.literalsNeeded
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.literalsNeeded
        </td>
      </tr><tr>
        <td>
          423
        </td>
        <td>
          2894
        </td>
        <td>
          19496
          -
          19497
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          3
        </td>
      </tr><tr>
        <td>
          423
        </td>
        <td>
          2891
        </td>
        <td>
          19436
          -
          19452
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;field_Based_ID&quot;
        </td>
      </tr><tr>
        <td>
          423
        </td>
        <td>
          2893
        </td>
        <td>
          19454
          -
          19484
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.fieldBasedID
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          fieldBasedID({
  ((prefix: String, digestImpl: String, cols: Seq[org.apache.spark.sql.Column]) =&gt; com.sparkutils.quality.functions.`package`.field_based_id(prefix, digestImpl, (cols: _*)))
})
        </td>
      </tr><tr>
        <td>
          423
        </td>
        <td>
          2892
        </td>
        <td>
          19467
          -
          19481
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashRelatedFunctionImports.field_based_id
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.field_based_id(prefix, digestImpl, (cols: _*))
        </td>
      </tr><tr>
        <td>
          423
        </td>
        <td>
          2895
        </td>
        <td>
          19427
          -
          19498
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(x$17, x$18, x$20, x$19)
        </td>
      </tr><tr>
        <td>
          424
        </td>
        <td>
          2897
        </td>
        <td>
          19552
          -
          19575
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashRelatedFunctionImports.za_longs_field_based_id
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.za_longs_field_based_id(prefix, digestImpl, (cols: _*))
        </td>
      </tr><tr>
        <td>
          424
        </td>
        <td>
          2900
        </td>
        <td>
          19503
          -
          19592
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(x$21, x$22, x$24, x$23)
        </td>
      </tr><tr>
        <td>
          424
        </td>
        <td>
          2896
        </td>
        <td>
          19512
          -
          19537
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;za_Longs_Field_Based_ID&quot;
        </td>
      </tr><tr>
        <td>
          424
        </td>
        <td>
          2899
        </td>
        <td>
          19590
          -
          19591
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          3
        </td>
      </tr><tr>
        <td>
          424
        </td>
        <td>
          2898
        </td>
        <td>
          19539
          -
          19578
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.fieldBasedID
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          fieldBasedID({
  ((prefix: String, digestImpl: String, cols: Seq[org.apache.spark.sql.Column]) =&gt; com.sparkutils.quality.functions.`package`.za_longs_field_based_id(prefix, digestImpl, (cols: _*)))
})
        </td>
      </tr><tr>
        <td>
          425
        </td>
        <td>
          2903
        </td>
        <td>
          19627
          -
          19660
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.fieldBasedID
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          fieldBasedID({
  ((prefix: String, digestImpl: String, cols: Seq[org.apache.spark.sql.Column]) =&gt; com.sparkutils.quality.functions.`package`.za_field_based_id(prefix, digestImpl, (cols: _*)))
})
        </td>
      </tr><tr>
        <td>
          425
        </td>
        <td>
          2902
        </td>
        <td>
          19640
          -
          19657
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashRelatedFunctionImports.za_field_based_id
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.za_field_based_id(prefix, digestImpl, (cols: _*))
        </td>
      </tr><tr>
        <td>
          425
        </td>
        <td>
          2905
        </td>
        <td>
          19597
          -
          19674
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(x$25, x$26, x$28, x$27)
        </td>
      </tr><tr>
        <td>
          425
        </td>
        <td>
          2901
        </td>
        <td>
          19606
          -
          19625
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;za_Field_Based_ID&quot;
        </td>
      </tr><tr>
        <td>
          425
        </td>
        <td>
          2904
        </td>
        <td>
          19672
          -
          19673
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          3
        </td>
      </tr><tr>
        <td>
          426
        </td>
        <td>
          2906
        </td>
        <td>
          19688
          -
          19709
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;hash_Field_Based_ID&quot;
        </td>
      </tr><tr>
        <td>
          426
        </td>
        <td>
          2909
        </td>
        <td>
          19758
          -
          19759
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          3
        </td>
      </tr><tr>
        <td>
          426
        </td>
        <td>
          2908
        </td>
        <td>
          19711
          -
          19746
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.fieldBasedID
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          fieldBasedID({
  ((prefix: String, digestImpl: String, cols: Seq[org.apache.spark.sql.Column]) =&gt; com.sparkutils.quality.functions.`package`.hash_field_based_id(prefix, digestImpl, (cols: _*)))
})
        </td>
      </tr><tr>
        <td>
          426
        </td>
        <td>
          2907
        </td>
        <td>
          19724
          -
          19743
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashRelatedFunctionImports.hash_field_based_id
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.hash_field_based_id(prefix, digestImpl, (cols: _*))
        </td>
      </tr><tr>
        <td>
          426
        </td>
        <td>
          2910
        </td>
        <td>
          19679
          -
          19760
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(x$29, x$30, x$32, x$31)
        </td>
      </tr><tr>
        <td>
          429
        </td>
        <td>
          2911
        </td>
        <td>
          19816
          -
          19825
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size
        </td>
      </tr><tr>
        <td>
          431
        </td>
        <td>
          2912
        </td>
        <td>
          19891
          -
          19907
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.id.model.ProvidedID
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.model.ProvidedID
        </td>
      </tr><tr>
        <td>
          431
        </td>
        <td>
          2917
        </td>
        <td>
          19862
          -
          19954
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.id.GenericLongBasedIDExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.GenericLongBasedIDExpression.apply(com.sparkutils.quality.impl.id.model.ProvidedID, exps.apply(1), RuleRegistrationFunctions.this.getString(exps.head, 0))
        </td>
      </tr><tr>
        <td>
          432
        </td>
        <td>
          2915
        </td>
        <td>
          19951
          -
          19952
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          432
        </td>
        <td>
          2914
        </td>
        <td>
          19940
          -
          19949
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          432
        </td>
        <td>
          2916
        </td>
        <td>
          19930
          -
          19953
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.getString(exps.head, 0)
        </td>
      </tr><tr>
        <td>
          432
        </td>
        <td>
          2913
        </td>
        <td>
          19921
          -
          19928
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          434
        </td>
        <td>
          2918
        </td>
        <td>
          19974
          -
          19988
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.literalsNeeded
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.literalsNeeded
        </td>
      </tr><tr>
        <td>
          437
        </td>
        <td>
          2921
        </td>
        <td>
          20002
          -
          20045
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;provided_ID&quot;, providedID, scala.Predef.Set.apply[Int](2), register$default$4)
        </td>
      </tr><tr>
        <td>
          437
        </td>
        <td>
          2920
        </td>
        <td>
          20038
          -
          20044
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](2)
        </td>
      </tr><tr>
        <td>
          437
        </td>
        <td>
          2919
        </td>
        <td>
          20011
          -
          20024
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;provided_ID&quot;
        </td>
      </tr><tr>
        <td>
          440
        </td>
        <td>
          2922
        </td>
        <td>
          20109
          -
          20118
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size
        </td>
      </tr><tr>
        <td>
          442
        </td>
        <td>
          2924
        </td>
        <td>
          20188
          -
          20203
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.column
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.column(exps.apply(1))
        </td>
      </tr><tr>
        <td>
          442
        </td>
        <td>
          2927
        </td>
        <td>
          20205
          -
          20228
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.getString(exps.head, 0)
        </td>
      </tr><tr>
        <td>
          442
        </td>
        <td>
          2926
        </td>
        <td>
          20226
          -
          20227
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          442
        </td>
        <td>
          2929
        </td>
        <td>
          20155
          -
          20230
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.prefixed_to_long_pair(org.apache.spark.sql.ShimUtils.column(exps.apply(1)), RuleRegistrationFunctions.this.getString(exps.head, 0)))
        </td>
      </tr><tr>
        <td>
          442
        </td>
        <td>
          2923
        </td>
        <td>
          20195
          -
          20202
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          442
        </td>
        <td>
          2925
        </td>
        <td>
          20215
          -
          20224
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          442
        </td>
        <td>
          2928
        </td>
        <td>
          20166
          -
          20229
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.imports.LongPairImports.prefixed_to_long_pair
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.prefixed_to_long_pair(org.apache.spark.sql.ShimUtils.column(exps.apply(1)), RuleRegistrationFunctions.this.getString(exps.head, 0))
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          2930
        </td>
        <td>
          20250
          -
          20264
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.literalsNeeded
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.literalsNeeded
        </td>
      </tr><tr>
        <td>
          446
        </td>
        <td>
          2933
        </td>
        <td>
          20277
          -
          20338
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;prefixed_To_Long_Pair&quot;, prefixedToLongPair, scala.Predef.Set.apply[Int](2), register$default$4)
        </td>
      </tr><tr>
        <td>
          446
        </td>
        <td>
          2932
        </td>
        <td>
          20331
          -
          20337
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](2)
        </td>
      </tr><tr>
        <td>
          446
        </td>
        <td>
          2931
        </td>
        <td>
          20286
          -
          20309
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;prefixed_To_Long_Pair&quot;
        </td>
      </tr><tr>
        <td>
          449
        </td>
        <td>
          2936
        </td>
        <td>
          20422
          -
          20422
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$17._3
        </td>
      </tr><tr>
        <td>
          449
        </td>
        <td>
          2935
        </td>
        <td>
          20410
          -
          20410
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$17._2
        </td>
      </tr><tr>
        <td>
          449
        </td>
        <td>
          2934
        </td>
        <td>
          20396
          -
          20396
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$17._1
        </td>
      </tr><tr>
        <td>
          457
        </td>
        <td>
          2938
        </td>
        <td>
          20762
          -
          20808
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.rng_id(prefix, randomSource, seed))
        </td>
      </tr><tr>
        <td>
          457
        </td>
        <td>
          2937
        </td>
        <td>
          20773
          -
          20807
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.id.GenericLongBasedImports.rng_id
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.rng_id(prefix, randomSource, seed)
        </td>
      </tr><tr>
        <td>
          459
        </td>
        <td>
          2939
        </td>
        <td>
          20828
          -
          20836
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;rng_ID&quot;
        </td>
      </tr><tr>
        <td>
          459
        </td>
        <td>
          2941
        </td>
        <td>
          20819
          -
          20856
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;rng_ID&quot;, rngID, scala.Predef.Set.apply[Int](1, 2, 3), register$default$4)
        </td>
      </tr><tr>
        <td>
          459
        </td>
        <td>
          2940
        </td>
        <td>
          20845
          -
          20855
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1, 2, 3)
        </td>
      </tr><tr>
        <td>
          463
        </td>
        <td>
          2942
        </td>
        <td>
          20935
          -
          20944
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          2945
        </td>
        <td>
          20973
          -
          20996
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.getString(exps.head, 0)
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          2944
        </td>
        <td>
          20994
          -
          20995
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          2943
        </td>
        <td>
          20983
          -
          20992
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          465
        </td>
        <td>
          2946
        </td>
        <td>
          21017
          -
          21031
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.literalsNeeded
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.literalsNeeded
        </td>
      </tr><tr>
        <td>
          468
        </td>
        <td>
          2948
        </td>
        <td>
          21049
          -
          21078
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.unique_id(prefix))
        </td>
      </tr><tr>
        <td>
          468
        </td>
        <td>
          2947
        </td>
        <td>
          21060
          -
          21077
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.id.GuaranteedUniqueIDImports.unique_id
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.unique_id(prefix)
        </td>
      </tr><tr>
        <td>
          470
        </td>
        <td>
          2951
        </td>
        <td>
          21089
          -
          21128
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;unique_ID&quot;, uniqueID, scala.Predef.Set.apply[Int](1), register$default$4)
        </td>
      </tr><tr>
        <td>
          470
        </td>
        <td>
          2950
        </td>
        <td>
          21121
          -
          21127
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1)
        </td>
      </tr><tr>
        <td>
          470
        </td>
        <td>
          2949
        </td>
        <td>
          21098
          -
          21109
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;unique_ID&quot;
        </td>
      </tr><tr>
        <td>
          472
        </td>
        <td>
          2954
        </td>
        <td>
          21181
          -
          21198
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.column
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.column(exps.head)
        </td>
      </tr><tr>
        <td>
          472
        </td>
        <td>
          2957
        </td>
        <td>
          21202
          -
          21208
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1)
        </td>
      </tr><tr>
        <td>
          472
        </td>
        <td>
          2953
        </td>
        <td>
          21188
          -
          21197
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          472
        </td>
        <td>
          2956
        </td>
        <td>
          21162
          -
          21200
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.id_size(org.apache.spark.sql.ShimUtils.column(exps.head)))
        </td>
      </tr><tr>
        <td>
          472
        </td>
        <td>
          2958
        </td>
        <td>
          21134
          -
          21209
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;id_size&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.id_size(org.apache.spark.sql.ShimUtils.column(exps.head)))), scala.Predef.Set.apply[Int](1), register$default$4)
        </td>
      </tr><tr>
        <td>
          472
        </td>
        <td>
          2952
        </td>
        <td>
          21143
          -
          21152
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;id_size&quot;
        </td>
      </tr><tr>
        <td>
          472
        </td>
        <td>
          2955
        </td>
        <td>
          21173
          -
          21199
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.id.GuaranteedUniqueIDImports.id_size
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.id_size(org.apache.spark.sql.ShimUtils.column(exps.head))
        </td>
      </tr><tr>
        <td>
          473
        </td>
        <td>
          2963
        </td>
        <td>
          21255
          -
          21289
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.id.GuaranteedUniqueIDImports.id_base64
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.id_base64((exps.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$18: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$18)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]): _*))
        </td>
      </tr><tr>
        <td>
          473
        </td>
        <td>
          2966
        </td>
        <td>
          21214
          -
          21304
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(x$33, x$34, x$36, x$35)
        </td>
      </tr><tr>
        <td>
          473
        </td>
        <td>
          2960
        </td>
        <td>
          21274
          -
          21283
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.column
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.column(x$18)
        </td>
      </tr><tr>
        <td>
          473
        </td>
        <td>
          2962
        </td>
        <td>
          21265
          -
          21284
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$18: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$18)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column])
        </td>
      </tr><tr>
        <td>
          473
        </td>
        <td>
          2965
        </td>
        <td>
          21302
          -
          21303
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          1
        </td>
      </tr><tr>
        <td>
          473
        </td>
        <td>
          2959
        </td>
        <td>
          21223
          -
          21234
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;id_base64&quot;
        </td>
      </tr><tr>
        <td>
          473
        </td>
        <td>
          2961
        </td>
        <td>
          21273
          -
          21273
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]
        </td>
      </tr><tr>
        <td>
          473
        </td>
        <td>
          2964
        </td>
        <td>
          21244
          -
          21290
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.id_base64((exps.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$18: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$18)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]): _*)))
        </td>
      </tr><tr>
        <td>
          474
        </td>
        <td>
          2978
        </td>
        <td>
          21309
          -
          21490
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;id_from_base64&quot;, ((x0$4: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; x0$4 match {
  case scala.collection.Seq.unapplySeq[org.apache.spark.sql.catalyst.expressions.Expression](&lt;unapply-selector&gt;) &lt;unapply&gt; ((e @ _)) =&gt; org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.id_from_base64(org.apache.spark.sql.ShimUtils.column(e), com.sparkutils.quality.functions.`package`.id_from_base64$default$2))
  case scala.collection.Seq.unapplySeq[org.apache.spark.sql.catalyst.expressions.Expression](&lt;unapply-selector&gt;) &lt;unapply&gt; ((e @ _), (s @ _)) =&gt; org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.id_from_base64(org.apache.spark.sql.ShimUtils.column(e), RuleRegistrationFunctions.this.getInteger(s, RuleRegistrationFunctions.this.getInteger$default$2)))
}), scala.Predef.Set.apply[Int](1, 2), register$default$4)
        </td>
      </tr><tr>
        <td>
          474
        </td>
        <td>
          2967
        </td>
        <td>
          21318
          -
          21334
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;id_from_base64&quot;
        </td>
      </tr><tr>
        <td>
          475
        </td>
        <td>
          2969
        </td>
        <td>
          21370
          -
          21370
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.id.GuaranteedUniqueIDImports.id_from_base64$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.id_from_base64$default$2
        </td>
      </tr><tr>
        <td>
          475
        </td>
        <td>
          2968
        </td>
        <td>
          21385
          -
          21394
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.column
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.column(e)
        </td>
      </tr><tr>
        <td>
          475
        </td>
        <td>
          2971
        </td>
        <td>
          21359
          -
          21396
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.id_from_base64(org.apache.spark.sql.ShimUtils.column(e), com.sparkutils.quality.functions.`package`.id_from_base64$default$2))
        </td>
      </tr><tr>
        <td>
          475
        </td>
        <td>
          2970
        </td>
        <td>
          21370
          -
          21395
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.id.GuaranteedUniqueIDImports.id_from_base64
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.id_from_base64(org.apache.spark.sql.ShimUtils.column(e), com.sparkutils.quality.functions.`package`.id_from_base64$default$2)
        </td>
      </tr><tr>
        <td>
          476
        </td>
        <td>
          2972
        </td>
        <td>
          21447
          -
          21456
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.column
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.column(e)
        </td>
      </tr><tr>
        <td>
          476
        </td>
        <td>
          2975
        </td>
        <td>
          21432
          -
          21472
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.id.GuaranteedUniqueIDImports.id_from_base64
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.id_from_base64(org.apache.spark.sql.ShimUtils.column(e), RuleRegistrationFunctions.this.getInteger(s, RuleRegistrationFunctions.this.getInteger$default$2))
        </td>
      </tr><tr>
        <td>
          476
        </td>
        <td>
          2974
        </td>
        <td>
          21458
          -
          21471
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getInteger
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.getInteger(s, RuleRegistrationFunctions.this.getInteger$default$2)
        </td>
      </tr><tr>
        <td>
          476
        </td>
        <td>
          2976
        </td>
        <td>
          21421
          -
          21473
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.id_from_base64(org.apache.spark.sql.ShimUtils.column(e), RuleRegistrationFunctions.this.getInteger(s, RuleRegistrationFunctions.this.getInteger$default$2)))
        </td>
      </tr><tr>
        <td>
          476
        </td>
        <td>
          2973
        </td>
        <td>
          21458
          -
          21458
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getInteger$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.getInteger$default$2
        </td>
      </tr><tr>
        <td>
          477
        </td>
        <td>
          2977
        </td>
        <td>
          21481
          -
          21489
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1, 2)
        </td>
      </tr><tr>
        <td>
          478
        </td>
        <td>
          2981
        </td>
        <td>
          21550
          -
          21567
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.column
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.column(exps.head)
        </td>
      </tr><tr>
        <td>
          478
        </td>
        <td>
          2984
        </td>
        <td>
          21571
          -
          21577
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1)
        </td>
      </tr><tr>
        <td>
          478
        </td>
        <td>
          2980
        </td>
        <td>
          21557
          -
          21566
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          478
        </td>
        <td>
          2983
        </td>
        <td>
          21527
          -
          21569
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.id_raw_type(org.apache.spark.sql.ShimUtils.column(exps.head)))
        </td>
      </tr><tr>
        <td>
          478
        </td>
        <td>
          2985
        </td>
        <td>
          21495
          -
          21578
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;id_raw_type&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.id_raw_type(org.apache.spark.sql.ShimUtils.column(exps.head)))), scala.Predef.Set.apply[Int](1), register$default$4)
        </td>
      </tr><tr>
        <td>
          478
        </td>
        <td>
          2979
        </td>
        <td>
          21504
          -
          21517
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;id_raw_type&quot;
        </td>
      </tr><tr>
        <td>
          478
        </td>
        <td>
          2982
        </td>
        <td>
          21538
          -
          21568
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.id.GuaranteedUniqueIDImports.id_raw_type
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.id_raw_type(org.apache.spark.sql.ShimUtils.column(exps.head))
        </td>
      </tr><tr>
        <td>
          482
        </td>
        <td>
          2986
        </td>
        <td>
          21663
          -
          21672
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size
        </td>
      </tr><tr>
        <td>
          483
        </td>
        <td>
          2987
        </td>
        <td>
          21701
          -
          21706
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.&lt;(2)
        </td>
      </tr><tr>
        <td>
          483
        </td>
        <td>
          2988
        </td>
        <td>
          21710
          -
          21724
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.literalsNeeded
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.literalsNeeded
        </td>
      </tr><tr>
        <td>
          484
        </td>
        <td>
          2990
        </td>
        <td>
          21766
          -
          21767
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          484
        </td>
        <td>
          2989
        </td>
        <td>
          21755
          -
          21764
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          484
        </td>
        <td>
          2991
        </td>
        <td>
          21745
          -
          21768
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.getString(exps.head, 0)
        </td>
      </tr><tr>
        <td>
          486
        </td>
        <td>
          2998
        </td>
        <td>
          21785
          -
          21934
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.id.GenericLongBasedIDExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.GenericLongBasedIDExpression.apply(com.sparkutils.quality.impl.id.model.FieldBasedID, com.sparkutils.quality.impl.hash.HashFunctionsExpression.apply(exps.tail, &quot;IGNORED&quot;, true, com.sparkutils.quality.impl.hash.HashFunctionFactory.apply(&quot;IGNORED&quot;)), prefix)
        </td>
      </tr><tr>
        <td>
          486
        </td>
        <td>
          2992
        </td>
        <td>
          21814
          -
          21832
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.id.model.FieldBasedID
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.model.FieldBasedID
        </td>
      </tr><tr>
        <td>
          487
        </td>
        <td>
          2993
        </td>
        <td>
          21866
          -
          21875
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableLike.tail
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.tail
        </td>
      </tr><tr>
        <td>
          487
        </td>
        <td>
          2996
        </td>
        <td>
          21894
          -
          21924
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashFunctionFactory.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.hash.HashFunctionFactory.apply(&quot;IGNORED&quot;)
        </td>
      </tr><tr>
        <td>
          487
        </td>
        <td>
          2995
        </td>
        <td>
          21888
          -
          21892
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          487
        </td>
        <td>
          2994
        </td>
        <td>
          21877
          -
          21886
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;IGNORED&quot;
        </td>
      </tr><tr>
        <td>
          487
        </td>
        <td>
          2997
        </td>
        <td>
          21842
          -
          21925
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashFunctionsExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.hash.HashFunctionsExpression.apply(exps.tail, &quot;IGNORED&quot;, true, com.sparkutils.quality.impl.hash.HashFunctionFactory.apply(&quot;IGNORED&quot;))
        </td>
      </tr><tr>
        <td>
          489
        </td>
        <td>
          2999
        </td>
        <td>
          21954
          -
          21966
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;murmur3_ID&quot;
        </td>
      </tr><tr>
        <td>
          489
        </td>
        <td>
          3001
        </td>
        <td>
          21945
          -
          21996
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(x$37, x$38, x$40, x$39)
        </td>
      </tr><tr>
        <td>
          489
        </td>
        <td>
          3000
        </td>
        <td>
          21994
          -
          21995
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          2
        </td>
      </tr><tr>
        <td>
          491
        </td>
        <td>
          3008
        </td>
        <td>
          22078
          -
          22102
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$19: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$19)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column])
        </td>
      </tr><tr>
        <td>
          491
        </td>
        <td>
          3011
        </td>
        <td>
          22120
          -
          22121
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          2
        </td>
      </tr><tr>
        <td>
          491
        </td>
        <td>
          3002
        </td>
        <td>
          22011
          -
          22022
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;hash_With&quot;
        </td>
      </tr><tr>
        <td>
          491
        </td>
        <td>
          3005
        </td>
        <td>
          22053
          -
          22076
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.getString(exps.head, 0)
        </td>
      </tr><tr>
        <td>
          491
        </td>
        <td>
          3004
        </td>
        <td>
          22074
          -
          22075
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          491
        </td>
        <td>
          3007
        </td>
        <td>
          22091
          -
          22091
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]
        </td>
      </tr><tr>
        <td>
          491
        </td>
        <td>
          3010
        </td>
        <td>
          22032
          -
          22108
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.hash_with(RuleRegistrationFunctions.this.getString(exps.head, 0), (exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$19: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$19)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]): _*)))
        </td>
      </tr><tr>
        <td>
          491
        </td>
        <td>
          3009
        </td>
        <td>
          22043
          -
          22107
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashRelatedFunctionImports.hash_with
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.hash_with(RuleRegistrationFunctions.this.getString(exps.head, 0), (exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$19: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$19)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]): _*))
        </td>
      </tr><tr>
        <td>
          491
        </td>
        <td>
          3003
        </td>
        <td>
          22063
          -
          22072
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          491
        </td>
        <td>
          3012
        </td>
        <td>
          22002
          -
          22122
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(x$41, x$42, x$44, x$43)
        </td>
      </tr><tr>
        <td>
          491
        </td>
        <td>
          3006
        </td>
        <td>
          22092
          -
          22101
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.column
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.column(x$19)
        </td>
      </tr><tr>
        <td>
          492
        </td>
        <td>
          3017
        </td>
        <td>
          22231
          -
          22240
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.column
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.column(x$20)
        </td>
      </tr><tr>
        <td>
          492
        </td>
        <td>
          3020
        </td>
        <td>
          22175
          -
          22246
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashRelatedFunctionImports.hash_with_struct
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.hash_with_struct(RuleRegistrationFunctions.this.getString(exps.head, 0), (exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$20: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$20)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]): _*))
        </td>
      </tr><tr>
        <td>
          492
        </td>
        <td>
          3022
        </td>
        <td>
          22259
          -
          22260
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          2
        </td>
      </tr><tr>
        <td>
          492
        </td>
        <td>
          3013
        </td>
        <td>
          22136
          -
          22154
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;hash_With_Struct&quot;
        </td>
      </tr><tr>
        <td>
          492
        </td>
        <td>
          3016
        </td>
        <td>
          22192
          -
          22215
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.getString(exps.head, 0)
        </td>
      </tr><tr>
        <td>
          492
        </td>
        <td>
          3019
        </td>
        <td>
          22217
          -
          22241
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$20: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$20)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column])
        </td>
      </tr><tr>
        <td>
          492
        </td>
        <td>
          3018
        </td>
        <td>
          22230
          -
          22230
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]
        </td>
      </tr><tr>
        <td>
          492
        </td>
        <td>
          3021
        </td>
        <td>
          22164
          -
          22247
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.hash_with_struct(RuleRegistrationFunctions.this.getString(exps.head, 0), (exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$20: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$20)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]): _*)))
        </td>
      </tr><tr>
        <td>
          492
        </td>
        <td>
          3015
        </td>
        <td>
          22213
          -
          22214
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          492
        </td>
        <td>
          3023
        </td>
        <td>
          22127
          -
          22261
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(x$45, x$46, x$48, x$47)
        </td>
      </tr><tr>
        <td>
          492
        </td>
        <td>
          3014
        </td>
        <td>
          22202
          -
          22211
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          494
        </td>
        <td>
          3026
        </td>
        <td>
          22345
          -
          22346
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          494
        </td>
        <td>
          3029
        </td>
        <td>
          22362
          -
          22362
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]
        </td>
      </tr><tr>
        <td>
          494
        </td>
        <td>
          3028
        </td>
        <td>
          22363
          -
          22372
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.column
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.column(x$21)
        </td>
      </tr><tr>
        <td>
          494
        </td>
        <td>
          3031
        </td>
        <td>
          22311
          -
          22378
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashRelatedFunctionImports.za_hash_with
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.za_hash_with(RuleRegistrationFunctions.this.getString(exps.head, 0), (exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$21: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$21)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]): _*))
        </td>
      </tr><tr>
        <td>
          494
        </td>
        <td>
          3025
        </td>
        <td>
          22334
          -
          22343
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          494
        </td>
        <td>
          3034
        </td>
        <td>
          22267
          -
          22393
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(x$49, x$50, x$52, x$51)
        </td>
      </tr><tr>
        <td>
          494
        </td>
        <td>
          3027
        </td>
        <td>
          22324
          -
          22347
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.getString(exps.head, 0)
        </td>
      </tr><tr>
        <td>
          494
        </td>
        <td>
          3030
        </td>
        <td>
          22349
          -
          22373
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$21: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$21)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column])
        </td>
      </tr><tr>
        <td>
          494
        </td>
        <td>
          3024
        </td>
        <td>
          22276
          -
          22290
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;za_Hash_With&quot;
        </td>
      </tr><tr>
        <td>
          494
        </td>
        <td>
          3033
        </td>
        <td>
          22391
          -
          22392
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          2
        </td>
      </tr><tr>
        <td>
          494
        </td>
        <td>
          3032
        </td>
        <td>
          22300
          -
          22379
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.za_hash_with(RuleRegistrationFunctions.this.getString(exps.head, 0), (exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$21: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$21)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]): _*)))
        </td>
      </tr><tr>
        <td>
          495
        </td>
        <td>
          3035
        </td>
        <td>
          22444
          -
          22465
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;za_Hash_With_Struct&quot;
        </td>
      </tr><tr>
        <td>
          495
        </td>
        <td>
          3044
        </td>
        <td>
          22573
          -
          22574
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          2
        </td>
      </tr><tr>
        <td>
          495
        </td>
        <td>
          3038
        </td>
        <td>
          22506
          -
          22529
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.getString(exps.head, 0)
        </td>
      </tr><tr>
        <td>
          495
        </td>
        <td>
          3037
        </td>
        <td>
          22526
          -
          22527
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          495
        </td>
        <td>
          3040
        </td>
        <td>
          22544
          -
          22544
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]
        </td>
      </tr><tr>
        <td>
          495
        </td>
        <td>
          3043
        </td>
        <td>
          22475
          -
          22561
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.za_hash_with_struct(RuleRegistrationFunctions.this.getString(exps.head, 0), (exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$22: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$22)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]): _*)))
        </td>
      </tr><tr>
        <td>
          495
        </td>
        <td>
          3045
        </td>
        <td>
          22435
          -
          22575
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(x$53, x$54, x$56, x$55)
        </td>
      </tr><tr>
        <td>
          495
        </td>
        <td>
          3036
        </td>
        <td>
          22516
          -
          22525
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          495
        </td>
        <td>
          3039
        </td>
        <td>
          22545
          -
          22554
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.column
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.column(x$22)
        </td>
      </tr><tr>
        <td>
          495
        </td>
        <td>
          3042
        </td>
        <td>
          22486
          -
          22560
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashRelatedFunctionImports.za_hash_with_struct
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.za_hash_with_struct(RuleRegistrationFunctions.this.getString(exps.head, 0), (exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$22: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$22)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]): _*))
        </td>
      </tr><tr>
        <td>
          495
        </td>
        <td>
          3041
        </td>
        <td>
          22531
          -
          22555
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$22: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$22)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column])
        </td>
      </tr><tr>
        <td>
          497
        </td>
        <td>
          3053
        </td>
        <td>
          22668
          -
          22741
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashRelatedFunctionImports.za_hash_longs_with
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.za_hash_longs_with(RuleRegistrationFunctions.this.getString(exps.head, 0), (exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$23: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$23)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]): _*))
        </td>
      </tr><tr>
        <td>
          497
        </td>
        <td>
          3056
        </td>
        <td>
          22618
          -
          22756
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(x$57, x$58, x$60, x$59)
        </td>
      </tr><tr>
        <td>
          497
        </td>
        <td>
          3047
        </td>
        <td>
          22697
          -
          22706
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          497
        </td>
        <td>
          3046
        </td>
        <td>
          22627
          -
          22647
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;za_Hash_Longs_With&quot;
        </td>
      </tr><tr>
        <td>
          497
        </td>
        <td>
          3055
        </td>
        <td>
          22754
          -
          22755
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          2
        </td>
      </tr><tr>
        <td>
          497
        </td>
        <td>
          3049
        </td>
        <td>
          22687
          -
          22710
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.getString(exps.head, 0)
        </td>
      </tr><tr>
        <td>
          497
        </td>
        <td>
          3052
        </td>
        <td>
          22712
          -
          22736
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$23: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$23)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column])
        </td>
      </tr><tr>
        <td>
          497
        </td>
        <td>
          3051
        </td>
        <td>
          22725
          -
          22725
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]
        </td>
      </tr><tr>
        <td>
          497
        </td>
        <td>
          3054
        </td>
        <td>
          22657
          -
          22742
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.za_hash_longs_with(RuleRegistrationFunctions.this.getString(exps.head, 0), (exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$23: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$23)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]): _*)))
        </td>
      </tr><tr>
        <td>
          497
        </td>
        <td>
          3048
        </td>
        <td>
          22708
          -
          22709
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          497
        </td>
        <td>
          3050
        </td>
        <td>
          22726
          -
          22735
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.column
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.column(x$23)
        </td>
      </tr><tr>
        <td>
          498
        </td>
        <td>
          3062
        </td>
        <td>
          22882
          -
          22882
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]
        </td>
      </tr><tr>
        <td>
          498
        </td>
        <td>
          3065
        </td>
        <td>
          22807
          -
          22899
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.za_hash_longs_with_struct(RuleRegistrationFunctions.this.getString(exps.head, 0), (exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$24: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$24)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]): _*)))
        </td>
      </tr><tr>
        <td>
          498
        </td>
        <td>
          3064
        </td>
        <td>
          22818
          -
          22898
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashRelatedFunctionImports.za_hash_longs_with_struct
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.za_hash_longs_with_struct(RuleRegistrationFunctions.this.getString(exps.head, 0), (exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$24: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$24)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column]): _*))
        </td>
      </tr><tr>
        <td>
          498
        </td>
        <td>
          3058
        </td>
        <td>
          22854
          -
          22863
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          498
        </td>
        <td>
          3067
        </td>
        <td>
          22761
          -
          22913
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(x$61, x$62, x$64, x$63)
        </td>
      </tr><tr>
        <td>
          498
        </td>
        <td>
          3061
        </td>
        <td>
          22883
          -
          22892
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.column
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.column(x$24)
        </td>
      </tr><tr>
        <td>
          498
        </td>
        <td>
          3060
        </td>
        <td>
          22844
          -
          22867
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.getString(exps.head, 0)
        </td>
      </tr><tr>
        <td>
          498
        </td>
        <td>
          3063
        </td>
        <td>
          22869
          -
          22893
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.tail.map[org.apache.spark.sql.Column, Seq[org.apache.spark.sql.Column]](((x$24: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.ShimUtils.column(x$24)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.Column])
        </td>
      </tr><tr>
        <td>
          498
        </td>
        <td>
          3066
        </td>
        <td>
          22911
          -
          22912
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          2
        </td>
      </tr><tr>
        <td>
          498
        </td>
        <td>
          3057
        </td>
        <td>
          22770
          -
          22797
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;za_Hash_Longs_With_Struct&quot;
        </td>
      </tr><tr>
        <td>
          498
        </td>
        <td>
          3059
        </td>
        <td>
          22865
          -
          22866
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          501
        </td>
        <td>
          3071
        </td>
        <td>
          23035
          -
          23093
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.QualityException.qualityException(&quot;coalesceIf functions cannot be created&quot;, com.sparkutils.quality.QualityException.qualityException$default$2)
        </td>
      </tr><tr>
        <td>
          501
        </td>
        <td>
          3070
        </td>
        <td>
          23035
          -
          23035
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
      </tr><tr>
        <td>
          501
        </td>
        <td>
          3069
        </td>
        <td>
          23052
          -
          23092
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;coalesceIf functions cannot be created&quot;
        </td>
      </tr><tr>
        <td>
          501
        </td>
        <td>
          3072
        </td>
        <td>
          22987
          -
          23095
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;coalesce_If_Attributes_Missing&quot;, ((x$25: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.QualityException.qualityException(&quot;coalesceIf functions cannot be created&quot;, com.sparkutils.quality.QualityException.qualityException$default$2)), register$default$3, register$default$4)
        </td>
      </tr><tr>
        <td>
          501
        </td>
        <td>
          3068
        </td>
        <td>
          22996
          -
          23028
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;coalesce_If_Attributes_Missing&quot;
        </td>
      </tr><tr>
        <td>
          502
        </td>
        <td>
          3073
        </td>
        <td>
          23109
          -
          23149
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;coalesce_If_Attributes_Missing_Disable&quot;
        </td>
      </tr><tr>
        <td>
          502
        </td>
        <td>
          3076
        </td>
        <td>
          23156
          -
          23214
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.QualityException.qualityException(&quot;coalesceIf functions cannot be created&quot;, com.sparkutils.quality.QualityException.qualityException$default$2)
        </td>
      </tr><tr>
        <td>
          502
        </td>
        <td>
          3075
        </td>
        <td>
          23156
          -
          23156
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
      </tr><tr>
        <td>
          502
        </td>
        <td>
          3074
        </td>
        <td>
          23173
          -
          23213
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;coalesceIf functions cannot be created&quot;
        </td>
      </tr><tr>
        <td>
          502
        </td>
        <td>
          3077
        </td>
        <td>
          23100
          -
          23216
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;coalesce_If_Attributes_Missing_Disable&quot;, ((x$26: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.QualityException.qualityException(&quot;coalesceIf functions cannot be created&quot;, com.sparkutils.quality.QualityException.qualityException$default$2)), register$default$3, register$default$4)
        </td>
      </tr><tr>
        <td>
          505
        </td>
        <td>
          3078
        </td>
        <td>
          23325
          -
          23339
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;update_field&quot;
        </td>
      </tr><tr>
        <td>
          505
        </td>
        <td>
          3092
        </td>
        <td>
          23316
          -
          23504
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(x$65, x$66, x$68, x$67)
        </td>
      </tr><tr>
        <td>
          506
        </td>
        <td>
          3080
        </td>
        <td>
          23381
          -
          23398
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.column
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.column(exps.head)
        </td>
      </tr><tr>
        <td>
          506
        </td>
        <td>
          3089
        </td>
        <td>
          23368
          -
          23483
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.StructFunctionsImport.update_field
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.update_field(org.apache.spark.sql.ShimUtils.column(exps.head), (exps.tail.grouped(2).map[(String, org.apache.spark.sql.Column)](((p: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; scala.Predef.ArrowAssoc[String](RuleRegistrationFunctions.this.getString(p.head, 0)).-&gt;[org.apache.spark.sql.Column](org.apache.spark.sql.ShimUtils.column(p.last)))).toSeq: _*))
        </td>
      </tr><tr>
        <td>
          506
        </td>
        <td>
          3088
        </td>
        <td>
          23402
          -
          23477
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableOnce.toSeq
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.tail.grouped(2).map[(String, org.apache.spark.sql.Column)](((p: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; scala.Predef.ArrowAssoc[String](RuleRegistrationFunctions.this.getString(p.head, 0)).-&gt;[org.apache.spark.sql.Column](org.apache.spark.sql.ShimUtils.column(p.last)))).toSeq
        </td>
      </tr><tr>
        <td>
          506
        </td>
        <td>
          3082
        </td>
        <td>
          23442
          -
          23448
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          p.head
        </td>
      </tr><tr>
        <td>
          506
        </td>
        <td>
          3085
        </td>
        <td>
          23463
          -
          23469
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableLike.last
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          p.last
        </td>
      </tr><tr>
        <td>
          506
        </td>
        <td>
          3079
        </td>
        <td>
          23388
          -
          23397
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          506
        </td>
        <td>
          3087
        </td>
        <td>
          23432
          -
          23470
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Predef.ArrowAssoc.-&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.ArrowAssoc[String](RuleRegistrationFunctions.this.getString(p.head, 0)).-&gt;[org.apache.spark.sql.Column](org.apache.spark.sql.ShimUtils.column(p.last))
        </td>
      </tr><tr>
        <td>
          506
        </td>
        <td>
          3090
        </td>
        <td>
          23357
          -
          23484
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.update_field(org.apache.spark.sql.ShimUtils.column(exps.head), (exps.tail.grouped(2).map[(String, org.apache.spark.sql.Column)](((p: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; scala.Predef.ArrowAssoc[String](RuleRegistrationFunctions.this.getString(p.head, 0)).-&gt;[org.apache.spark.sql.Column](org.apache.spark.sql.ShimUtils.column(p.last)))).toSeq: _*)))
        </td>
      </tr><tr>
        <td>
          506
        </td>
        <td>
          3081
        </td>
        <td>
          23420
          -
          23421
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          2
        </td>
      </tr><tr>
        <td>
          506
        </td>
        <td>
          3084
        </td>
        <td>
          23432
          -
          23452
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.getString(p.head, 0)
        </td>
      </tr><tr>
        <td>
          506
        </td>
        <td>
          3083
        </td>
        <td>
          23450
          -
          23451
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          506
        </td>
        <td>
          3086
        </td>
        <td>
          23456
          -
          23470
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.column
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.column(p.last)
        </td>
      </tr><tr>
        <td>
          507
        </td>
        <td>
          3091
        </td>
        <td>
          23502
          -
          23503
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          3
        </td>
      </tr><tr>
        <td>
          508
        </td>
        <td>
          3093
        </td>
        <td>
          23518
          -
          23530
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;drop_field&quot;
        </td>
      </tr><tr>
        <td>
          508
        </td>
        <td>
          3104
        </td>
        <td>
          23509
          -
          23676
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(x$69, x$70, x$72, x$71)
        </td>
      </tr><tr>
        <td>
          509
        </td>
        <td>
          3098
        </td>
        <td>
          23631
          -
          23648
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRegistrationFunctions.this.getString(p, i.+(1))
        </td>
      </tr><tr>
        <td>
          509
        </td>
        <td>
          3097
        </td>
        <td>
          23644
          -
          23647
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.+(1)
        </td>
      </tr><tr>
        <td>
          509
        </td>
        <td>
          3100
        </td>
        <td>
          23589
          -
          23649
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.tail.zipWithIndex[org.apache.spark.sql.catalyst.expressions.Expression, Seq[(org.apache.spark.sql.catalyst.expressions.Expression, Int)]](collection.this.Seq.canBuildFrom[(org.apache.spark.sql.catalyst.expressions.Expression, Int)]).map[String, Seq[String]](((x0$5: (org.apache.spark.sql.catalyst.expressions.Expression, Int)) =&gt; x0$5 match {
  case (_1: org.apache.spark.sql.catalyst.expressions.Expression, _2: Int)(org.apache.spark.sql.catalyst.expressions.Expression, Int)((p @ _), (i @ _)) =&gt; RuleRegistrationFunctions.this.getString(p, i.+(1))
}))(collection.this.Seq.canBuildFrom[String])
        </td>
      </tr><tr>
        <td>
          509
        </td>
        <td>
          3094
        </td>
        <td>
          23577
          -
          23586
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          509
        </td>
        <td>
          3096
        </td>
        <td>
          23599
          -
          23599
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[(org.apache.spark.sql.catalyst.expressions.Expression, Int)]
        </td>
      </tr><tr>
        <td>
          509
        </td>
        <td>
          3099
        </td>
        <td>
          23615
          -
          23615
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[String]
        </td>
      </tr><tr>
        <td>
          509
        </td>
        <td>
          3102
        </td>
        <td>
          23548
          -
          23656
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expression(com.sparkutils.quality.functions.`package`.drop_field(org.apache.spark.sql.ShimUtils.column(exps.head), (exps.tail.zipWithIndex[org.apache.spark.sql.catalyst.expressions.Expression, Seq[(org.apache.spark.sql.catalyst.expressions.Expression, Int)]](collection.this.Seq.canBuildFrom[(org.apache.spark.sql.catalyst.expressions.Expression, Int)]).map[String, Seq[String]](((x0$5: (org.apache.spark.sql.catalyst.expressions.Expression, Int)) =&gt; x0$5 match {
  case (_1: org.apache.spark.sql.catalyst.expressions.Expression, _2: Int)(org.apache.spark.sql.catalyst.expressions.Expression, Int)((p @ _), (i @ _)) =&gt; RuleRegistrationFunctions.this.getString(p, i.+(1))
}))(collection.this.Seq.canBuildFrom[String]): _*)))
        </td>
      </tr><tr>
        <td>
          509
        </td>
        <td>
          3101
        </td>
        <td>
          23559
          -
          23655
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.StructFunctionsImport.drop_field
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.functions.`package`.drop_field(org.apache.spark.sql.ShimUtils.column(exps.head), (exps.tail.zipWithIndex[org.apache.spark.sql.catalyst.expressions.Expression, Seq[(org.apache.spark.sql.catalyst.expressions.Expression, Int)]](collection.this.Seq.canBuildFrom[(org.apache.spark.sql.catalyst.expressions.Expression, Int)]).map[String, Seq[String]](((x0$5: (org.apache.spark.sql.catalyst.expressions.Expression, Int)) =&gt; x0$5 match {
  case (_1: org.apache.spark.sql.catalyst.expressions.Expression, _2: Int)(org.apache.spark.sql.catalyst.expressions.Expression, Int)((p @ _), (i @ _)) =&gt; RuleRegistrationFunctions.this.getString(p, i.+(1))
}))(collection.this.Seq.canBuildFrom[String]): _*))
        </td>
      </tr><tr>
        <td>
          509
        </td>
        <td>
          3095
        </td>
        <td>
          23570
          -
          23587
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.column
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.column(exps.head)
        </td>
      </tr><tr>
        <td>
          510
        </td>
        <td>
          3103
        </td>
        <td>
          23674
          -
          23675
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          2
        </td>
      </tr><tr>
        <td>
          514
        </td>
        <td>
          3106
        </td>
        <td>
          23834
          -
          23851
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[String, org.apache.spark.sql.catalyst.expressions.Expression](str.toString(), e)
        </td>
      </tr><tr>
        <td>
          514
        </td>
        <td>
          3105
        </td>
        <td>
          23835
          -
          23847
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.unsafe.types.UTF8String.toString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          str.toString()
        </td>
      </tr><tr>
        <td>
          516
        </td>
        <td>
          3107
        </td>
        <td>
          23893
          -
          23908
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[String, org.apache.spark.sql.catalyst.expressions.Expression](msgDefault, e)
        </td>
      </tr><tr>
        <td>
          519
        </td>
        <td>
          3108
        </td>
        <td>
          23929
          -
          23941
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;print_Code&quot;
        </td>
      </tr><tr>
        <td>
          519
        </td>
        <td>
          3113
        </td>
        <td>
          23920
          -
          24087
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;print_Code&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; {
  &lt;synthetic&gt; &lt;artifact&gt; private[this] val x$27: (String, org.apache.spark.sql.catalyst.expressions.Expression) = (msgAndExpr(com.sparkutils.quality.impl.util.PrintCode.apply(exps.apply(0), com.sparkutils.quality.impl.util.PrintCode.apply$default$2, com.sparkutils.quality.impl.util.PrintCode.apply$default$3).msg, exps): (String, org.apache.spark.sql.catalyst.expressions.Expression) @unchecked) match {
    case (_1: String, _2: org.apache.spark.sql.catalyst.expressions.Expression)(String, org.apache.spark.sql.catalyst.expressions.Expression)((msg @ _), (exp @ _)) =&gt; scala.Tuple2.apply[String, org.apache.spark.sql.catalyst.expressions.Expression](msg, exp)
  };
  val msg: String = x$27._1;
  val exp: org.apache.spark.sql.catalyst.expressions.Expression = x$27._2;
  com.sparkutils.quality.impl.util.PrintCode.apply(exp, msg, writer)
}), scala.Predef.Set.apply[Int](1, 2), register$default$4)
        </td>
      </tr><tr>
        <td>
          520
        </td>
        <td>
          3109
        </td>
        <td>
          23983
          -
          23983
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$27._1
        </td>
      </tr><tr>
        <td>
          520
        </td>
        <td>
          3110
        </td>
        <td>
          23988
          -
          23988
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$27._2
        </td>
      </tr><tr>
        <td>
          521
        </td>
        <td>
          3111
        </td>
        <td>
          24042
          -
          24069
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.PrintCode.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.util.PrintCode.apply(exp, msg, writer)
        </td>
      </tr><tr>
        <td>
          522
        </td>
        <td>
          3112
        </td>
        <td>
          24077
          -
          24086
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1, 2)
        </td>
      </tr><tr>
        <td>
          523
        </td>
        <td>
          3125
        </td>
        <td>
          24092
          -
          24284
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRegistrationFunctions.register
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register(&quot;print_Expr&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; {
  &lt;synthetic&gt; &lt;artifact&gt; private[this] val x$28: (String, org.apache.spark.sql.catalyst.expressions.Expression) = (msgAndExpr(&quot;Expression toStr is -&gt;&quot;, exps): (String, org.apache.spark.sql.catalyst.expressions.Expression) @unchecked) match {
    case (_1: String, _2: org.apache.spark.sql.catalyst.expressions.Expression)(String, org.apache.spark.sql.catalyst.expressions.Expression)((msg @ _), (exp @ _)) =&gt; scala.Tuple2.apply[String, org.apache.spark.sql.catalyst.expressions.Expression](msg, exp)
  };
  val msg: String = x$28._1;
  val exp: org.apache.spark.sql.catalyst.expressions.Expression = x$28._2;
  writer.apply(scala.StringContext.apply(&quot;&quot;, &quot; &quot;, &quot; .  Sql is &quot;, &quot;&quot;).s(msg, exp, exp.sql));
  exp
}), scala.Predef.Set.apply[Int](1, 2), register$default$4)
        </td>
      </tr><tr>
        <td>
          523
        </td>
        <td>
          3114
        </td>
        <td>
          24101
          -
          24113
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;print_Expr&quot;
        </td>
      </tr><tr>
        <td>
          524
        </td>
        <td>
          3116
        </td>
        <td>
          24160
          -
          24160
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$28._2
        </td>
      </tr><tr>
        <td>
          524
        </td>
        <td>
          3115
        </td>
        <td>
          24155
          -
          24155
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$28._1
        </td>
      </tr><tr>
        <td>
          525
        </td>
        <td>
          3118
        </td>
        <td>
          24229
          -
          24231
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot; &quot;
        </td>
      </tr><tr>
        <td>
          525
        </td>
        <td>
          3121
        </td>
        <td>
          24247
          -
          24254
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.sql
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exp.sql
        </td>
      </tr><tr>
        <td>
          525
        </td>
        <td>
          3120
        </td>
        <td>
          24255
          -
          24256
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          525
        </td>
        <td>
          3123
        </td>
        <td>
          24216
          -
          24257
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function1.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          writer.apply(scala.StringContext.apply(&quot;&quot;, &quot; &quot;, &quot; .  Sql is &quot;, &quot;&quot;).s(msg, exp, exp.sql))
        </td>
      </tr><tr>
        <td>
          525
        </td>
        <td>
          3117
        </td>
        <td>
          24225
          -
          24226
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          525
        </td>
        <td>
          3119
        </td>
        <td>
          24234
          -
          24246
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot; .  Sql is &quot;
        </td>
      </tr><tr>
        <td>
          525
        </td>
        <td>
          3122
        </td>
        <td>
          24223
          -
          24256
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;&quot;, &quot; &quot;, &quot; .  Sql is &quot;, &quot;&quot;).s(msg, exp, exp.sql)
        </td>
      </tr><tr>
        <td>
          527
        </td>
        <td>
          3124
        </td>
        <td>
          24275
          -
          24283
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[Int](1, 2)
        </td>
      </tr>
    </table>
          </div>
        </div>
      </body>
    </html>