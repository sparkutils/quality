<html>
      <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <title id="title">
          com/sparkutils/quality/impl/util/Utils.scala.html
        </title>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/css/theme.default.min.css" type="text/css"/><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/js/jquery.tablesorter.min.js"></script><link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css" type="text/css"/><script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script><script type="text/javascript">
        $(document).ready(function() {$(".tablesorter").tablesorter();});
      </script>
        <style>
          table.codegrid { font-family: monospace; font-size: 12px; width: auto!important; }table.statementlist { width: auto!important; font-size: 13px; } table.codegrid td { padding: 0!important; border: 0!important } table td.linenumber { width: 40px!important; } 
        </style>
      </head>
      <body style="font-family: monospace;">
        <ul class="nav nav-tabs">
          <li>
            <a href="#codegrid" data-toggle="tab">Codegrid</a>
          </li>
          <li>
            <a href="#statementlist" data-toggle="tab">Statement List</a>
          </li>
        </ul>
        <div class="tab-content">
          <div class="tab-pane active" id="codegrid">
            <pre style='font-size: 12pt; font-family: courier, monospace;'>1 <span style=''>package com.sparkutils.quality.impl.util
</span>2 <span style=''>
</span>3 <span style=''>import com.sparkutils.quality._
</span>4 <span style=''>import com.sparkutils.quality.impl.RuleLogicUtils
</span>5 <span style=''>import com.sparkutils.shim.expressions.{CreateNamedStruct1, GetStructField3}
</span>6 <span style=''>import frameless.TypedEncoder
</span>7 <span style=''>import org.apache.spark.sql.catalyst.InternalRow
</span>8 <span style=''>import org.apache.spark.sql.catalyst.expressions.codegen.{CodegenContext, CodegenFallback, ExprCode, QualityExprUtils, ExprValue, JavaCode, VariableValue}
</span>9 <span style=''>import org.apache.spark.sql.catalyst.expressions.{Alias, BoundReference, Expression, If, IsNull, Literal, NamedExpression, Unevaluable, UnsafeArrayData}
</span>10 <span style=''>import org.apache.spark.sql.catalyst.util.ArrayData
</span>11 <span style=''>import org.apache.spark.sql.types.{BooleanType, DataType, StructField, StructType}
</span>12 <span style=''>
</span>13 <span style=''>import java.util.concurrent.atomic.AtomicBoolean
</span>14 <span style=''>import org.apache.spark.internal.Logging
</span>15 <span style=''>import org.apache.spark.sql.catalyst.analysis.{GetColumnByOrdinal, UnresolvedAttribute}
</span>16 <span style=''>import org.apache.spark.sql.{Encoder, ShimUtils, SparkSession}
</span>17 <span style=''>import org.apache.spark.sql.catalyst.expressions.codegen.Block.BlockHelper
</span>18 <span style=''>import org.apache.spark.sql.catalyst.expressions.objects.{InitializeJavaBean, Invoke, MapObjects, NewInstance, UnresolvedMapObjects}
</span>19 <span style=''>
</span>20 <span style=''>import scala.reflect.ClassTag
</span>21 <span style=''>
</span>22 <span style=''>object DebugTime extends Logging {
</span>23 <span style=''>
</span>24 <span style=''>  def debugTime[T](what: String, log: (Long, String)=&gt;Unit = (i, what) =&gt; {logDebug(s&quot;----&gt; ${i}ms for $what&quot;)} )( thunk: =&gt; T): T = {
</span>25 <span style=''>    val start = </span><span style='background: #AEF1AE'>System.currentTimeMillis</span><span style=''>
</span>26 <span style=''>    try {
</span>27 <span style=''>      </span><span style='background: #AEF1AE'>thunk</span><span style=''>
</span>28 <span style=''>    } finally </span><span style='background: #AEF1AE'>{
</span>29 <span style=''></span><span style='background: #AEF1AE'>      val stop = System.currentTimeMillis
</span>30 <span style=''></span><span style='background: #AEF1AE'>
</span>31 <span style=''></span><span style='background: #AEF1AE'>      log(stop - start, what)
</span>32 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>33 <span style=''>  }
</span>34 <span style=''>
</span>35 <span style=''>}
</span>36 <span style=''>
</span>37 <span style=''>trait PassThrough extends Expression {
</span>38 <span style=''>  override def nullable: Boolean = </span><span style='background: #F0ADAD'>true</span><span style=''>
</span>39 <span style=''>
</span>40 <span style=''>  override def eval(input: InternalRow): Any = </span><span style='background: #AEF1AE'>Literal(true).eval(input)</span><span style=''>
</span>41 <span style=''>
</span>42 <span style=''>  override def dataType: DataType = </span><span style='background: #AEF1AE'>BooleanType</span><span style=''>
</span>43 <span style=''>
</span>44 <span style=''>  // TODO #21 - migrate to withNewChildren when 2.4 is dropped
</span>45 <span style=''>  def withNewChilds(newChildren: IndexedSeq[Expression]): Expression
</span>46 <span style=''>}
</span>47 <span style=''>
</span>48 <span style=''>/**
</span>49 <span style=''> * Same as unevaluable but the queryplan runs.  This version requires compileEvals = true (rules are independent and
</span>50 <span style=''> * will not use Subexpression Elimination at eval time) and as such cannot be used with SubExprEvaluationRuntime
</span>51 <span style=''> * @param children
</span>52 <span style=''> */
</span>53 <span style=''>case class PassThroughCompileEvals(children: Seq[Expression]) extends PassThrough with CodegenFallback {
</span>54 <span style=''>
</span>55 <span style=''>  protected def withNewChildrenInternal(newChildren: IndexedSeq[Expression]): Expression = </span><span style='background: #AEF1AE'>copy(children = newChildren)</span><span style=''>
</span>56 <span style=''>
</span>57 <span style=''>  override def withNewChilds(newChildren: IndexedSeq[Expression]): Expression = </span><span style='background: #AEF1AE'>copy(children = newChildren)</span><span style=''>
</span>58 <span style=''>}
</span>59 <span style=''>
</span>60 <span style=''>/**
</span>61 <span style=''> * Same as unevaluable but the queryplan runs.  This version should only be used for eval (compileEvals = false) of
</span>62 <span style=''> * rules / triggers and for any output expressions, it may take part in SubExprEvaluationRuntime
</span>63 <span style=''> * @param children
</span>64 <span style=''> */
</span>65 <span style=''>case class PassThroughEvalOnly(children: Seq[Expression]) extends PassThrough {
</span>66 <span style=''>  protected def withNewChildrenInternal(newChildren: IndexedSeq[Expression]): Expression = </span><span style='background: #AEF1AE'>copy(children = newChildren)</span><span style=''>
</span>67 <span style=''>
</span>68 <span style=''>  protected def doGenCode(ctx: org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext, ev: org.apache.spark.sql.catalyst.expressions.codegen.ExprCode): org.apache.spark.sql.catalyst.expressions.codegen.ExprCode = </span><span style='background: #AEF1AE'>???</span><span style=''>
</span>69 <span style=''>
</span>70 <span style=''>  override def withNewChilds(newChildren: IndexedSeq[Expression]): Expression = </span><span style='background: #AEF1AE'>copy(children = newChildren)</span><span style=''>
</span>71 <span style=''>}
</span>72 <span style=''>
</span>73 <span style=''>/**
</span>74 <span style=''> * Should not be used in queryplanning
</span>75 <span style=''> * @param rules should be hidden from plans
</span>76 <span style=''> */
</span>77 <span style=''>case class NonPassThrough(rules: Seq[Expression]) extends Unevaluable {
</span>78 <span style=''>  override def nullable: Boolean = </span><span style='background: #AEF1AE'>true</span><span style=''>
</span>79 <span style=''>
</span>80 <span style=''>  override def dataType: DataType = </span><span style='background: #AEF1AE'>BooleanType</span><span style=''>
</span>81 <span style=''>
</span>82 <span style=''>  override def children: Seq[Expression] = </span><span style='background: #AEF1AE'>Seq(Literal(true))</span><span style=''>
</span>83 <span style=''>
</span>84 <span style=''>  protected def withNewChildrenInternal(newChildren: IndexedSeq[Expression]): Expression = this
</span>85 <span style=''>}
</span>86 <span style=''>
</span>87 <span style=''>sealed trait LookupType {
</span>88 <span style=''>  val name: String
</span>89 <span style=''>}
</span>90 <span style=''>
</span>91 <span style=''>case class MapLookupType(name: String) extends LookupType
</span>92 <span style=''>case class BloomLookupType(name: String) extends LookupType
</span>93 <span style=''>
</span>94 <span style=''>/**
</span>95 <span style=''> * Represents the results of lookups.  RuleRows will have empty expressions
</span>96 <span style=''> *
</span>97 <span style=''> * @param ruleSuite
</span>98 <span style=''> * @param ruleResults
</span>99 <span style=''> * @param lambdaResults it's not always possible to toString against an expression tree
</span>100 <span style=''> */
</span>101 <span style=''>case class LookupResults(ruleSuite: RuleSuite, ruleResults: ExpressionLookupResults[RuleRow], lambdaResults: ExpressionLookupResults[Id])
</span>102 <span style=''>
</span>103 <span style=''>case class ExpressionLookupResults[A](lookupConstants: Map[A, Set[LookupType]], lookupExpressions: Set[A])
</span>104 <span style=''>
</span>105 <span style=''>case class ExpressionLookupResult(constants: Set[LookupType], hasExpressionLookups: Boolean)
</span>106 <span style=''>
</span>107 <span style=''>
</span>108 <span style=''>object LookupIdFunctions {
</span>109 <span style=''>
</span>110 <span style=''>  def namesFromSchema(schema: StructType): Set[String] = {
</span>111 <span style=''>
</span>112 <span style=''>    def withParent(name: String, parent: String) =
</span>113 <span style=''>      if (</span><span style='background: #AEF1AE'>parent.isEmpty</span><span style=''>)
</span>114 <span style=''>        </span><span style='background: #AEF1AE'>name</span><span style=''>
</span>115 <span style=''>      else
</span>116 <span style=''>        </span><span style='background: #AEF1AE'>parent + &quot;.&quot; + name</span><span style=''>
</span>117 <span style=''>
</span>118 <span style=''>    def accumulate(set: Set[String], schema: StructType, parent: String): Set[String] =
</span>119 <span style=''>      </span><span style='background: #AEF1AE'>schema.foldLeft(set) {
</span>120 <span style=''></span><span style='background: #AEF1AE'>        (s, field) =&gt;
</span>121 <span style=''></span><span style='background: #AEF1AE'>          val name = withParent(field.name, parent)
</span>122 <span style=''></span><span style='background: #AEF1AE'>          field.dataType match {
</span>123 <span style=''></span><span style='background: #AEF1AE'>            case struct: StructType =&gt;
</span>124 <span style=''></span><span style='background: #AEF1AE'>              accumulate(s + name, struct, name)
</span>125 <span style=''></span><span style='background: #AEF1AE'>            case _ =&gt; s + name
</span>126 <span style=''></span><span style='background: #AEF1AE'>          }
</span>127 <span style=''></span><span style='background: #AEF1AE'>      }</span><span style=''>
</span>128 <span style=''>
</span>129 <span style=''>    </span><span style='background: #AEF1AE'>accumulate(Set.empty, schema, &quot;&quot;)</span><span style=''>
</span>130 <span style=''>  }
</span>131 <span style=''>
</span>132 <span style=''>  /**
</span>133 <span style=''>   * Use this function to identify which maps / blooms etc. are used by a given rulesuite
</span>134 <span style=''>   * collects all rules that are using lookup functions but without constant expressions and the list of lookups that are constants.
</span>135 <span style=''>   *
</span>136 <span style=''>   */
</span>137 <span style=''>  def identifyLookups(ruleSuite: RuleSuite): LookupResults = {
</span>138 <span style=''>    val olambdaResults =
</span>139 <span style=''>      </span><span style='background: #AEF1AE'>ruleSuite.lambdaFunctions.flatMap{r =&gt;
</span>140 <span style=''></span><span style='background: #AEF1AE'>        val exp = RuleLogicUtils.expr(r.rule)
</span>141 <span style=''></span><span style='background: #AEF1AE'>        LookupIdFunctionImpl.identifyLookups(exp).map((_,r))
</span>142 <span style=''></span><span style='background: #AEF1AE'>      }.foldLeft(ExpressionLookupResults[Id](Map.empty, Set.empty)) {
</span>143 <span style=''></span><span style='background: #AEF1AE'>        (acc, res) =&gt;
</span>144 <span style=''></span><span style='background: #AEF1AE'>          val r = acc.copy( lookupConstants = acc.lookupConstants + (res._2.id -&gt; res._1.constants))
</span>145 <span style=''></span><span style='background: #AEF1AE'>          if (res._1.hasExpressionLookups)
</span>146 <span style=''></span><span style='background: #AEF1AE'>            r.copy(lookupExpressions = r.lookupExpressions + res._2.id)
</span>147 <span style=''></span><span style='background: #AEF1AE'>          else
</span>148 <span style=''></span><span style='background: #AEF1AE'>            r
</span>149 <span style=''></span><span style='background: #AEF1AE'>      }</span><span style=''>
</span>150 <span style=''>    val lambdaResults = </span><span style='background: #AEF1AE'>olambdaResults.copy(lookupConstants = olambdaResults.lookupConstants.filter(_._2.nonEmpty))</span><span style=''>
</span>151 <span style=''>
</span>152 <span style=''>    </span><span style='background: #AEF1AE'>LookupResults(ruleSuite, ExpressionLookupResults(Map.empty, Set.empty), lambdaResults)</span><span style=''>
</span>153 <span style=''>  }
</span>154 <span style=''>}
</span>155 <span style=''>
</span>156 <span style=''>case class TSLocal[T](val initialValue: () =&gt; T) extends Serializable {
</span>157 <span style=''>  @volatile @transient private var threadLocal: ThreadLocal[T] = _
</span>158 <span style=''>  def get(): T = {
</span>159 <span style=''>    if (</span><span style='background: #AEF1AE'>threadLocal eq null</span><span style=''>) </span><span style='background: #AEF1AE'>{
</span>160 <span style=''></span><span style='background: #AEF1AE'>      val init = initialValue
</span>161 <span style=''></span><span style='background: #AEF1AE'>      this.synchronized {
</span>162 <span style=''></span><span style='background: #AEF1AE'>
</span>163 <span style=''></span><span style='background: #AEF1AE'>        threadLocal = new ThreadLocal[T] {
</span>164 <span style=''></span><span style='background: #AEF1AE'>          override def initialValue(): T = init()
</span>165 <span style=''></span><span style='background: #AEF1AE'>        }
</span>166 <span style=''></span><span style='background: #AEF1AE'>
</span>167 <span style=''></span><span style='background: #AEF1AE'>      }
</span>168 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>169 <span style=''>    </span><span style='background: #AEF1AE'>threadLocal.get()</span><span style=''>
</span>170 <span style=''>  }
</span>171 <span style=''>}
</span>172 <span style=''>
</span>173 <span style=''>case class TransientHolder[T](val initialise: () =&gt; T) extends Serializable {
</span>174 <span style=''>  @volatile @transient private var it: T = _
</span>175 <span style=''>  def get(): T = {
</span>176 <span style=''>    if (</span><span style='background: #AEF1AE'>it == null</span><span style=''>) {
</span>177 <span style=''>      </span><span style='background: #AEF1AE'>this.synchronized {
</span>178 <span style=''></span><span style='background: #AEF1AE'>
</span>179 <span style=''></span><span style='background: #AEF1AE'>        it = initialise()
</span>180 <span style=''></span><span style='background: #AEF1AE'>
</span>181 <span style=''></span><span style='background: #AEF1AE'>      }</span><span style=''>
</span>182 <span style=''>    }
</span>183 <span style=''>    </span><span style='background: #AEF1AE'>it</span><span style=''>
</span>184 <span style=''>  }
</span>185 <span style=''>  def reset: Unit ={
</span>186 <span style=''>    </span><span style='background: #F0ADAD'>this.synchronized {
</span>187 <span style=''></span><span style='background: #F0ADAD'>      it = null.asInstanceOf[T]
</span>188 <span style=''></span><span style='background: #F0ADAD'>    }</span><span style=''>
</span>189 <span style=''>  }
</span>190 <span style=''>}
</span>191 <span style=''>
</span>192 <span style=''>/**
</span>193 <span style=''> * Signifies that testing is being done, it should be ignored by users.
</span>194 <span style=''> */
</span>195 <span style=''>object Testing {
</span>196 <span style=''>  // horrible hack for testing, but at least attempt to make it performant
</span>197 <span style=''>  private val testingFlag = </span><span style='background: #AEF1AE'>new AtomicBoolean(false)</span><span style=''>
</span>198 <span style=''>
</span>199 <span style=''>  /**
</span>200 <span style=''>   * Should not be used by users but currently (0.0.2) only forces re-evaluation of the quality.lambdaHandlers configuration rather than caching once.
</span>201 <span style=''>   */
</span>202 <span style=''>  protected[quality] def setTesting() = {
</span>203 <span style=''>    </span><span style='background: #AEF1AE'>testingFlag.set(true)</span><span style=''>
</span>204 <span style=''>  }
</span>205 <span style=''>
</span>206 <span style=''>  def testing = </span><span style='background: #AEF1AE'>testingFlag.get</span><span style=''>
</span>207 <span style=''>
</span>208 <span style=''>  /**
</span>209 <span style=''>   * Should not be called by users of the library and is provided for testing support only
</span>210 <span style=''>   * @param thunk
</span>211 <span style=''>   */
</span>212 <span style=''>  def test(thunk: =&gt; Unit): Unit = try {
</span>213 <span style=''>    </span><span style='background: #AEF1AE'>setTesting()
</span>214 <span style=''></span><span style='background: #AEF1AE'>    thunk</span><span style=''>
</span>215 <span style=''>  } finally {
</span>216 <span style=''>    </span><span style='background: #AEF1AE'>testingFlag.set(false)</span><span style=''>
</span>217 <span style=''>  }
</span>218 <span style=''>}
</span>219 <span style=''>
</span>220 <span style=''>object Comparison {
</span>221 <span style=''>
</span>222 <span style=''>  /**
</span>223 <span style=''>   * Forwards to compareTo, allows for compareTo[Type] syntax with internal casts
</span>224 <span style=''>   * @param left
</span>225 <span style=''>   * @param right
</span>226 <span style=''>   * @tparam T
</span>227 <span style=''>   * @return
</span>228 <span style=''>   */
</span>229 <span style=''>  def compareTo[T &lt;: Comparable[T]](left: Any, right: Any): Int =
</span>230 <span style=''>    if (</span><span style='background: #F0ADAD'>left == null &amp;&amp; right != null</span><span style=''>)
</span>231 <span style=''>      </span><span style='background: #F0ADAD'>-100</span><span style=''>
</span>232 <span style=''>    else
</span>233 <span style=''>      </span><span style='background: #F0ADAD'>if (right == null &amp;&amp; left != null)
</span>234 <span style=''></span><span style='background: #F0ADAD'>        100
</span>235 <span style=''></span><span style='background: #F0ADAD'>      else
</span>236 <span style=''></span><span style='background: #F0ADAD'>        left.asInstanceOf[T].compareTo( right.asInstanceOf[T])</span><span style=''>
</span>237 <span style=''>
</span>238 <span style=''>  /**
</span>239 <span style=''>   * Forwards to compare, allows for compareToOrdering(ordering) syntax with internal casts
</span>240 <span style=''>   * @param left
</span>241 <span style=''>   * @param right
</span>242 <span style=''>   * @tparam T
</span>243 <span style=''>   * @return
</span>244 <span style=''>   */
</span>245 <span style=''>  def compareToOrdering[T](ordering: Ordering[T])(left: Any, right: Any): Int =
</span>246 <span style=''>    if (</span><span style='background: #AEF1AE'>left == null &amp;&amp; right != null</span><span style=''>)
</span>247 <span style=''>      </span><span style='background: #AEF1AE'>-100</span><span style=''>
</span>248 <span style=''>    else
</span>249 <span style=''>      </span><span style='background: #AEF1AE'>if (right == null &amp;&amp; left != null)
</span>250 <span style=''></span><span style='background: #AEF1AE'>        100
</span>251 <span style=''></span><span style='background: #AEF1AE'>      else
</span>252 <span style=''></span><span style='background: #AEF1AE'>        ordering.compare(left.asInstanceOf[T], right.asInstanceOf[T])</span><span style=''>
</span>253 <span style=''>
</span>254 <span style=''>}
</span>255 <span style=''>
</span>256 <span style=''>object Optional {
</span>257 <span style=''>  def toOptional[T](option: Option[T]): java.util.Optional[T] =
</span>258 <span style=''>    if (</span><span style='background: #AEF1AE'>option.isEmpty</span><span style=''>)
</span>259 <span style=''>      </span><span style='background: #AEF1AE'>java.util.Optional.empty()</span><span style=''>
</span>260 <span style=''>    else
</span>261 <span style=''>      </span><span style='background: #AEF1AE'>java.util.Optional.of(option.get)</span><span style=''>
</span>262 <span style=''>}
</span>263 <span style=''>
</span>264 <span style=''>object Arrays {
</span>265 <span style=''>  /**
</span>266 <span style=''>   * UnsafeArrayData doesn't allow calling .array, foreach when needed and for others use array
</span>267 <span style=''>   *
</span>268 <span style=''>   * @param array
</span>269 <span style=''>   * @param dataType
</span>270 <span style=''>   * @param f
</span>271 <span style=''>   * @return
</span>272 <span style=''>   */
</span>273 <span style=''>  def mapArray[T: ClassTag](array: ArrayData, dataType: DataType, f: Any =&gt; T): Array[T] =
</span>274 <span style=''>    array match {
</span>275 <span style=''>      case _: UnsafeArrayData =&gt;
</span>276 <span style=''>        val res = </span><span style='background: #AEF1AE'>Array.ofDim[T](array.numElements())</span><span style=''>
</span>277 <span style=''>        </span><span style='background: #AEF1AE'>array.foreach(dataType, (i, v) =&gt; res.update(i, f(v)))</span><span style=''>
</span>278 <span style=''>        res
</span>279 <span style=''>      case _ =&gt; </span><span style='background: #AEF1AE'>array.array.map(f)</span><span style=''>
</span>280 <span style=''>    }
</span>281 <span style=''>
</span>282 <span style=''>  /**
</span>283 <span style=''>   * gets an array out of UnsafeArrayData or others
</span>284 <span style=''>   * @param array
</span>285 <span style=''>   * @param dataType
</span>286 <span style=''>   * @return
</span>287 <span style=''>   */
</span>288 <span style=''>  def toArray(array: ArrayData, dataType: DataType): Array[Any] =
</span>289 <span style=''>    array match {
</span>290 <span style=''>      case _: UnsafeArrayData =&gt;
</span>291 <span style=''>        </span><span style='background: #AEF1AE'>mapArray(array, dataType, identity)</span><span style=''>
</span>292 <span style=''>      case _ =&gt; </span><span style='background: #AEF1AE'>array.array</span><span style=''>
</span>293 <span style=''>    }
</span>294 <span style=''>
</span>295 <span style=''>}
</span>296 <span style=''>
</span>297 <span style=''>/**
</span>298 <span style=''> * With the introduction of the 4 runtime folder needs different
</span>299 <span style=''> * resolved behaviour on lazytyperef, as such these move here
</span>300 <span style=''> * from TestUtils
</span>301 <span style=''> */
</span>302 <span style=''>object SparkVersions {
</span>303 <span style=''>
</span>304 <span style=''>  lazy val sparkFullVersion = {
</span>305 <span style=''>    val pos = classOf[Expression].getPackage.getSpecificationVersion
</span>306 <span style=''>    if ((pos eq null) || pos == &quot;0.0&quot;) // DBR is always null, Fabric 0.0
</span>307 <span style=''>      SparkSession.active.version
</span>308 <span style=''>    else
</span>309 <span style=''>      pos
</span>310 <span style=''>  }
</span>311 <span style=''>
</span>312 <span style=''>  lazy val sparkVersion = sparkFullVersion.split('.').take(2).mkString(&quot;.&quot;)
</span>313 <span style=''>
</span>314 <span style=''>  lazy val sparkMajorVersion = sparkFullVersion.split('.').head
</span>315 <span style=''>}
</span>316 <span style=''>
</span>317 <span style=''>/**
</span>318 <span style=''> * Frameless sets path in foldable encoders to nullable == false, but it really is nullable
</span>319 <span style=''> * Spark then just accesses the struct which is null.  This forces codegen only
</span>320 <span style=''> */
</span>321 <span style=''>case class ForceNullable(child: Expression) extends Expression {
</span>322 <span style=''>
</span>323 <span style=''>  val children = </span><span style='background: #AEF1AE'>Seq(child)</span><span style=''>
</span>324 <span style=''>
</span>325 <span style=''>  override def nullable: Boolean = </span><span style='background: #AEF1AE'>true</span><span style=''>
</span>326 <span style=''>
</span>327 <span style=''>  override def eval(input: InternalRow): Any = </span><span style='background: #AEF1AE'>child.eval(input)</span><span style=''>
</span>328 <span style=''>
</span>329 <span style=''>  override protected def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {
</span>330 <span style=''>    val c = </span><span style='background: #AEF1AE'>child.genCode(ctx)</span><span style=''>
</span>331 <span style=''>    val typ = </span><span style='background: #AEF1AE'>JavaCode.javaType(dataType)</span><span style=''>
</span>332 <span style=''>    val boxed = </span><span style='background: #AEF1AE'>JavaCode.boxedType(dataType)</span><span style=''>
</span>333 <span style=''>    </span><span style='background: #AEF1AE'>ev.copy(code =
</span>334 <span style=''></span><span style='background: #AEF1AE'>      code&quot;&quot;&quot;
</span>335 <span style=''></span><span style='background: #AEF1AE'>            ${c.code}
</span>336 <span style=''></span><span style='background: #AEF1AE'>            boolean ${ev.isNull} = true;
</span>337 <span style=''></span><span style='background: #AEF1AE'>            $typ ${ev.value} = null;
</span>338 <span style=''></span><span style='background: #AEF1AE'>            if (${c.value} != null) {
</span>339 <span style=''></span><span style='background: #AEF1AE'>              ${ev.isNull} = false;
</span>340 <span style=''></span><span style='background: #AEF1AE'>              ${ev.value} = ($boxed) ${c.value};
</span>341 <span style=''></span><span style='background: #AEF1AE'>            }
</span>342 <span style=''></span><span style='background: #AEF1AE'>            &quot;&quot;&quot;)</span><span style=''>
</span>343 <span style=''>  }
</span>344 <span style=''>
</span>345 <span style=''>
</span>346 <span style=''>  override def dataType: DataType = </span><span style='background: #AEF1AE'>child.dataType</span><span style=''>
</span>347 <span style=''>
</span>348 <span style=''>  protected def withNewChildrenInternal(newChildren: IndexedSeq[Expression]): Expression =
</span>349 <span style=''>    </span><span style='background: #AEF1AE'>copy(child = newChildren.head)</span><span style=''>
</span>350 <span style=''>}
</span>351 <span style=''>
</span>352 <span style=''>object Encoding {
</span>353 <span style=''>
</span>354 <span style=''>  /**
</span>355 <span style=''>   * Wraps a non Frameless encoder in a TypedEncoder, adjusting paths as needed.
</span>356 <span style=''>   *
</span>357 <span style=''>   * This is not intended for general use and is used by the ProcessFunctions.
</span>358 <span style=''>   *
</span>359 <span style=''>   * @param outputType
</span>360 <span style=''>   * @tparam T
</span>361 <span style=''>   * @return
</span>362 <span style=''>   */
</span>363 <span style=''>  def fromNormalEncoder[T: Encoder](outputType: DataType): TypedEncoder[T] = {
</span>364 <span style=''>    val oexpr = </span><span style='background: #AEF1AE'>ShimUtils.expressionEncoder(implicitly[Encoder[T]])</span><span style=''>
</span>365 <span style=''>
</span>366 <span style=''>    implicit val cltag = </span><span style='background: #AEF1AE'>oexpr.clsTag</span><span style=''>
</span>367 <span style=''>
</span>368 <span style=''>    </span><span style='background: #AEF1AE'>new</span><span style=''> TypedEncoder[T] {
</span>369 <span style=''>
</span>370 <span style=''>      override def nullable: Boolean = </span><span style='background: #AEF1AE'>true</span><span style=''>
</span>371 <span style=''>
</span>372 <span style=''>      override def jvmRepr: DataType = </span><span style='background: #AEF1AE'>oexpr.deserializer.dataType</span><span style=''>
</span>373 <span style=''>
</span>374 <span style=''>      override def catalystRepr: DataType = {
</span>375 <span style=''>        val se = </span><span style='background: #AEF1AE'>oexpr.serializer</span><span style=''>
</span>376 <span style=''>        if (</span><span style='background: #AEF1AE'>se.length == 1</span><span style=''>)
</span>377 <span style=''>          </span><span style='background: #AEF1AE'>se.head.dataType</span><span style=''>
</span>378 <span style=''>        else
</span>379 <span style=''>          </span><span style='background: #AEF1AE'>StructType( // 2.4 only cast
</span>380 <span style=''></span><span style='background: #AEF1AE'>            se.map(n =&gt; StructField(n.asInstanceOf[NamedExpression].qualifiedName, n.dataType, n.nullable))
</span>381 <span style=''></span><span style='background: #AEF1AE'>          )</span><span style=''>
</span>382 <span style=''>      }
</span>383 <span style=''>
</span>384 <span style=''>      override def fromCatalyst(path: Expression): Expression = {
</span>385 <span style=''>        val de = </span><span style='background: #AEF1AE'>oexpr.deserializer</span><span style=''>
</span>386 <span style=''>        val r =
</span>387 <span style=''>          de match {
</span>388 <span style=''>            case a: Alias =&gt;
</span>389 <span style=''>              </span><span style='background: #AEF1AE'>a.child</span><span style=''> match {
</span>390 <span style=''>                case m: UnresolvedMapObjects =&gt; </span><span style='background: #AEF1AE'>a.withNewChildren(Seq( m.copy(child = path) ))</span><span style=''>
</span>391 <span style=''>                case a =&gt; </span><span style='background: #AEF1AE'>a.transformUp {
</span>392 <span style=''></span><span style='background: #AEF1AE'>                  case _: GetColumnByOrdinal =&gt;
</span>393 <span style=''></span><span style='background: #AEF1AE'>                    path
</span>394 <span style=''></span><span style='background: #AEF1AE'>                }</span><span style=''>
</span>395 <span style=''>              }
</span>396 <span style=''>            case m: UnresolvedMapObjects =&gt; </span><span style='background: #AEF1AE'>m.copy(child = path)</span><span style=''>
</span>397 <span style=''>            case n: NewInstance =&gt;
</span>398 <span style=''>              val o = </span><span style='background: #AEF1AE'>outputType.asInstanceOf[StructType].zipWithIndex.map{case (e,i) =&gt; e.name -&gt; i }.toMap</span><span style=''>
</span>399 <span style=''>
</span>400 <span style=''>              </span><span style='background: #AEF1AE'>If(IsNull(ForceNullable(path)), Literal(null),
</span>401 <span style=''></span><span style='background: #AEF1AE'>                n.withNewChildren(n.children map {
</span>402 <span style=''></span><span style='background: #AEF1AE'>                  _.transform {
</span>403 <span style=''></span><span style='background: #AEF1AE'>                    case u: UnresolvedAttribute if o.contains(u.name) =&gt;
</span>404 <span style=''></span><span style='background: #AEF1AE'>                      GetStructField3(path, o(u.name))
</span>405 <span style=''></span><span style='background: #AEF1AE'>                  }
</span>406 <span style=''></span><span style='background: #AEF1AE'>                })
</span>407 <span style=''></span><span style='background: #AEF1AE'>              )</span><span style=''>
</span>408 <span style=''>            case i: InitializeJavaBean =&gt;
</span>409 <span style=''>              val o = </span><span style='background: #AEF1AE'>outputType.asInstanceOf[StructType].zipWithIndex.map{case (e,i) =&gt; e.name -&gt; i }.toMap</span><span style=''>
</span>410 <span style=''>
</span>411 <span style=''>              </span><span style='background: #AEF1AE'>If(IsNull(ForceNullable(path)), Literal(null),
</span>412 <span style=''></span><span style='background: #AEF1AE'>                i.copy(setters =
</span>413 <span style=''></span><span style='background: #AEF1AE'>                  i.setters.map{ p =&gt;
</span>414 <span style=''></span><span style='background: #AEF1AE'>                    (p._1, p._2.transform {
</span>415 <span style=''></span><span style='background: #AEF1AE'>                      case u: UnresolvedAttribute if o.contains(u.name) =&gt;
</span>416 <span style=''></span><span style='background: #AEF1AE'>                        GetStructField3(path, o(u.name))
</span>417 <span style=''></span><span style='background: #AEF1AE'>                    })
</span>418 <span style=''></span><span style='background: #AEF1AE'>                  }
</span>419 <span style=''></span><span style='background: #AEF1AE'>                )
</span>420 <span style=''></span><span style='background: #AEF1AE'>              )</span><span style=''>
</span>421 <span style=''>            // all single fields from a struct
</span>422 <span style=''>            case i: Invoke =&gt;
</span>423 <span style=''>              </span><span style='background: #AEF1AE'>i.transformUp {
</span>424 <span style=''></span><span style='background: #AEF1AE'>                case _: GetColumnByOrdinal =&gt;
</span>425 <span style=''></span><span style='background: #AEF1AE'>                  path
</span>426 <span style=''></span><span style='background: #AEF1AE'>              }</span><span style=''>
</span>427 <span style=''>            case a =&gt; </span><span style='background: #AEF1AE'>a.transformUp {
</span>428 <span style=''></span><span style='background: #AEF1AE'>              case _: GetColumnByOrdinal =&gt;
</span>429 <span style=''></span><span style='background: #AEF1AE'>                path
</span>430 <span style=''></span><span style='background: #AEF1AE'>            }</span><span style=''>
</span>431 <span style=''>          }
</span>432 <span style=''>        r
</span>433 <span style=''>
</span>434 <span style=''>      }
</span>435 <span style=''>
</span>436 <span style=''>      // only used by resolveAndBind
</span>437 <span style=''>      override def toCatalyst(path: Expression): Expression = {
</span>438 <span style=''>        val se = </span><span style='background: #AEF1AE'>oexpr.serializer</span><span style=''>
</span>439 <span style=''>        if (</span><span style='background: #AEF1AE'>se.length == 1</span><span style=''>)
</span>440 <span style=''>          </span><span style='background: #AEF1AE'>se.head match {
</span>441 <span style=''></span><span style='background: #AEF1AE'>            case a: Alias =&gt;
</span>442 <span style=''></span><span style='background: #AEF1AE'>              a.child match {
</span>443 <span style=''></span><span style='background: #AEF1AE'>                case m: MapObjects =&gt; a.withNewChildren(Seq( m.copy(inputData = path) ))
</span>444 <span style=''></span><span style='background: #AEF1AE'>                case a =&gt; a.transformUp {
</span>445 <span style=''></span><span style='background: #AEF1AE'>                  case b: BoundReference =&gt; path
</span>446 <span style=''></span><span style='background: #AEF1AE'>                }
</span>447 <span style=''></span><span style='background: #AEF1AE'>              }
</span>448 <span style=''></span><span style='background: #AEF1AE'>            case m: MapObjects =&gt; m.copy(inputData = path)
</span>449 <span style=''></span><span style='background: #AEF1AE'>            case a =&gt; a.transformUp {
</span>450 <span style=''></span><span style='background: #AEF1AE'>              case b: BoundReference =&gt; path
</span>451 <span style=''></span><span style='background: #AEF1AE'>            }
</span>452 <span style=''></span><span style='background: #AEF1AE'>          }</span><span style=''>
</span>453 <span style=''>        else </span><span style='background: #AEF1AE'>{
</span>454 <span style=''></span><span style='background: #AEF1AE'>          val o = outputType.asInstanceOf[StructType]
</span>455 <span style=''></span><span style='background: #AEF1AE'>
</span>456 <span style=''></span><span style='background: #AEF1AE'>          val dealiased = se.map {
</span>457 <span style=''></span><span style='background: #AEF1AE'>            case a: Alias =&gt;
</span>458 <span style=''></span><span style='background: #AEF1AE'>              a.name -&gt; a.child.transformUp {
</span>459 <span style=''></span><span style='background: #AEF1AE'>                case b: BoundReference =&gt; path
</span>460 <span style=''></span><span style='background: #AEF1AE'>              }
</span>461 <span style=''></span><span style='background: #AEF1AE'>          }.toMap
</span>462 <span style=''></span><span style='background: #AEF1AE'>
</span>463 <span style=''></span><span style='background: #AEF1AE'>          CreateNamedStruct1(
</span>464 <span style=''></span><span style='background: #AEF1AE'>            o.fields.map(f =&gt; f.name -&gt; dealiased(f.name)).flatMap {
</span>465 <span style=''></span><span style='background: #AEF1AE'>              case (name, e) =&gt;
</span>466 <span style=''></span><span style='background: #AEF1AE'>                Seq[Expression](Literal(name), e)
</span>467 <span style=''></span><span style='background: #AEF1AE'>            }
</span>468 <span style=''></span><span style='background: #AEF1AE'>          )
</span>469 <span style=''></span><span style='background: #AEF1AE'>        }</span><span style=''>
</span>470 <span style=''>      }
</span>471 <span style=''>    }
</span>472 <span style=''>
</span>473 <span style=''>  }
</span>474 <span style=''>
</span>475 <span style=''>}
</span>476 <span style=''>
</span>477 <span style=''>/**
</span>478 <span style=''> * wrap subexprs so we can correctly identify the subquery post bindreferences
</span>479 <span style=''> * @param children
</span>480 <span style=''> */
</span>481 <span style=''>case class SubQueryWrapper(children: Seq[Expression]) extends Expression {
</span>482 <span style=''>
</span>483 <span style=''>  override def nullable: Boolean = </span><span style='background: #AEF1AE'>children.head.nullable</span><span style=''>
</span>484 <span style=''>  override def foldable: Boolean = </span><span style='background: #AEF1AE'>false</span><span style=''>
</span>485 <span style=''>  override protected def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {
</span>486 <span style=''>    val expr = </span><span style='background: #AEF1AE'>children.head.genCode(ctx)</span><span style=''>
</span>487 <span style=''>    expr
</span>488 <span style=''>  }
</span>489 <span style=''>
</span>490 <span style=''>  override def eval(input: InternalRow): Any = </span><span style='background: #AEF1AE'>children.head.eval(input)</span><span style=''>
</span>491 <span style=''>
</span>492 <span style=''>  override def dataType: DataType = </span><span style='background: #AEF1AE'>children.head.dataType</span><span style=''>
</span>493 <span style=''>
</span>494 <span style=''>  protected def withNewChildrenInternal(newChildren: IndexedSeq[Expression]): Expression = </span><span style='background: #AEF1AE'>copy(children = newChildren)</span><span style=''>
</span>495 <span style=''>}
</span>496 <span style=''>
</span>497 <span style=''>object SubQueryWrapper {
</span>498 <span style=''>  def hasASubQuery(expr: Expression): Boolean =
</span>499 <span style=''>    (</span><span style='background: #AEF1AE'>expr.collectFirst {
</span>500 <span style=''></span><span style='background: #AEF1AE'>      case s: SubQueryWrapper =&gt; s
</span>501 <span style=''></span><span style='background: #AEF1AE'>    }.isDefined</span><span style=''>)
</span>502 <span style=''>}
</span>503 <span style=''>
</span>504 <span style=''>object Params {
</span>505 <span style=''>
</span>506 <span style=''>  def stripBrackets(v: VariableValue): (String, String) = {
</span>507 <span style=''>    val openb = </span><span style='background: #AEF1AE'>v.toString().indexOf(&quot;[&quot;)</span><span style=''>
</span>508 <span style=''>    if (</span><span style='background: #AEF1AE'>openb == -1</span><span style=''>)
</span>509 <span style=''>      </span><span style='background: #AEF1AE'>(v.variableName, &quot;&quot;)</span><span style=''>
</span>510 <span style=''>    else
</span>511 <span style=''>      </span><span style='background: #AEF1AE'>(v.variableName.dropRight(v.length - openb), v.variableName.drop(openb))</span><span style=''>
</span>512 <span style=''>  }
</span>513 <span style=''>
</span>514 <span style=''>  def formatParams(ctx: CodegenContext, a: Seq[ExprValue], callsKeepArrays: Boolean = false): (String, String) = {
</span>515 <span style=''>    // filter out any top level arrays, the input is a set, so params need the same order
</span>516 <span style=''>    val ordered = </span><span style='background: #AEF1AE'>a.flatMap {
</span>517 <span style=''></span><span style='background: #AEF1AE'>      //case a: VariableValue if ExprUtils.isVariableMutableArray(ctx, a) =&gt; None
</span>518 <span style=''></span><span style='background: #AEF1AE'>      case a: VariableValue =&gt; Some(a)
</span>519 <span style=''></span><span style='background: #AEF1AE'>      case _ =&gt; None
</span>520 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>521 <span style=''>
</span>522 <span style=''>    </span><span style='background: #AEF1AE'>(ordered.map { v =&gt;
</span>523 <span style=''></span><span style='background: #AEF1AE'>      val (stripped, arrayInName) = stripBrackets(v)
</span>524 <span style=''></span><span style='background: #AEF1AE'>
</span>525 <span style=''></span><span style='background: #AEF1AE'>      val (typ, array) =
</span>526 <span style=''></span><span style='background: #AEF1AE'>        if (v.javaType.isArray)
</span>527 <span style=''></span><span style='background: #AEF1AE'>          (s&quot;${v.javaType.getComponentType.getName}&quot;, &quot;[]&quot;)
</span>528 <span style=''></span><span style='background: #AEF1AE'>        else if (v.javaType.isPrimitive)
</span>529 <span style=''></span><span style='background: #AEF1AE'>          (v.javaType.toString, arrayInName.replaceAll(&quot;[^\\[\\]]&quot;,&quot;&quot;))
</span>530 <span style=''></span><span style='background: #AEF1AE'>        else
</span>531 <span style=''></span><span style='background: #AEF1AE'>          (v.javaType.getName, arrayInName.replaceAll(&quot;[^\\[\\]]&quot;,&quot;&quot;))
</span>532 <span style=''></span><span style='background: #AEF1AE'>
</span>533 <span style=''></span><span style='background: #AEF1AE'>      s&quot;$typ$array $stripped&quot;
</span>534 <span style=''></span><span style='background: #AEF1AE'>    }.mkString(&quot;, &quot;)
</span>535 <span style=''></span><span style='background: #AEF1AE'>      , ordered.map(v =&gt;
</span>536 <span style=''></span><span style='background: #AEF1AE'>        if (v.javaType.isArray &amp;&amp; callsKeepArrays)
</span>537 <span style=''></span><span style='background: #AEF1AE'>          v.variableName
</span>538 <span style=''></span><span style='background: #AEF1AE'>        else
</span>539 <span style=''></span><span style='background: #AEF1AE'>          stripBrackets(v)._1
</span>540 <span style=''></span><span style='background: #AEF1AE'>      ).mkString(&quot;, &quot;))</span><span style=''>
</span>541 <span style=''>  }
</span>542 <span style=''>}
</span></pre>
          </div>
          <div class="tab-pane" id="statementlist">
            <table cellspacing="0" cellpadding="0" class="table statementlist">
      <tr>
        <th>Line</th>
        <th>Stmt Id</th>
        <th>Pos</th>
        <th>Tree</th>
        <th>Symbol</th>
        <th>Tests</th>
        <th>Code</th>
      </tr><tr>
        <td>
          25
        </td>
        <td>
          7690
        </td>
        <td>
          1392
          -
          1416
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.System.currentTimeMillis
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          java.lang.System.currentTimeMillis()
        </td>
      </tr><tr>
        <td>
          27
        </td>
        <td>
          7691
        </td>
        <td>
          1433
          -
          1438
        </td>
        <td>
          Ident
        </td>
        <td>
          com.sparkutils.quality.impl.util.DebugTime.thunk
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          thunk
        </td>
      </tr><tr>
        <td>
          28
        </td>
        <td>
          7695
        </td>
        <td>
          1453
          -
          1533
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  val stop: Long = java.lang.System.currentTimeMillis();
  log.apply(stop.-(start), what)
}
        </td>
      </tr><tr>
        <td>
          29
        </td>
        <td>
          7692
        </td>
        <td>
          1472
          -
          1496
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.System.currentTimeMillis
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          java.lang.System.currentTimeMillis()
        </td>
      </tr><tr>
        <td>
          31
        </td>
        <td>
          7694
        </td>
        <td>
          1504
          -
          1527
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          log.apply(stop.-(start), what)
        </td>
      </tr><tr>
        <td>
          31
        </td>
        <td>
          7693
        </td>
        <td>
          1508
          -
          1520
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Long.-
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          stop.-(start)
        </td>
      </tr><tr>
        <td>
          38
        </td>
        <td>
          7696
        </td>
        <td>
          1616
          -
          1620
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          true
        </td>
      </tr><tr>
        <td>
          40
        </td>
        <td>
          7697
        </td>
        <td>
          1669
          -
          1694
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Literal.eval
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Literal.apply(true).eval(input)
        </td>
      </tr><tr>
        <td>
          42
        </td>
        <td>
          7698
        </td>
        <td>
          1732
          -
          1743
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.BooleanType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.BooleanType
        </td>
      </tr><tr>
        <td>
          55
        </td>
        <td>
          7699
        </td>
        <td>
          2334
          -
          2362
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.PassThroughCompileEvals.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          PassThroughCompileEvals.this.copy(newChildren)
        </td>
      </tr><tr>
        <td>
          57
        </td>
        <td>
          7700
        </td>
        <td>
          2444
          -
          2472
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.PassThroughCompileEvals.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          PassThroughCompileEvals.this.copy(newChildren)
        </td>
      </tr><tr>
        <td>
          66
        </td>
        <td>
          7701
        </td>
        <td>
          2887
          -
          2915
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.PassThroughEvalOnly.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          PassThroughEvalOnly.this.copy(newChildren)
        </td>
      </tr><tr>
        <td>
          68
        </td>
        <td>
          7702
        </td>
        <td>
          3140
          -
          3143
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Predef.???
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.???
        </td>
      </tr><tr>
        <td>
          70
        </td>
        <td>
          7703
        </td>
        <td>
          3225
          -
          3253
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.PassThroughEvalOnly.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          PassThroughEvalOnly.this.copy(newChildren)
        </td>
      </tr><tr>
        <td>
          78
        </td>
        <td>
          7704
        </td>
        <td>
          3455
          -
          3459
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          80
        </td>
        <td>
          7705
        </td>
        <td>
          3497
          -
          3508
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.BooleanType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.BooleanType
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          7706
        </td>
        <td>
          3557
          -
          3570
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Literal.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Literal.apply(true)
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          7707
        </td>
        <td>
          3553
          -
          3571
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Literal](org.apache.spark.sql.catalyst.expressions.Literal.apply(true))
        </td>
      </tr><tr>
        <td>
          113
        </td>
        <td>
          7708
        </td>
        <td>
          4547
          -
          4561
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.isEmpty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          parent.isEmpty()
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          7709
        </td>
        <td>
          4571
          -
          4575
        </td>
        <td>
          Ident
        </td>
        <td>
          com.sparkutils.quality.impl.util.LookupIdFunctions.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          name
        </td>
      </tr><tr>
        <td>
          116
        </td>
        <td>
          7711
        </td>
        <td>
          4595
          -
          4614
        </td>
        <td>
          Block
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          parent.+(&quot;.&quot;).+(name)
        </td>
      </tr><tr>
        <td>
          116
        </td>
        <td>
          7710
        </td>
        <td>
          4595
          -
          4614
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          parent.+(&quot;.&quot;).+(name)
        </td>
      </tr><tr>
        <td>
          119
        </td>
        <td>
          7718
        </td>
        <td>
          4710
          -
          4978
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.foldLeft
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          schema.foldLeft[Set[String]](set)(((s: Set[String], field: org.apache.spark.sql.types.StructField) =&gt; {
  val name: String = withParent(field.name, parent);
  field.dataType match {
    case (struct @ (_: org.apache.spark.sql.types.StructType)) =&gt; accumulate(s.+(name), struct, name)
    case _ =&gt; s.+(name)
  }
}))
        </td>
      </tr><tr>
        <td>
          121
        </td>
        <td>
          7712
        </td>
        <td>
          4787
          -
          4797
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          field.name
        </td>
      </tr><tr>
        <td>
          121
        </td>
        <td>
          7713
        </td>
        <td>
          4776
          -
          4806
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.LookupIdFunctions.withParent
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          withParent(field.name, parent)
        </td>
      </tr><tr>
        <td>
          122
        </td>
        <td>
          7714
        </td>
        <td>
          4817
          -
          4831
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          field.dataType
        </td>
      </tr><tr>
        <td>
          124
        </td>
        <td>
          7715
        </td>
        <td>
          4904
          -
          4912
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SetLike.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          s.+(name)
        </td>
      </tr><tr>
        <td>
          124
        </td>
        <td>
          7716
        </td>
        <td>
          4893
          -
          4927
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.LookupIdFunctions.accumulate
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          accumulate(s.+(name), struct, name)
        </td>
      </tr><tr>
        <td>
          125
        </td>
        <td>
          7717
        </td>
        <td>
          4950
          -
          4958
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SetLike.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          s.+(name)
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          7721
        </td>
        <td>
          4984
          -
          5017
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.LookupIdFunctions.accumulate
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          accumulate(scala.Predef.Set.empty[String], schema, &quot;&quot;)
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          7720
        </td>
        <td>
          5014
          -
          5016
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          7719
        </td>
        <td>
          4995
          -
          5004
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.generic.ImmutableSetFactory.empty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.empty[String]
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          7727
        </td>
        <td>
          5390
          -
          5390
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[(com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction)]
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          7723
        </td>
        <td>
          5414
          -
          5441
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleLogicUtils.expr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.RuleLogicUtils.expr(r.rule)
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          7722
        </td>
        <td>
          5434
          -
          5440
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.HasRuleText.rule
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          r.rule
        </td>
      </tr><tr>
        <td>
          141
        </td>
        <td>
          7724
        </td>
        <td>
          5496
          -
          5501
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction](x$1, r)
        </td>
      </tr><tr>
        <td>
          141
        </td>
        <td>
          7726
        </td>
        <td>
          5450
          -
          5502
        </td>
        <td>
          ApplyImplicitView
        </td>
        <td>
          scala.Option.option2Iterable
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.this.Option.option2Iterable[(com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction)](LookupIdFunctionImpl.identifyLookups(exp).map[(com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction)](((x$1: com.sparkutils.quality.impl.util.ExpressionLookupResult) =&gt; scala.Tuple2.apply[com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction](x$1, r))))
        </td>
      </tr><tr>
        <td>
          141
        </td>
        <td>
          7725
        </td>
        <td>
          5450
          -
          5502
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Option.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          LookupIdFunctionImpl.identifyLookups(exp).map[(com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction)](((x$1: com.sparkutils.quality.impl.util.ExpressionLookupResult) =&gt; scala.Tuple2.apply[com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction](x$1, r)))
        </td>
      </tr><tr>
        <td>
          142
        </td>
        <td>
          7730
        </td>
        <td>
          5520
          -
          5569
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ExpressionLookupResults.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ExpressionLookupResults.apply[com.sparkutils.quality.Id](scala.Predef.Map.empty[com.sparkutils.quality.Id, Nothing], scala.Predef.Set.empty[com.sparkutils.quality.Id])
        </td>
      </tr><tr>
        <td>
          142
        </td>
        <td>
          7744
        </td>
        <td>
          5357
          -
          5847
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.foldLeft
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ruleSuite.lambdaFunctions.flatMap[(com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction), Seq[(com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction)]](((r: com.sparkutils.quality.impl.LambdaFunction) =&gt; {
  val exp: org.apache.spark.sql.catalyst.expressions.Expression = com.sparkutils.quality.impl.RuleLogicUtils.expr(r.rule);
  scala.this.Option.option2Iterable[(com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction)](LookupIdFunctionImpl.identifyLookups(exp).map[(com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction)](((x$1: com.sparkutils.quality.impl.util.ExpressionLookupResult) =&gt; scala.Tuple2.apply[com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction](x$1, r))))
}))(collection.this.Seq.canBuildFrom[(com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction)]).foldLeft[com.sparkutils.quality.impl.util.ExpressionLookupResults[com.sparkutils.quality.Id]](ExpressionLookupResults.apply[com.sparkutils.quality.Id](scala.Predef.Map.empty[com.sparkutils.quality.Id, Nothing], scala.Predef.Set.empty[com.sparkutils.quality.Id]))(((acc: com.sparkutils.quality.impl.util.ExpressionLookupResults[com.sparkutils.quality.Id], res: (com.sparkutils.quality.impl.util.ExpressionLookupResult, com.sparkutils.quality.impl.LambdaFunction)) =&gt; {
  val r: com.sparkutils.quality.impl.util.ExpressionLookupResults[com.sparkutils.quality.Id] = acc.copy[com.sparkutils.quality.Id](acc.lookupConstants.+[Set[com.sparkutils.quality.impl.util.LookupType]](scala.Predef.ArrowAssoc[com.sparkutils.quality.Id](res._2.id).-&gt;[Set[com.sparkutils.quality.impl.util.LookupType]](res._1.constants)), acc.copy$default$2[Nothing]);
  if (res._1.hasExpressionLookups)
    {
      &lt;artifact&gt; val x$1: scala.collection.immutable.Set[com.sparkutils.quality.Id] @scala.reflect.internal.annotations.uncheckedBounds = r.lookupExpressions.+(res._2.id);
      &lt;artifact&gt; val x$2: Map[com.sparkutils.quality.Id,Set[com.sparkutils.quality.impl.util.LookupType]] @scala.reflect.internal.annotations.uncheckedBounds = r.copy$default$1[Nothing];
      r.copy[com.sparkutils.quality.Id](x$2, x$1)
    }
  else
    r
}))
        </td>
      </tr><tr>
        <td>
          142
        </td>
        <td>
          7729
        </td>
        <td>
          5559
          -
          5568
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.generic.ImmutableSetFactory.empty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.empty[com.sparkutils.quality.Id]
        </td>
      </tr><tr>
        <td>
          142
        </td>
        <td>
          7728
        </td>
        <td>
          5548
          -
          5557
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.Map.empty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Map.empty[com.sparkutils.quality.Id, Nothing]
        </td>
      </tr><tr>
        <td>
          144
        </td>
        <td>
          7736
        </td>
        <td>
          5613
          -
          5695
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ExpressionLookupResults.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          acc.copy[com.sparkutils.quality.Id](acc.lookupConstants.+[Set[com.sparkutils.quality.impl.util.LookupType]](scala.Predef.ArrowAssoc[com.sparkutils.quality.Id](res._2.id).-&gt;[Set[com.sparkutils.quality.impl.util.LookupType]](res._1.constants)), acc.copy$default$2[Nothing])
        </td>
      </tr><tr>
        <td>
          144
        </td>
        <td>
          7733
        </td>
        <td>
          5664
          -
          5693
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Predef.ArrowAssoc.-&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.ArrowAssoc[com.sparkutils.quality.Id](res._2.id).-&gt;[Set[com.sparkutils.quality.impl.util.LookupType]](res._1.constants)
        </td>
      </tr><tr>
        <td>
          144
        </td>
        <td>
          7735
        </td>
        <td>
          5617
          -
          5617
        </td>
        <td>
          TypeApply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ExpressionLookupResults.copy$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          acc.copy$default$2[Nothing]
        </td>
      </tr><tr>
        <td>
          144
        </td>
        <td>
          7732
        </td>
        <td>
          5677
          -
          5693
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.util.ExpressionLookupResult.constants
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          res._1.constants
        </td>
      </tr><tr>
        <td>
          144
        </td>
        <td>
          7731
        </td>
        <td>
          5664
          -
          5673
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.LambdaFunction.id
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          res._2.id
        </td>
      </tr><tr>
        <td>
          144
        </td>
        <td>
          7734
        </td>
        <td>
          5641
          -
          5694
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.immutable.Map.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          acc.lookupConstants.+[Set[com.sparkutils.quality.impl.util.LookupType]](scala.Predef.ArrowAssoc[com.sparkutils.quality.Id](res._2.id).-&gt;[Set[com.sparkutils.quality.impl.util.LookupType]](res._1.constants))
        </td>
      </tr><tr>
        <td>
          145
        </td>
        <td>
          7737
        </td>
        <td>
          5710
          -
          5737
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.util.ExpressionLookupResult.hasExpressionLookups
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          res._1.hasExpressionLookups
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          7739
        </td>
        <td>
          5778
          -
          5809
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SetLike.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          r.lookupExpressions.+(res._2.id)
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          7742
        </td>
        <td>
          5751
          -
          5810
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  &lt;artifact&gt; val x$1: scala.collection.immutable.Set[com.sparkutils.quality.Id] @scala.reflect.internal.annotations.uncheckedBounds = r.lookupExpressions.+(res._2.id);
  &lt;artifact&gt; val x$2: Map[com.sparkutils.quality.Id,Set[com.sparkutils.quality.impl.util.LookupType]] @scala.reflect.internal.annotations.uncheckedBounds = r.copy$default$1[Nothing];
  r.copy[com.sparkutils.quality.Id](x$2, x$1)
}
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          7738
        </td>
        <td>
          5800
          -
          5809
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.LambdaFunction.id
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          res._2.id
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          7741
        </td>
        <td>
          5751
          -
          5810
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ExpressionLookupResults.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          r.copy[com.sparkutils.quality.Id](x$2, x$1)
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          7740
        </td>
        <td>
          5753
          -
          5753
        </td>
        <td>
          TypeApply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ExpressionLookupResults.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          r.copy$default$1[Nothing]
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          7743
        </td>
        <td>
          5838
          -
          5839
        </td>
        <td>
          Ident
        </td>
        <td>
          com.sparkutils.quality.impl.util.LookupIdFunctions.r
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          r
        </td>
      </tr><tr>
        <td>
          150
        </td>
        <td>
          7745
        </td>
        <td>
          5948
          -
          5961
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableOnce.nonEmpty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$2._2.nonEmpty
        </td>
      </tr><tr>
        <td>
          150
        </td>
        <td>
          7748
        </td>
        <td>
          5872
          -
          5963
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ExpressionLookupResults.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          olambdaResults.copy[com.sparkutils.quality.Id](olambdaResults.lookupConstants.filter(((x$2: (com.sparkutils.quality.Id, Set[com.sparkutils.quality.impl.util.LookupType])) =&gt; x$2._2.nonEmpty)), olambdaResults.copy$default$2[Nothing])
        </td>
      </tr><tr>
        <td>
          150
        </td>
        <td>
          7747
        </td>
        <td>
          5887
          -
          5887
        </td>
        <td>
          TypeApply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ExpressionLookupResults.copy$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          olambdaResults.copy$default$2[Nothing]
        </td>
      </tr><tr>
        <td>
          150
        </td>
        <td>
          7746
        </td>
        <td>
          5910
          -
          5962
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableLike.filter
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          olambdaResults.lookupConstants.filter(((x$2: (com.sparkutils.quality.Id, Set[com.sparkutils.quality.impl.util.LookupType])) =&gt; x$2._2.nonEmpty))
        </td>
      </tr><tr>
        <td>
          152
        </td>
        <td>
          7751
        </td>
        <td>
          5994
          -
          6039
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ExpressionLookupResults.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ExpressionLookupResults.apply[com.sparkutils.quality.impl.util.RuleRow](scala.Predef.Map.empty[com.sparkutils.quality.impl.util.RuleRow, Nothing], scala.Predef.Set.empty[com.sparkutils.quality.impl.util.RuleRow])
        </td>
      </tr><tr>
        <td>
          152
        </td>
        <td>
          7750
        </td>
        <td>
          6029
          -
          6038
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.generic.ImmutableSetFactory.empty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.empty[com.sparkutils.quality.impl.util.RuleRow]
        </td>
      </tr><tr>
        <td>
          152
        </td>
        <td>
          7749
        </td>
        <td>
          6018
          -
          6027
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.Map.empty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Map.empty[com.sparkutils.quality.impl.util.RuleRow, Nothing]
        </td>
      </tr><tr>
        <td>
          152
        </td>
        <td>
          7752
        </td>
        <td>
          5969
          -
          6055
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.LookupResults.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          LookupResults.apply(ruleSuite, ExpressionLookupResults.apply[com.sparkutils.quality.impl.util.RuleRow](scala.Predef.Map.empty[com.sparkutils.quality.impl.util.RuleRow, Nothing], scala.Predef.Set.empty[com.sparkutils.quality.impl.util.RuleRow]), lambdaResults)
        </td>
      </tr><tr>
        <td>
          159
        </td>
        <td>
          7760
        </td>
        <td>
          6225
          -
          6225
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          159
        </td>
        <td>
          7759
        </td>
        <td>
          6250
          -
          6426
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  val init: () =&gt; T = TSLocal.this.initialValue;
  this.synchronized[Unit](TSLocal.this.threadLocal_=({
    final class $anon extends ThreadLocal[T] {
      def &lt;init&gt;(): &lt;$anon: ThreadLocal[T]&gt; = {
        $anon.super.&lt;init&gt;();
        ()
      };
      override def initialValue(): T = init.apply()
    };
    new $anon()
  }))
}
        </td>
      </tr><tr>
        <td>
          159
        </td>
        <td>
          7753
        </td>
        <td>
          6229
          -
          6248
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.eq
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          TSLocal.this.threadLocal.eq(null)
        </td>
      </tr><tr>
        <td>
          159
        </td>
        <td>
          7761
        </td>
        <td>
          6225
          -
          6225
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          160
        </td>
        <td>
          7754
        </td>
        <td>
          6269
          -
          6281
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.util.TSLocal.initialValue
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          TSLocal.this.initialValue
        </td>
      </tr><tr>
        <td>
          161
        </td>
        <td>
          7758
        </td>
        <td>
          6288
          -
          6420
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.synchronized
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.synchronized[Unit](TSLocal.this.threadLocal_=({
  final class $anon extends ThreadLocal[T] {
    def &lt;init&gt;(): &lt;$anon: ThreadLocal[T]&gt; = {
      $anon.super.&lt;init&gt;();
      ()
    };
    override def initialValue(): T = init.apply()
  };
  new $anon()
}))
        </td>
      </tr><tr>
        <td>
          163
        </td>
        <td>
          7757
        </td>
        <td>
          6317
          -
          6411
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.TSLocal.threadLocal_=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          TSLocal.this.threadLocal_=({
  final class $anon extends ThreadLocal[T] {
    def &lt;init&gt;(): &lt;$anon: ThreadLocal[T]&gt; = {
      $anon.super.&lt;init&gt;();
      ()
    };
    override def initialValue(): T = init.apply()
  };
  new $anon()
})
        </td>
      </tr><tr>
        <td>
          163
        </td>
        <td>
          7756
        </td>
        <td>
          6331
          -
          6334
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.TSLocal.$anon.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anon()
        </td>
      </tr><tr>
        <td>
          164
        </td>
        <td>
          7755
        </td>
        <td>
          6395
          -
          6401
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function0.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          init.apply()
        </td>
      </tr><tr>
        <td>
          169
        </td>
        <td>
          7762
        </td>
        <td>
          6431
          -
          6448
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.ThreadLocal.get
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          TSLocal.this.threadLocal.get()
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          7763
        </td>
        <td>
          6606
          -
          6616
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          TransientHolder.this.it.==(null)
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          7769
        </td>
        <td>
          6602
          -
          6602
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          7768
        </td>
        <td>
          6602
          -
          6602
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          7766
        </td>
        <td>
          6626
          -
          6681
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.synchronized
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.synchronized[Unit](TransientHolder.this.it_=(TransientHolder.this.initialise.apply()))
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          7767
        </td>
        <td>
          6626
          -
          6681
        </td>
        <td>
          Block
        </td>
        <td>
          java.lang.Object.synchronized
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          this.synchronized[Unit](TransientHolder.this.it_=(TransientHolder.this.initialise.apply()))
        </td>
      </tr><tr>
        <td>
          179
        </td>
        <td>
          7765
        </td>
        <td>
          6655
          -
          6672
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.TransientHolder.it_=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          TransientHolder.this.it_=(TransientHolder.this.initialise.apply())
        </td>
      </tr><tr>
        <td>
          179
        </td>
        <td>
          7764
        </td>
        <td>
          6660
          -
          6672
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function0.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          TransientHolder.this.initialise.apply()
        </td>
      </tr><tr>
        <td>
          183
        </td>
        <td>
          7770
        </td>
        <td>
          6692
          -
          6694
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.util.TransientHolder.it
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          TransientHolder.this.it
        </td>
      </tr><tr>
        <td>
          186
        </td>
        <td>
          7774
        </td>
        <td>
          6724
          -
          6781
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.synchronized
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          this.synchronized[Unit](TransientHolder.this.it_=(null.asInstanceOf[T]))
        </td>
      </tr><tr>
        <td>
          187
        </td>
        <td>
          7772
        </td>
        <td>
          6755
          -
          6775
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.asInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          null.asInstanceOf[T]
        </td>
      </tr><tr>
        <td>
          187
        </td>
        <td>
          7771
        </td>
        <td>
          6755
          -
          6759
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          null
        </td>
      </tr><tr>
        <td>
          187
        </td>
        <td>
          7773
        </td>
        <td>
          6750
          -
          6775
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.TransientHolder.it_=
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          TransientHolder.this.it_=(null.asInstanceOf[T])
        </td>
      </tr><tr>
        <td>
          197
        </td>
        <td>
          7775
        </td>
        <td>
          6989
          -
          7013
        </td>
        <td>
          Apply
        </td>
        <td>
          java.util.concurrent.atomic.AtomicBoolean.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new java.util.concurrent.atomic.AtomicBoolean(false)
        </td>
      </tr><tr>
        <td>
          203
        </td>
        <td>
          7776
        </td>
        <td>
          7224
          -
          7245
        </td>
        <td>
          Apply
        </td>
        <td>
          java.util.concurrent.atomic.AtomicBoolean.set
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          Testing.this.testingFlag.set(true)
        </td>
      </tr><tr>
        <td>
          206
        </td>
        <td>
          7777
        </td>
        <td>
          7267
          -
          7282
        </td>
        <td>
          Apply
        </td>
        <td>
          java.util.concurrent.atomic.AtomicBoolean.get
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          Testing.this.testingFlag.get()
        </td>
      </tr><tr>
        <td>
          212
        </td>
        <td>
          7779
        </td>
        <td>
          7450
          -
          7472
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  Testing.this.setTesting();
  thunk
}
        </td>
      </tr><tr>
        <td>
          213
        </td>
        <td>
          7778
        </td>
        <td>
          7450
          -
          7462
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.Testing.setTesting
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          Testing.this.setTesting()
        </td>
      </tr><tr>
        <td>
          216
        </td>
        <td>
          7781
        </td>
        <td>
          7491
          -
          7513
        </td>
        <td>
          Block
        </td>
        <td>
          java.util.concurrent.atomic.AtomicBoolean.set
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          Testing.this.testingFlag.set(false)
        </td>
      </tr><tr>
        <td>
          216
        </td>
        <td>
          7780
        </td>
        <td>
          7491
          -
          7513
        </td>
        <td>
          Apply
        </td>
        <td>
          java.util.concurrent.atomic.AtomicBoolean.set
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          Testing.this.testingFlag.set(false)
        </td>
      </tr><tr>
        <td>
          230
        </td>
        <td>
          7784
        </td>
        <td>
          7773
          -
          7802
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          left.==(null).&amp;&amp;(right.!=(null))
        </td>
      </tr><tr>
        <td>
          230
        </td>
        <td>
          7783
        </td>
        <td>
          7789
          -
          7802
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.!=
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          right.!=(null)
        </td>
      </tr><tr>
        <td>
          230
        </td>
        <td>
          7782
        </td>
        <td>
          7781
          -
          7785
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          null
        </td>
      </tr><tr>
        <td>
          231
        </td>
        <td>
          7786
        </td>
        <td>
          7810
          -
          7814
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          -100
        </td>
      </tr><tr>
        <td>
          231
        </td>
        <td>
          7785
        </td>
        <td>
          7810
          -
          7814
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          -100
        </td>
      </tr><tr>
        <td>
          233
        </td>
        <td>
          7795
        </td>
        <td>
          7830
          -
          7950
        </td>
        <td>
          If
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          if (right.==(null).&amp;&amp;(left.!=(null)))
  100
else
  left.asInstanceOf[T].compareTo(right.asInstanceOf[T])
        </td>
      </tr><tr>
        <td>
          233
        </td>
        <td>
          7789
        </td>
        <td>
          7834
          -
          7863
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          right.==(null).&amp;&amp;(left.!=(null))
        </td>
      </tr><tr>
        <td>
          233
        </td>
        <td>
          7788
        </td>
        <td>
          7851
          -
          7863
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.!=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          left.!=(null)
        </td>
      </tr><tr>
        <td>
          233
        </td>
        <td>
          7787
        </td>
        <td>
          7843
          -
          7847
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          null
        </td>
      </tr><tr>
        <td>
          234
        </td>
        <td>
          7790
        </td>
        <td>
          7873
          -
          7876
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          100
        </td>
      </tr><tr>
        <td>
          234
        </td>
        <td>
          7791
        </td>
        <td>
          7873
          -
          7876
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          100
        </td>
      </tr><tr>
        <td>
          236
        </td>
        <td>
          7793
        </td>
        <td>
          7896
          -
          7950
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Comparable.compareTo
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          left.asInstanceOf[T].compareTo(right.asInstanceOf[T])
        </td>
      </tr><tr>
        <td>
          236
        </td>
        <td>
          7792
        </td>
        <td>
          7928
          -
          7949
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.asInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          right.asInstanceOf[T]
        </td>
      </tr><tr>
        <td>
          236
        </td>
        <td>
          7794
        </td>
        <td>
          7896
          -
          7950
        </td>
        <td>
          Block
        </td>
        <td>
          java.lang.Comparable.compareTo
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          left.asInstanceOf[T].compareTo(right.asInstanceOf[T])
        </td>
      </tr><tr>
        <td>
          246
        </td>
        <td>
          7798
        </td>
        <td>
          8207
          -
          8236
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          left.==(null).&amp;&amp;(right.!=(null))
        </td>
      </tr><tr>
        <td>
          246
        </td>
        <td>
          7797
        </td>
        <td>
          8223
          -
          8236
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.!=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          right.!=(null)
        </td>
      </tr><tr>
        <td>
          246
        </td>
        <td>
          7796
        </td>
        <td>
          8215
          -
          8219
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          null
        </td>
      </tr><tr>
        <td>
          247
        </td>
        <td>
          7799
        </td>
        <td>
          8244
          -
          8248
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          -100
        </td>
      </tr><tr>
        <td>
          247
        </td>
        <td>
          7800
        </td>
        <td>
          8244
          -
          8248
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          -100
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          7802
        </td>
        <td>
          8285
          -
          8297
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.!=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          left.!=(null)
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          7810
        </td>
        <td>
          8264
          -
          8391
        </td>
        <td>
          If
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          if (right.==(null).&amp;&amp;(left.!=(null)))
  100
else
  ordering.compare(left.asInstanceOf[T], right.asInstanceOf[T])
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          7801
        </td>
        <td>
          8277
          -
          8281
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          null
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          7803
        </td>
        <td>
          8268
          -
          8297
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          right.==(null).&amp;&amp;(left.!=(null))
        </td>
      </tr><tr>
        <td>
          250
        </td>
        <td>
          7804
        </td>
        <td>
          8307
          -
          8310
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          100
        </td>
      </tr><tr>
        <td>
          250
        </td>
        <td>
          7805
        </td>
        <td>
          8307
          -
          8310
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          100
        </td>
      </tr><tr>
        <td>
          252
        </td>
        <td>
          7808
        </td>
        <td>
          8330
          -
          8391
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.math.Ordering.compare
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ordering.compare(left.asInstanceOf[T], right.asInstanceOf[T])
        </td>
      </tr><tr>
        <td>
          252
        </td>
        <td>
          7807
        </td>
        <td>
          8369
          -
          8390
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.asInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          right.asInstanceOf[T]
        </td>
      </tr><tr>
        <td>
          252
        </td>
        <td>
          7809
        </td>
        <td>
          8330
          -
          8391
        </td>
        <td>
          Block
        </td>
        <td>
          scala.math.Ordering.compare
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ordering.compare(left.asInstanceOf[T], right.asInstanceOf[T])
        </td>
      </tr><tr>
        <td>
          252
        </td>
        <td>
          7806
        </td>
        <td>
          8347
          -
          8367
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.asInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          left.asInstanceOf[T]
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          7811
        </td>
        <td>
          8486
          -
          8500
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.isEmpty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          option.isEmpty
        </td>
      </tr><tr>
        <td>
          259
        </td>
        <td>
          7813
        </td>
        <td>
          8508
          -
          8534
        </td>
        <td>
          Block
        </td>
        <td>
          java.util.Optional.empty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          java.util.Optional.empty[T]()
        </td>
      </tr><tr>
        <td>
          259
        </td>
        <td>
          7812
        </td>
        <td>
          8508
          -
          8534
        </td>
        <td>
          Apply
        </td>
        <td>
          java.util.Optional.empty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          java.util.Optional.empty[T]()
        </td>
      </tr><tr>
        <td>
          261
        </td>
        <td>
          7816
        </td>
        <td>
          8550
          -
          8583
        </td>
        <td>
          Block
        </td>
        <td>
          java.util.Optional.of
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          java.util.Optional.of[T](option.get)
        </td>
      </tr><tr>
        <td>
          261
        </td>
        <td>
          7815
        </td>
        <td>
          8550
          -
          8583
        </td>
        <td>
          Apply
        </td>
        <td>
          java.util.Optional.of
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          java.util.Optional.of[T](option.get)
        </td>
      </tr><tr>
        <td>
          261
        </td>
        <td>
          7814
        </td>
        <td>
          8572
          -
          8582
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.get
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          option.get
        </td>
      </tr><tr>
        <td>
          276
        </td>
        <td>
          7817
        </td>
        <td>
          8942
          -
          8977
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.ofDim
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Array.ofDim[T](array.numElements())(evidence$1)
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          7820
        </td>
        <td>
          8986
          -
          9040
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.util.ArrayData.foreach
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          array.foreach(dataType, ((i: Int, v: Any) =&gt; res.update(i, f.apply(v))))
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          7819
        </td>
        <td>
          9020
          -
          9039
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Array.update
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          res.update(i, f.apply(v))
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          7818
        </td>
        <td>
          9034
          -
          9038
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function1.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          f.apply(v)
        </td>
      </tr><tr>
        <td>
          279
        </td>
        <td>
          7822
        </td>
        <td>
          9084
          -
          9084
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.this.Array.canBuildFrom[T](evidence$1)
        </td>
      </tr><tr>
        <td>
          279
        </td>
        <td>
          7821
        </td>
        <td>
          9069
          -
          9080
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.util.ArrayData.array
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          array.array
        </td>
      </tr><tr>
        <td>
          279
        </td>
        <td>
          7823
        </td>
        <td>
          9069
          -
          9087
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.genericArrayOps[Any](array.array).map[T, Array[T]](f)(scala.this.Array.canBuildFrom[T](evidence$1))
        </td>
      </tr><tr>
        <td>
          291
        </td>
        <td>
          7825
        </td>
        <td>
          9336
          -
          9371
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          com.sparkutils.quality.impl.util.Arrays.mapArray
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          Arrays.this.mapArray[Any](array, dataType, {
  ((x: Any) =&gt; scala.Predef.identity[Any](x))
})((ClassTag.Any: scala.reflect.ClassTag[Any]))
        </td>
      </tr><tr>
        <td>
          291
        </td>
        <td>
          7824
        </td>
        <td>
          9362
          -
          9370
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Predef.identity
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.identity[Any](x)
        </td>
      </tr><tr>
        <td>
          292
        </td>
        <td>
          7826
        </td>
        <td>
          9388
          -
          9399
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.util.ArrayData.array
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          array.array
        </td>
      </tr><tr>
        <td>
          323
        </td>
        <td>
          7828
        </td>
        <td>
          10226
          -
          10236
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](ForceNullable.this.child)
        </td>
      </tr><tr>
        <td>
          323
        </td>
        <td>
          7827
        </td>
        <td>
          10230
          -
          10235
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.util.ForceNullable.child
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ForceNullable.this.child
        </td>
      </tr><tr>
        <td>
          325
        </td>
        <td>
          7829
        </td>
        <td>
          10273
          -
          10277
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          327
        </td>
        <td>
          7830
        </td>
        <td>
          10326
          -
          10343
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.eval
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ForceNullable.this.child.eval(input)
        </td>
      </tr><tr>
        <td>
          330
        </td>
        <td>
          7831
        </td>
        <td>
          10441
          -
          10459
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.genCode
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ForceNullable.this.child.genCode(ctx)
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          7833
        </td>
        <td>
          10474
          -
          10501
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.JavaCode.javaType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.codegen.JavaCode.javaType(ForceNullable.this.dataType)
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          7832
        </td>
        <td>
          10492
          -
          10500
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.util.ForceNullable.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ForceNullable.this.dataType
        </td>
      </tr><tr>
        <td>
          332
        </td>
        <td>
          7835
        </td>
        <td>
          10518
          -
          10546
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.JavaCode.boxedType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.codegen.JavaCode.boxedType(ForceNullable.this.dataType)
        </td>
      </tr><tr>
        <td>
          332
        </td>
        <td>
          7834
        </td>
        <td>
          10537
          -
          10545
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.util.ForceNullable.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ForceNullable.this.dataType
        </td>
      </tr><tr>
        <td>
          333
        </td>
        <td>
          7846
        </td>
        <td>
          10554
          -
          10554
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.copy$default$3
        </td>
      </tr><tr>
        <td>
          333
        </td>
        <td>
          7845
        </td>
        <td>
          10554
          -
          10554
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.copy$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.copy$default$2
        </td>
      </tr><tr>
        <td>
          333
        </td>
        <td>
          7847
        </td>
        <td>
          10551
          -
          10833
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.copy(org.apache.spark.sql.catalyst.expressions.codegen.Block.BlockHelper(scala.StringContext.apply(&quot;\n            &quot;, &quot;\n            boolean &quot;, &quot; = true;\n            &quot;, &quot; &quot;, &quot; = null;\n            if (&quot;, &quot; != null) {\n              &quot;, &quot; = false;\n              &quot;, &quot; = (&quot;, &quot;) &quot;, &quot;;\n            }\n            &quot;)).code(c.code, ev.isNull, typ, ev.value, c.value, ev.isNull, ev.value, boxed, c.value), ev.copy$default$2, ev.copy$default$3)
        </td>
      </tr><tr>
        <td>
          334
        </td>
        <td>
          7844
        </td>
        <td>
          10572
          -
          10832
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.Block.BlockHelper.code
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.codegen.Block.BlockHelper(scala.StringContext.apply(&quot;\n            &quot;, &quot;\n            boolean &quot;, &quot; = true;\n            &quot;, &quot; &quot;, &quot; = null;\n            if (&quot;, &quot; != null) {\n              &quot;, &quot; = false;\n              &quot;, &quot; = (&quot;, &quot;) &quot;, &quot;;\n            }\n            &quot;)).code(c.code, ev.isNull, typ, ev.value, c.value, ev.isNull, ev.value, boxed, c.value)
        </td>
      </tr><tr>
        <td>
          334
        </td>
        <td>
          7836
        </td>
        <td>
          10572
          -
          10832
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;\n            &quot;, &quot;\n            boolean &quot;, &quot; = true;\n            &quot;, &quot; &quot;, &quot; = null;\n            if (&quot;, &quot; != null) {\n              &quot;, &quot; = false;\n              &quot;, &quot; = (&quot;, &quot;) &quot;, &quot;;\n            }\n            &quot;)
        </td>
      </tr><tr>
        <td>
          335
        </td>
        <td>
          7837
        </td>
        <td>
          10594
          -
          10600
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.code
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          c.code
        </td>
      </tr><tr>
        <td>
          336
        </td>
        <td>
          7838
        </td>
        <td>
          10624
          -
          10633
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.isNull
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.isNull
        </td>
      </tr><tr>
        <td>
          337
        </td>
        <td>
          7839
        </td>
        <td>
          10662
          -
          10670
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.value
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.value
        </td>
      </tr><tr>
        <td>
          338
        </td>
        <td>
          7840
        </td>
        <td>
          10698
          -
          10705
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.value
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          c.value
        </td>
      </tr><tr>
        <td>
          339
        </td>
        <td>
          7841
        </td>
        <td>
          10734
          -
          10743
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.isNull
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.isNull
        </td>
      </tr><tr>
        <td>
          340
        </td>
        <td>
          7843
        </td>
        <td>
          10793
          -
          10800
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.value
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          c.value
        </td>
      </tr><tr>
        <td>
          340
        </td>
        <td>
          7842
        </td>
        <td>
          10770
          -
          10778
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.value
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.value
        </td>
      </tr><tr>
        <td>
          346
        </td>
        <td>
          7848
        </td>
        <td>
          10876
          -
          10890
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ForceNullable.this.child.dataType
        </td>
      </tr><tr>
        <td>
          349
        </td>
        <td>
          7849
        </td>
        <td>
          11000
          -
          11016
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          newChildren.head
        </td>
      </tr><tr>
        <td>
          349
        </td>
        <td>
          7850
        </td>
        <td>
          10987
          -
          11017
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ForceNullable.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ForceNullable.this.copy(newChildren.head)
        </td>
      </tr><tr>
        <td>
          364
        </td>
        <td>
          7852
        </td>
        <td>
          11368
          -
          11419
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.expressionEncoder
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.ShimUtils.expressionEncoder[T](scala.Predef.implicitly[org.apache.spark.sql.Encoder[T]](evidence$2))
        </td>
      </tr><tr>
        <td>
          364
        </td>
        <td>
          7851
        </td>
        <td>
          11396
          -
          11418
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Predef.implicitly
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.implicitly[org.apache.spark.sql.Encoder[T]](evidence$2)
        </td>
      </tr><tr>
        <td>
          366
        </td>
        <td>
          7853
        </td>
        <td>
          11446
          -
          11458
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.clsTag
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          oexpr.clsTag
        </td>
      </tr><tr>
        <td>
          368
        </td>
        <td>
          7969
        </td>
        <td>
          11464
          -
          11467
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.Encoding.$anon.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anon()
        </td>
      </tr><tr>
        <td>
          370
        </td>
        <td>
          7854
        </td>
        <td>
          11526
          -
          11530
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          372
        </td>
        <td>
          7855
        </td>
        <td>
          11571
          -
          11598
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          oexpr.deserializer.dataType
        </td>
      </tr><tr>
        <td>
          375
        </td>
        <td>
          7856
        </td>
        <td>
          11663
          -
          11679
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.serializer
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          oexpr.serializer
        </td>
      </tr><tr>
        <td>
          376
        </td>
        <td>
          7857
        </td>
        <td>
          11692
          -
          11706
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          se.length.==(1)
        </td>
      </tr><tr>
        <td>
          377
        </td>
        <td>
          7858
        </td>
        <td>
          11718
          -
          11734
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          se.head.dataType
        </td>
      </tr><tr>
        <td>
          377
        </td>
        <td>
          7859
        </td>
        <td>
          11718
          -
          11734
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          se.head.dataType
        </td>
      </tr><tr>
        <td>
          379
        </td>
        <td>
          7867
        </td>
        <td>
          11758
          -
          11906
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.StructType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructType.apply(se.map[org.apache.spark.sql.types.StructField, Seq[org.apache.spark.sql.types.StructField]](((n: org.apache.spark.sql.catalyst.expressions.NamedExpression) =&gt; org.apache.spark.sql.types.StructField.apply(n.asInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedExpression].qualifiedName, n.dataType, n.nullable, org.apache.spark.sql.types.StructField.apply$default$4)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.types.StructField]))
        </td>
      </tr><tr>
        <td>
          379
        </td>
        <td>
          7868
        </td>
        <td>
          11758
          -
          11906
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.types.StructType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructType.apply(se.map[org.apache.spark.sql.types.StructField, Seq[org.apache.spark.sql.types.StructField]](((n: org.apache.spark.sql.catalyst.expressions.NamedExpression) =&gt; org.apache.spark.sql.types.StructField.apply(n.asInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedExpression].qualifiedName, n.dataType, n.nullable, org.apache.spark.sql.types.StructField.apply$default$4)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.types.StructField]))
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          7862
        </td>
        <td>
          11882
          -
          11892
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.nullable
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          n.nullable
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          7864
        </td>
        <td>
          11811
          -
          11893
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.StructField.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructField.apply(n.asInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedExpression].qualifiedName, n.dataType, n.nullable, org.apache.spark.sql.types.StructField.apply$default$4)
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          7861
        </td>
        <td>
          11870
          -
          11880
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          n.dataType
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          7860
        </td>
        <td>
          11823
          -
          11868
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.NamedExpression.qualifiedName
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          n.asInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedExpression].qualifiedName
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          7863
        </td>
        <td>
          11811
          -
          11811
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.apply$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructField.apply$default$4
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          7866
        </td>
        <td>
          11799
          -
          11894
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          se.map[org.apache.spark.sql.types.StructField, Seq[org.apache.spark.sql.types.StructField]](((n: org.apache.spark.sql.catalyst.expressions.NamedExpression) =&gt; org.apache.spark.sql.types.StructField.apply(n.asInstanceOf[org.apache.spark.sql.catalyst.expressions.NamedExpression].qualifiedName, n.dataType, n.nullable, org.apache.spark.sql.types.StructField.apply$default$4)))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.types.StructField])
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          7865
        </td>
        <td>
          11805
          -
          11805
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.types.StructField]
        </td>
      </tr><tr>
        <td>
          385
        </td>
        <td>
          7869
        </td>
        <td>
          11999
          -
          12017
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.deserializer
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          oexpr.deserializer
        </td>
      </tr><tr>
        <td>
          389
        </td>
        <td>
          7870
        </td>
        <td>
          12098
          -
          12105
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Alias.child
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.child
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          7871
        </td>
        <td>
          12187
          -
          12187
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.UnresolvedMapObjects.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy$default$1
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          7873
        </td>
        <td>
          12185
          -
          12205
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.UnresolvedMapObjects.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy(x$2, x$1, x$3)
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          7872
        </td>
        <td>
          12187
          -
          12187
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.UnresolvedMapObjects.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy$default$3
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          7875
        </td>
        <td>
          12162
          -
          12208
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.withNewChildren(scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.objects.UnresolvedMapObjects]({
  &lt;artifact&gt; val x$1: org.apache.spark.sql.catalyst.expressions.Expression = path;
  &lt;artifact&gt; val x$2: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression @scala.reflect.internal.annotations.uncheckedBounds = m.copy$default$1;
  &lt;artifact&gt; val x$3: Option[Class[_]] @scala.reflect.internal.annotations.uncheckedBounds = m.copy$default$3;
  m.copy(x$2, x$1, x$3)
}))
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          7874
        </td>
        <td>
          12180
          -
          12207
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.objects.UnresolvedMapObjects]({
  &lt;artifact&gt; val x$1: org.apache.spark.sql.catalyst.expressions.Expression = path;
  &lt;artifact&gt; val x$2: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression @scala.reflect.internal.annotations.uncheckedBounds = m.copy$default$1;
  &lt;artifact&gt; val x$3: Option[Class[_]] @scala.reflect.internal.annotations.uncheckedBounds = m.copy$default$3;
  m.copy(x$2, x$1, x$3)
})
        </td>
      </tr><tr>
        <td>
          391
        </td>
        <td>
          7876
        </td>
        <td>
          12249
          -
          12249
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.Encoding.$anon.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          391
        </td>
        <td>
          7877
        </td>
        <td>
          12235
          -
          12341
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.transformUp(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x1: A1, default: A1 =&gt; B1): B1 = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (_: org.apache.spark.sql.catalyst.analysis.GetColumnByOrdinal) =&gt; path
      case (defaultCase$ @ _) =&gt; default.apply(x1)
    };
    final def isDefinedAt(x1: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (_: org.apache.spark.sql.catalyst.analysis.GetColumnByOrdinal) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          396
        </td>
        <td>
          7880
        </td>
        <td>
          12402
          -
          12422
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.UnresolvedMapObjects.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy(x$5, x$4, x$6)
        </td>
      </tr><tr>
        <td>
          396
        </td>
        <td>
          7879
        </td>
        <td>
          12404
          -
          12404
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.UnresolvedMapObjects.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy$default$3
        </td>
      </tr><tr>
        <td>
          396
        </td>
        <td>
          7878
        </td>
        <td>
          12404
          -
          12404
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.UnresolvedMapObjects.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy$default$1
        </td>
      </tr><tr>
        <td>
          398
        </td>
        <td>
          7882
        </td>
        <td>
          12547
          -
          12558
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Predef.ArrowAssoc.-&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.ArrowAssoc[String](e.name).-&gt;[Int](i)
        </td>
      </tr><tr>
        <td>
          398
        </td>
        <td>
          7885
        </td>
        <td>
          12480
          -
          12566
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableOnce.toMap
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          outputType.asInstanceOf[org.apache.spark.sql.types.StructType].zipWithIndex[org.apache.spark.sql.types.StructField, Seq[(org.apache.spark.sql.types.StructField, Int)]](collection.this.Seq.canBuildFrom[(org.apache.spark.sql.types.StructField, Int)]).map[(String, Int), Seq[(String, Int)]](((x0$1: (org.apache.spark.sql.types.StructField, Int)) =&gt; x0$1 match {
  case (_1: org.apache.spark.sql.types.StructField, _2: Int)(org.apache.spark.sql.types.StructField, Int)((e @ _), (i @ _)) =&gt; scala.Predef.ArrowAssoc[String](e.name).-&gt;[Int](i)
}))(collection.this.Seq.canBuildFrom[(String, Int)]).toMap[String, Int](scala.Predef.$conforms[(String, Int)])
        </td>
      </tr><tr>
        <td>
          398
        </td>
        <td>
          7884
        </td>
        <td>
          12561
          -
          12561
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Predef.$conforms
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.$conforms[(String, Int)]
        </td>
      </tr><tr>
        <td>
          398
        </td>
        <td>
          7881
        </td>
        <td>
          12516
          -
          12516
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[(org.apache.spark.sql.types.StructField, Int)]
        </td>
      </tr><tr>
        <td>
          398
        </td>
        <td>
          7883
        </td>
        <td>
          12532
          -
          12532
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[(String, Int)]
        </td>
      </tr><tr>
        <td>
          400
        </td>
        <td>
          7888
        </td>
        <td>
          12614
          -
          12627
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Literal.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Literal.apply(null)
        </td>
      </tr><tr>
        <td>
          400
        </td>
        <td>
          7900
        </td>
        <td>
          12582
          -
          12894
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.If.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.If.apply(org.apache.spark.sql.catalyst.expressions.IsNull.apply(ForceNullable.apply(path)), org.apache.spark.sql.catalyst.expressions.Literal.apply(null), n.withNewChildren(n.children.map[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](((x$3: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; x$3.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x2: A1, default: A1 =&gt; B1): B1 = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; com.sparkutils.shim.expressions.GetStructField3.apply(path, o.apply(u.name), com.sparkutils.shim.expressions.GetStructField3.apply$default$3)
      case (defaultCase$ @ _) =&gt; default.apply(x2)
    };
    final def isDefinedAt(x2: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression])))
        </td>
      </tr><tr>
        <td>
          400
        </td>
        <td>
          7887
        </td>
        <td>
          12585
          -
          12612
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.IsNull.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.IsNull.apply(ForceNullable.apply(path))
        </td>
      </tr><tr>
        <td>
          400
        </td>
        <td>
          7886
        </td>
        <td>
          12592
          -
          12611
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ForceNullable.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ForceNullable.apply(path)
        </td>
      </tr><tr>
        <td>
          401
        </td>
        <td>
          7898
        </td>
        <td>
          12663
          -
          12859
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          n.children.map[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](((x$3: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; x$3.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x2: A1, default: A1 =&gt; B1): B1 = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; com.sparkutils.shim.expressions.GetStructField3.apply(path, o.apply(u.name), com.sparkutils.shim.expressions.GetStructField3.apply$default$3)
      case (defaultCase$ @ _) =&gt; default.apply(x2)
    };
    final def isDefinedAt(x2: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression])
        </td>
      </tr><tr>
        <td>
          401
        </td>
        <td>
          7897
        </td>
        <td>
          12674
          -
          12674
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]
        </td>
      </tr><tr>
        <td>
          401
        </td>
        <td>
          7899
        </td>
        <td>
          12645
          -
          12878
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          n.withNewChildren(n.children.map[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](((x$3: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; x$3.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x2: A1, default: A1 =&gt; B1): B1 = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; com.sparkutils.shim.expressions.GetStructField3.apply(path, o.apply(u.name), com.sparkutils.shim.expressions.GetStructField3.apply$default$3)
      case (defaultCase$ @ _) =&gt; default.apply(x2)
    };
    final def isDefinedAt(x2: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          402
        </td>
        <td>
          7896
        </td>
        <td>
          12698
          -
          12859
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.transform
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$3.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x2: A1, default: A1 =&gt; B1): B1 = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; com.sparkutils.shim.expressions.GetStructField3.apply(path, o.apply(u.name), com.sparkutils.shim.expressions.GetStructField3.apply$default$3)
      case (defaultCase$ @ _) =&gt; default.apply(x2)
    };
    final def isDefinedAt(x2: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          402
        </td>
        <td>
          7895
        </td>
        <td>
          12710
          -
          12710
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.Encoding.$anon.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          403
        </td>
        <td>
          7889
        </td>
        <td>
          12774
          -
          12780
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          u.name
        </td>
      </tr><tr>
        <td>
          403
        </td>
        <td>
          7890
        </td>
        <td>
          12763
          -
          12781
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.contains
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          o.contains(u.name)
        </td>
      </tr><tr>
        <td>
          404
        </td>
        <td>
          7891
        </td>
        <td>
          12831
          -
          12837
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          u.name
        </td>
      </tr><tr>
        <td>
          404
        </td>
        <td>
          7894
        </td>
        <td>
          12807
          -
          12839
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.shim.expressions.GetStructField3.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.shim.expressions.GetStructField3.apply(path, o.apply(u.name), com.sparkutils.shim.expressions.GetStructField3.apply$default$3)
        </td>
      </tr><tr>
        <td>
          404
        </td>
        <td>
          7893
        </td>
        <td>
          12807
          -
          12807
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.shim.expressions.GetStructField3.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.shim.expressions.GetStructField3.apply$default$3
        </td>
      </tr><tr>
        <td>
          404
        </td>
        <td>
          7892
        </td>
        <td>
          12829
          -
          12838
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          o.apply(u.name)
        </td>
      </tr><tr>
        <td>
          409
        </td>
        <td>
          7903
        </td>
        <td>
          13011
          -
          13011
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[(String, Int)]
        </td>
      </tr><tr>
        <td>
          409
        </td>
        <td>
          7902
        </td>
        <td>
          13026
          -
          13037
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Predef.ArrowAssoc.-&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.ArrowAssoc[String](e.name).-&gt;[Int](i)
        </td>
      </tr><tr>
        <td>
          409
        </td>
        <td>
          7905
        </td>
        <td>
          12959
          -
          13045
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableOnce.toMap
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          outputType.asInstanceOf[org.apache.spark.sql.types.StructType].zipWithIndex[org.apache.spark.sql.types.StructField, Seq[(org.apache.spark.sql.types.StructField, Int)]](collection.this.Seq.canBuildFrom[(org.apache.spark.sql.types.StructField, Int)]).map[(String, Int), Seq[(String, Int)]](((x0$2: (org.apache.spark.sql.types.StructField, Int)) =&gt; x0$2 match {
  case (_1: org.apache.spark.sql.types.StructField, _2: Int)(org.apache.spark.sql.types.StructField, Int)((e @ _), (i @ _)) =&gt; scala.Predef.ArrowAssoc[String](e.name).-&gt;[Int](i)
}))(collection.this.Seq.canBuildFrom[(String, Int)]).toMap[String, Int](scala.Predef.$conforms[(String, Int)])
        </td>
      </tr><tr>
        <td>
          409
        </td>
        <td>
          7901
        </td>
        <td>
          12995
          -
          12995
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[(org.apache.spark.sql.types.StructField, Int)]
        </td>
      </tr><tr>
        <td>
          409
        </td>
        <td>
          7904
        </td>
        <td>
          13040
          -
          13040
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Predef.$conforms
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.$conforms[(String, Int)]
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          7906
        </td>
        <td>
          13071
          -
          13090
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ForceNullable.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ForceNullable.apply(path)
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          7923
        </td>
        <td>
          13061
          -
          13431
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.If.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.If.apply(org.apache.spark.sql.catalyst.expressions.IsNull.apply(ForceNullable.apply(path)), org.apache.spark.sql.catalyst.expressions.Literal.apply(null), {
  &lt;artifact&gt; val x$7: Map[String,org.apache.spark.sql.catalyst.expressions.Expression] @scala.reflect.internal.annotations.uncheckedBounds = i.setters.map[(String, org.apache.spark.sql.catalyst.expressions.Expression), Map[String,org.apache.spark.sql.catalyst.expressions.Expression]](((p: (String, org.apache.spark.sql.catalyst.expressions.Expression)) =&gt; scala.Tuple2.apply[String, org.apache.spark.sql.catalyst.expressions.Expression](p._1, p._2.transform(({
    @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
      def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
        $anonfun.super.&lt;init&gt;();
        ()
      };
      final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x3: A1, default: A1 =&gt; B1): B1 = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
        case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; com.sparkutils.shim.expressions.GetStructField3.apply(path, o.apply(u.name), com.sparkutils.shim.expressions.GetStructField3.apply$default$3)
        case (defaultCase$ @ _) =&gt; default.apply(x3)
      };
      final def isDefinedAt(x3: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
        case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; true
        case (defaultCase$ @ _) =&gt; false
      }
    };
    new $anonfun()
  }: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression])))))(immutable.this.Map.canBuildFrom[String, org.apache.spark.sql.catalyst.expressions.Expression]);
  &lt;artifact&gt; val x$8: org.apache.spark.sql.catalyst.expressions.Expression = i.copy$default$1;
  i.copy(x$8, x$7)
})
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          7908
        </td>
        <td>
          13093
          -
          13106
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Literal.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Literal.apply(null)
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          7907
        </td>
        <td>
          13064
          -
          13091
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.IsNull.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.IsNull.apply(ForceNullable.apply(path))
        </td>
      </tr><tr>
        <td>
          412
        </td>
        <td>
          7921
        </td>
        <td>
          13126
          -
          13126
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.InitializeJavaBean.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.copy$default$1
        </td>
      </tr><tr>
        <td>
          412
        </td>
        <td>
          7922
        </td>
        <td>
          13124
          -
          13415
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.InitializeJavaBean.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.copy(x$8, x$7)
        </td>
      </tr><tr>
        <td>
          413
        </td>
        <td>
          7920
        </td>
        <td>
          13159
          -
          13397
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.setters.map[(String, org.apache.spark.sql.catalyst.expressions.Expression), Map[String,org.apache.spark.sql.catalyst.expressions.Expression]](((p: (String, org.apache.spark.sql.catalyst.expressions.Expression)) =&gt; scala.Tuple2.apply[String, org.apache.spark.sql.catalyst.expressions.Expression](p._1, p._2.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x3: A1, default: A1 =&gt; B1): B1 = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; com.sparkutils.shim.expressions.GetStructField3.apply(path, o.apply(u.name), com.sparkutils.shim.expressions.GetStructField3.apply$default$3)
      case (defaultCase$ @ _) =&gt; default.apply(x3)
    };
    final def isDefinedAt(x3: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression])))))(immutable.this.Map.canBuildFrom[String, org.apache.spark.sql.catalyst.expressions.Expression])
        </td>
      </tr><tr>
        <td>
          413
        </td>
        <td>
          7919
        </td>
        <td>
          13172
          -
          13172
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.Map.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          immutable.this.Map.canBuildFrom[String, org.apache.spark.sql.catalyst.expressions.Expression]
        </td>
      </tr><tr>
        <td>
          414
        </td>
        <td>
          7909
        </td>
        <td>
          13200
          -
          13204
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          p._1
        </td>
      </tr><tr>
        <td>
          414
        </td>
        <td>
          7918
        </td>
        <td>
          13199
          -
          13377
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[String, org.apache.spark.sql.catalyst.expressions.Expression](p._1, p._2.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x3: A1, default: A1 =&gt; B1): B1 = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; com.sparkutils.shim.expressions.GetStructField3.apply(path, o.apply(u.name), com.sparkutils.shim.expressions.GetStructField3.apply$default$3)
      case (defaultCase$ @ _) =&gt; default.apply(x3)
    };
    final def isDefinedAt(x3: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression])))
        </td>
      </tr><tr>
        <td>
          414
        </td>
        <td>
          7917
        </td>
        <td>
          13206
          -
          13376
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.transform
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          p._2.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x3: A1, default: A1 =&gt; B1): B1 = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; com.sparkutils.shim.expressions.GetStructField3.apply(path, o.apply(u.name), com.sparkutils.shim.expressions.GetStructField3.apply$default$3)
      case (defaultCase$ @ _) =&gt; default.apply(x3)
    };
    final def isDefinedAt(x3: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (u @ (_: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)) if o.contains(u.name) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          414
        </td>
        <td>
          7916
        </td>
        <td>
          13221
          -
          13221
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.Encoding.$anon.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          415
        </td>
        <td>
          7911
        </td>
        <td>
          13276
          -
          13294
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.contains
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          o.contains(u.name)
        </td>
      </tr><tr>
        <td>
          415
        </td>
        <td>
          7910
        </td>
        <td>
          13287
          -
          13293
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          u.name
        </td>
      </tr><tr>
        <td>
          416
        </td>
        <td>
          7915
        </td>
        <td>
          13322
          -
          13354
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.shim.expressions.GetStructField3.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.shim.expressions.GetStructField3.apply(path, o.apply(u.name), com.sparkutils.shim.expressions.GetStructField3.apply$default$3)
        </td>
      </tr><tr>
        <td>
          416
        </td>
        <td>
          7912
        </td>
        <td>
          13346
          -
          13352
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          u.name
        </td>
      </tr><tr>
        <td>
          416
        </td>
        <td>
          7914
        </td>
        <td>
          13322
          -
          13322
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.shim.expressions.GetStructField3.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.shim.expressions.GetStructField3.apply$default$3
        </td>
      </tr><tr>
        <td>
          416
        </td>
        <td>
          7913
        </td>
        <td>
          13344
          -
          13353
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          o.apply(u.name)
        </td>
      </tr><tr>
        <td>
          423
        </td>
        <td>
          7924
        </td>
        <td>
          13537
          -
          13537
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.Encoding.$anon.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          423
        </td>
        <td>
          7925
        </td>
        <td>
          13523
          -
          13623
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.transformUp(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x4: A1, default: A1 =&gt; B1): B1 = ((x4.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (_: org.apache.spark.sql.catalyst.analysis.GetColumnByOrdinal) =&gt; path
      case (defaultCase$ @ _) =&gt; default.apply(x4)
    };
    final def isDefinedAt(x4: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x4.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (_: org.apache.spark.sql.catalyst.analysis.GetColumnByOrdinal) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          427
        </td>
        <td>
          7927
        </td>
        <td>
          13646
          -
          13740
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.transformUp(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x5: A1, default: A1 =&gt; B1): B1 = ((x5.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (_: org.apache.spark.sql.catalyst.analysis.GetColumnByOrdinal) =&gt; path
      case (defaultCase$ @ _) =&gt; default.apply(x5)
    };
    final def isDefinedAt(x5: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x5.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (_: org.apache.spark.sql.catalyst.analysis.GetColumnByOrdinal) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          427
        </td>
        <td>
          7926
        </td>
        <td>
          13660
          -
          13660
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.Encoding.$anon.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          438
        </td>
        <td>
          7928
        </td>
        <td>
          13891
          -
          13907
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.serializer
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          oexpr.serializer
        </td>
      </tr><tr>
        <td>
          439
        </td>
        <td>
          7929
        </td>
        <td>
          13920
          -
          13934
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          se.length.==(1)
        </td>
      </tr><tr>
        <td>
          440
        </td>
        <td>
          7930
        </td>
        <td>
          13946
          -
          13953
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          se.head
        </td>
      </tr><tr>
        <td>
          440
        </td>
        <td>
          7946
        </td>
        <td>
          13946
          -
          14402
        </td>
        <td>
          Match
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          se.head match {
  case (a @ (_: org.apache.spark.sql.catalyst.expressions.Alias)) =&gt; a.child match {
    case (m @ (_: org.apache.spark.sql.catalyst.expressions.objects.MapObjects)) =&gt; a.withNewChildren(scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.objects.MapObjects]({
      &lt;artifact&gt; val x$1: org.apache.spark.sql.catalyst.expressions.Expression = path;
      &lt;artifact&gt; val x$2: org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable = m.copy$default$1;
      &lt;artifact&gt; val x$3: org.apache.spark.sql.catalyst.expressions.Expression = m.copy$default$2;
      &lt;artifact&gt; val x$4: Option[Class[_]] @scala.reflect.internal.annotations.uncheckedBounds = m.copy$default$4;
      m.copy(x$2, x$3, x$1, x$4)
    }))
    case (a @ _) =&gt; a.transformUp(({
      @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
        def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
          $anonfun.super.&lt;init&gt;();
          ()
        };
        final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x1: A1, default: A1 =&gt; B1): B1 = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
          case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; path
          case (defaultCase$ @ _) =&gt; default.apply(x1)
        };
        final def isDefinedAt(x1: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
          case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; true
          case (defaultCase$ @ _) =&gt; false
        }
      };
      new $anonfun()
    }: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
  }
  case (m @ (_: org.apache.spark.sql.catalyst.expressions.objects.MapObjects)) =&gt; {
    &lt;artifact&gt; val x$5: org.apache.spark.sql.catalyst.expressions.Expression = path;
    &lt;artifact&gt; val x$6: org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable = m.copy$default$1;
    &lt;artifact&gt; val x$7: org.apache.spark.sql.catalyst.expressions.Expression = m.copy$default$2;
    &lt;artifact&gt; val x$8: Option[Class[_]] @scala.reflect.internal.annotations.uncheckedBounds = m.copy$default$4;
    m.copy(x$6, x$7, x$5, x$8)
  }
  case (a @ _) =&gt; a.transformUp(({
    @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
      def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
        $anonfun.super.&lt;init&gt;();
        ()
      };
      final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x2: A1, default: A1 =&gt; B1): B1 = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
        case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; path
        case (defaultCase$ @ _) =&gt; default.apply(x2)
      };
      final def isDefinedAt(x2: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
        case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; true
        case (defaultCase$ @ _) =&gt; false
      }
    };
    new $anonfun()
  }: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
}
        </td>
      </tr><tr>
        <td>
          442
        </td>
        <td>
          7931
        </td>
        <td>
          14005
          -
          14012
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Alias.child
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.child
        </td>
      </tr><tr>
        <td>
          443
        </td>
        <td>
          7933
        </td>
        <td>
          14084
          -
          14084
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.MapObjects.copy$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy$default$2
        </td>
      </tr><tr>
        <td>
          443
        </td>
        <td>
          7936
        </td>
        <td>
          14077
          -
          14108
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.objects.MapObjects]({
  &lt;artifact&gt; val x$1: org.apache.spark.sql.catalyst.expressions.Expression = path;
  &lt;artifact&gt; val x$2: org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable = m.copy$default$1;
  &lt;artifact&gt; val x$3: org.apache.spark.sql.catalyst.expressions.Expression = m.copy$default$2;
  &lt;artifact&gt; val x$4: Option[Class[_]] @scala.reflect.internal.annotations.uncheckedBounds = m.copy$default$4;
  m.copy(x$2, x$3, x$1, x$4)
})
        </td>
      </tr><tr>
        <td>
          443
        </td>
        <td>
          7932
        </td>
        <td>
          14084
          -
          14084
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.MapObjects.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy$default$1
        </td>
      </tr><tr>
        <td>
          443
        </td>
        <td>
          7935
        </td>
        <td>
          14082
          -
          14106
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.MapObjects.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy(x$2, x$3, x$1, x$4)
        </td>
      </tr><tr>
        <td>
          443
        </td>
        <td>
          7934
        </td>
        <td>
          14084
          -
          14084
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.MapObjects.copy$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy$default$4
        </td>
      </tr><tr>
        <td>
          443
        </td>
        <td>
          7937
        </td>
        <td>
          14059
          -
          14109
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.withNewChildren(scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.objects.MapObjects]({
  &lt;artifact&gt; val x$1: org.apache.spark.sql.catalyst.expressions.Expression = path;
  &lt;artifact&gt; val x$2: org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable = m.copy$default$1;
  &lt;artifact&gt; val x$3: org.apache.spark.sql.catalyst.expressions.Expression = m.copy$default$2;
  &lt;artifact&gt; val x$4: Option[Class[_]] @scala.reflect.internal.annotations.uncheckedBounds = m.copy$default$4;
  m.copy(x$2, x$3, x$1, x$4)
}))
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          7939
        </td>
        <td>
          14136
          -
          14218
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.transformUp(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x1: A1, default: A1 =&gt; B1): B1 = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; path
      case (defaultCase$ @ _) =&gt; default.apply(x1)
    };
    final def isDefinedAt(x1: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          7938
        </td>
        <td>
          14150
          -
          14150
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.Encoding.$anon.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          448
        </td>
        <td>
          7942
        </td>
        <td>
          14271
          -
          14271
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.MapObjects.copy$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy$default$4
        </td>
      </tr><tr>
        <td>
          448
        </td>
        <td>
          7941
        </td>
        <td>
          14271
          -
          14271
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.MapObjects.copy$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy$default$2
        </td>
      </tr><tr>
        <td>
          448
        </td>
        <td>
          7943
        </td>
        <td>
          14269
          -
          14293
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.MapObjects.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy(x$6, x$7, x$5, x$8)
        </td>
      </tr><tr>
        <td>
          448
        </td>
        <td>
          7940
        </td>
        <td>
          14271
          -
          14271
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.objects.MapObjects.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          m.copy$default$1
        </td>
      </tr><tr>
        <td>
          449
        </td>
        <td>
          7945
        </td>
        <td>
          14316
          -
          14390
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.transformUp(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x2: A1, default: A1 =&gt; B1): B1 = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; path
      case (defaultCase$ @ _) =&gt; default.apply(x2)
    };
    final def isDefinedAt(x2: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x2.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          449
        </td>
        <td>
          7944
        </td>
        <td>
          14330
          -
          14330
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.Encoding.$anon.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          453
        </td>
        <td>
          7968
        </td>
        <td>
          14416
          -
          14881
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  val o: org.apache.spark.sql.types.StructType = outputType.asInstanceOf[org.apache.spark.sql.types.StructType];
  val dealiased: scala.collection.immutable.Map[String,org.apache.spark.sql.catalyst.expressions.Expression] = se.map[(String, org.apache.spark.sql.catalyst.expressions.Expression), Seq[(String, org.apache.spark.sql.catalyst.expressions.Expression)]](((x0$1: org.apache.spark.sql.catalyst.expressions.NamedExpression) =&gt; x0$1 match {
  case (a @ (_: org.apache.spark.sql.catalyst.expressions.Alias)) =&gt; scala.Predef.ArrowAssoc[String](a.name).-&gt;[org.apache.spark.sql.catalyst.expressions.Expression](a.child.transformUp(({
    @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
      def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
        $anonfun.super.&lt;init&gt;();
        ()
      };
      final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x3: A1, default: A1 =&gt; B1): B1 = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
        case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; path
        case (defaultCase$ @ _) =&gt; default.apply(x3)
      };
      final def isDefinedAt(x3: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
        case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; true
        case (defaultCase$ @ _) =&gt; false
      }
    };
    new $anonfun()
  }: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression])))
}))(collection.this.Seq.canBuildFrom[(String, org.apache.spark.sql.catalyst.expressions.Expression)]).toMap[String, org.apache.spark.sql.catalyst.expressions.Expression](scala.Predef.$conforms[(String, org.apache.spark.sql.catalyst.expressions.Expression)]);
  com.sparkutils.shim.expressions.CreateNamedStruct1.apply(scala.Predef.refArrayOps[(String, org.apache.spark.sql.catalyst.expressions.Expression)](scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](o.fields).map[(String, org.apache.spark.sql.catalyst.expressions.Expression), Array[(String, org.apache.spark.sql.catalyst.expressions.Expression)]](((f: org.apache.spark.sql.types.StructField) =&gt; scala.Predef.ArrowAssoc[String](f.name).-&gt;[org.apache.spark.sql.catalyst.expressions.Expression](dealiased.apply(f.name))))(scala.this.Array.canBuildFrom[(String, org.apache.spark.sql.catalyst.expressions.Expression)]((ClassTag.apply[(String, org.apache.spark.sql.catalyst.expressions.Expression)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(String, org.apache.spark.sql.catalyst.expressions.Expression)])))).flatMap[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](((x0$2: (String, org.apache.spark.sql.catalyst.expressions.Expression)) =&gt; x0$2 match {
    case (_1: String, _2: org.apache.spark.sql.catalyst.expressions.Expression)(String, org.apache.spark.sql.catalyst.expressions.Expression)((name @ _), (e @ _)) =&gt; scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](org.apache.spark.sql.catalyst.expressions.Literal.apply(name), e)
  }))(scala.this.Array.fallbackCanBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression](Predef.this.DummyImplicit.dummyImplicit)))
}
        </td>
      </tr><tr>
        <td>
          454
        </td>
        <td>
          7947
        </td>
        <td>
          14436
          -
          14471
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.asInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          outputType.asInstanceOf[org.apache.spark.sql.types.StructType]
        </td>
      </tr><tr>
        <td>
          456
        </td>
        <td>
          7952
        </td>
        <td>
          14506
          -
          14506
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[(String, org.apache.spark.sql.catalyst.expressions.Expression)]
        </td>
      </tr><tr>
        <td>
          458
        </td>
        <td>
          7948
        </td>
        <td>
          14551
          -
          14557
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Alias.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.name
        </td>
      </tr><tr>
        <td>
          458
        </td>
        <td>
          7951
        </td>
        <td>
          14551
          -
          14645
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Predef.ArrowAssoc.-&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.ArrowAssoc[String](a.name).-&gt;[org.apache.spark.sql.catalyst.expressions.Expression](a.child.transformUp(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x3: A1, default: A1 =&gt; B1): B1 = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; path
      case (defaultCase$ @ _) =&gt; default.apply(x3)
    };
    final def isDefinedAt(x3: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression])))
        </td>
      </tr><tr>
        <td>
          458
        </td>
        <td>
          7950
        </td>
        <td>
          14561
          -
          14645
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.transformUp
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.child.transformUp(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x3: A1, default: A1 =&gt; B1): B1 = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; path
      case (defaultCase$ @ _) =&gt; default.apply(x3)
    };
    final def isDefinedAt(x3: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          458
        </td>
        <td>
          7949
        </td>
        <td>
          14581
          -
          14581
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.Encoding.$anon.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          461
        </td>
        <td>
          7954
        </td>
        <td>
          14499
          -
          14663
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableOnce.toMap
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          se.map[(String, org.apache.spark.sql.catalyst.expressions.Expression), Seq[(String, org.apache.spark.sql.catalyst.expressions.Expression)]](((x0$1: org.apache.spark.sql.catalyst.expressions.NamedExpression) =&gt; x0$1 match {
  case (a @ (_: org.apache.spark.sql.catalyst.expressions.Alias)) =&gt; scala.Predef.ArrowAssoc[String](a.name).-&gt;[org.apache.spark.sql.catalyst.expressions.Expression](a.child.transformUp(({
    @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
      def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
        $anonfun.super.&lt;init&gt;();
        ()
      };
      final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x3: A1, default: A1 =&gt; B1): B1 = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
        case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; path
        case (defaultCase$ @ _) =&gt; default.apply(x3)
      };
      final def isDefinedAt(x3: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x3.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
        case (b @ (_: org.apache.spark.sql.catalyst.expressions.BoundReference)) =&gt; true
        case (defaultCase$ @ _) =&gt; false
      }
    };
    new $anonfun()
  }: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression])))
}))(collection.this.Seq.canBuildFrom[(String, org.apache.spark.sql.catalyst.expressions.Expression)]).toMap[String, org.apache.spark.sql.catalyst.expressions.Expression](scala.Predef.$conforms[(String, org.apache.spark.sql.catalyst.expressions.Expression)])
        </td>
      </tr><tr>
        <td>
          461
        </td>
        <td>
          7953
        </td>
        <td>
          14658
          -
          14658
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Predef.$conforms
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.$conforms[(String, org.apache.spark.sql.catalyst.expressions.Expression)]
        </td>
      </tr><tr>
        <td>
          463
        </td>
        <td>
          7967
        </td>
        <td>
          14675
          -
          14871
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.shim.expressions.CreateNamedStruct1.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.shim.expressions.CreateNamedStruct1.apply(scala.Predef.refArrayOps[(String, org.apache.spark.sql.catalyst.expressions.Expression)](scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](o.fields).map[(String, org.apache.spark.sql.catalyst.expressions.Expression), Array[(String, org.apache.spark.sql.catalyst.expressions.Expression)]](((f: org.apache.spark.sql.types.StructField) =&gt; scala.Predef.ArrowAssoc[String](f.name).-&gt;[org.apache.spark.sql.catalyst.expressions.Expression](dealiased.apply(f.name))))(scala.this.Array.canBuildFrom[(String, org.apache.spark.sql.catalyst.expressions.Expression)]((ClassTag.apply[(String, org.apache.spark.sql.catalyst.expressions.Expression)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(String, org.apache.spark.sql.catalyst.expressions.Expression)])))).flatMap[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](((x0$2: (String, org.apache.spark.sql.catalyst.expressions.Expression)) =&gt; x0$2 match {
  case (_1: String, _2: org.apache.spark.sql.catalyst.expressions.Expression)(String, org.apache.spark.sql.catalyst.expressions.Expression)((name @ _), (e @ _)) =&gt; scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](org.apache.spark.sql.catalyst.expressions.Literal.apply(name), e)
}))(scala.this.Array.fallbackCanBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression](Predef.this.DummyImplicit.dummyImplicit)))
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          7957
        </td>
        <td>
          14745
          -
          14751
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          f.name
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          7966
        </td>
        <td>
          14707
          -
          14859
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.flatMap
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.refArrayOps[(String, org.apache.spark.sql.catalyst.expressions.Expression)](scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](o.fields).map[(String, org.apache.spark.sql.catalyst.expressions.Expression), Array[(String, org.apache.spark.sql.catalyst.expressions.Expression)]](((f: org.apache.spark.sql.types.StructField) =&gt; scala.Predef.ArrowAssoc[String](f.name).-&gt;[org.apache.spark.sql.catalyst.expressions.Expression](dealiased.apply(f.name))))(scala.this.Array.canBuildFrom[(String, org.apache.spark.sql.catalyst.expressions.Expression)]((ClassTag.apply[(String, org.apache.spark.sql.catalyst.expressions.Expression)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(String, org.apache.spark.sql.catalyst.expressions.Expression)])))).flatMap[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](((x0$2: (String, org.apache.spark.sql.catalyst.expressions.Expression)) =&gt; x0$2 match {
  case (_1: String, _2: org.apache.spark.sql.catalyst.expressions.Expression)(String, org.apache.spark.sql.catalyst.expressions.Expression)((name @ _), (e @ _)) =&gt; scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](org.apache.spark.sql.catalyst.expressions.Literal.apply(name), e)
}))(scala.this.Array.fallbackCanBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression](Predef.this.DummyImplicit.dummyImplicit))
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          7960
        </td>
        <td>
          14719
          -
          14719
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.this.Array.canBuildFrom[(String, org.apache.spark.sql.catalyst.expressions.Expression)]((ClassTag.apply[(String, org.apache.spark.sql.catalyst.expressions.Expression)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(String, org.apache.spark.sql.catalyst.expressions.Expression)]))
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          7965
        </td>
        <td>
          14762
          -
          14762
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.FallbackArrayBuilding.fallbackCanBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.this.Array.fallbackCanBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression](Predef.this.DummyImplicit.dummyImplicit)
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          7956
        </td>
        <td>
          14725
          -
          14731
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          f.name
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          7959
        </td>
        <td>
          14725
          -
          14752
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Predef.ArrowAssoc.-&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.ArrowAssoc[String](f.name).-&gt;[org.apache.spark.sql.catalyst.expressions.Expression](dealiased.apply(f.name))
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          7961
        </td>
        <td>
          14707
          -
          14753
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](o.fields).map[(String, org.apache.spark.sql.catalyst.expressions.Expression), Array[(String, org.apache.spark.sql.catalyst.expressions.Expression)]](((f: org.apache.spark.sql.types.StructField) =&gt; scala.Predef.ArrowAssoc[String](f.name).-&gt;[org.apache.spark.sql.catalyst.expressions.Expression](dealiased.apply(f.name))))(scala.this.Array.canBuildFrom[(String, org.apache.spark.sql.catalyst.expressions.Expression)]((ClassTag.apply[(String, org.apache.spark.sql.catalyst.expressions.Expression)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(String, org.apache.spark.sql.catalyst.expressions.Expression)])))
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          7955
        </td>
        <td>
          14707
          -
          14715
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructType.fields
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          o.fields
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          7964
        </td>
        <td>
          14762
          -
          14762
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Predef.DummyImplicit.dummyImplicit
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          Predef.this.DummyImplicit.dummyImplicit
        </td>
      </tr><tr>
        <td>
          464
        </td>
        <td>
          7958
        </td>
        <td>
          14735
          -
          14752
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.MapLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          dealiased.apply(f.name)
        </td>
      </tr><tr>
        <td>
          466
        </td>
        <td>
          7963
        </td>
        <td>
          14812
          -
          14845
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](org.apache.spark.sql.catalyst.expressions.Literal.apply(name), e)
        </td>
      </tr><tr>
        <td>
          466
        </td>
        <td>
          7962
        </td>
        <td>
          14828
          -
          14841
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Literal.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Literal.apply(name)
        </td>
      </tr><tr>
        <td>
          483
        </td>
        <td>
          7970
        </td>
        <td>
          15122
          -
          15144
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.nullable
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SubQueryWrapper.this.children.head.nullable
        </td>
      </tr><tr>
        <td>
          484
        </td>
        <td>
          7971
        </td>
        <td>
          15180
          -
          15185
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          false
        </td>
      </tr><tr>
        <td>
          486
        </td>
        <td>
          7972
        </td>
        <td>
          15285
          -
          15311
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.genCode
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SubQueryWrapper.this.children.head.genCode(ctx)
        </td>
      </tr><tr>
        <td>
          490
        </td>
        <td>
          7973
        </td>
        <td>
          15373
          -
          15398
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.eval
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SubQueryWrapper.this.children.head.eval(input)
        </td>
      </tr><tr>
        <td>
          492
        </td>
        <td>
          7974
        </td>
        <td>
          15436
          -
          15458
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SubQueryWrapper.this.children.head.dataType
        </td>
      </tr><tr>
        <td>
          494
        </td>
        <td>
          7975
        </td>
        <td>
          15551
          -
          15579
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.SubQueryWrapper.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SubQueryWrapper.this.copy(newChildren)
        </td>
      </tr><tr>
        <td>
          499
        </td>
        <td>
          7976
        </td>
        <td>
          15679
          -
          15679
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.SubQueryWrapper.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          501
        </td>
        <td>
          7977
        </td>
        <td>
          15661
          -
          15731
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.isDefined
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          expr.collectFirst[com.sparkutils.quality.impl.util.SubQueryWrapper](({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,com.sparkutils.quality.impl.util.SubQueryWrapper] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; com.sparkutils.quality.impl.util.SubQueryWrapper&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: com.sparkutils.quality.impl.util.SubQueryWrapper](x1: A1, default: A1 =&gt; B1): B1 = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (s @ (_: com.sparkutils.quality.impl.util.SubQueryWrapper)) =&gt; s
      case (defaultCase$ @ _) =&gt; default.apply(x1)
    };
    final def isDefinedAt(x1: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (s @ (_: com.sparkutils.quality.impl.util.SubQueryWrapper)) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,com.sparkutils.quality.impl.util.SubQueryWrapper])).isDefined
        </td>
      </tr><tr>
        <td>
          507
        </td>
        <td>
          7978
        </td>
        <td>
          15829
          -
          15854
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.indexOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          v.toString().indexOf(&quot;[&quot;)
        </td>
      </tr><tr>
        <td>
          508
        </td>
        <td>
          7979
        </td>
        <td>
          15863
          -
          15874
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          openb.==(-1)
        </td>
      </tr><tr>
        <td>
          509
        </td>
        <td>
          7981
        </td>
        <td>
          15899
          -
          15901
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          509
        </td>
        <td>
          7980
        </td>
        <td>
          15883
          -
          15897
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.VariableValue.variableName
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          v.variableName
        </td>
      </tr><tr>
        <td>
          509
        </td>
        <td>
          7983
        </td>
        <td>
          15882
          -
          15902
        </td>
        <td>
          Block
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[String, String](v.variableName, &quot;&quot;)
        </td>
      </tr><tr>
        <td>
          509
        </td>
        <td>
          7982
        </td>
        <td>
          15882
          -
          15902
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[String, String](v.variableName, &quot;&quot;)
        </td>
      </tr><tr>
        <td>
          511
        </td>
        <td>
          7984
        </td>
        <td>
          15919
          -
          15933
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.VariableValue.variableName
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          v.variableName
        </td>
      </tr><tr>
        <td>
          511
        </td>
        <td>
          7987
        </td>
        <td>
          15963
          -
          15989
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.IndexedSeqOptimized.drop
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.augmentString(v.variableName).drop(openb)
        </td>
      </tr><tr>
        <td>
          511
        </td>
        <td>
          7989
        </td>
        <td>
          15918
          -
          15990
        </td>
        <td>
          Block
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[String, String](scala.Predef.augmentString(v.variableName).dropRight(codegen.this.ExprValue.exprValueToString(v).length().-(openb)), scala.Predef.augmentString(v.variableName).drop(openb))
        </td>
      </tr><tr>
        <td>
          511
        </td>
        <td>
          7986
        </td>
        <td>
          15919
          -
          15961
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.IndexedSeqOptimized.dropRight
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.augmentString(v.variableName).dropRight(codegen.this.ExprValue.exprValueToString(v).length().-(openb))
        </td>
      </tr><tr>
        <td>
          511
        </td>
        <td>
          7985
        </td>
        <td>
          15944
          -
          15960
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.-
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          codegen.this.ExprValue.exprValueToString(v).length().-(openb)
        </td>
      </tr><tr>
        <td>
          511
        </td>
        <td>
          7988
        </td>
        <td>
          15918
          -
          15990
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[String, String](scala.Predef.augmentString(v.variableName).dropRight(codegen.this.ExprValue.exprValueToString(v).length().-(openb)), scala.Predef.augmentString(v.variableName).drop(openb))
        </td>
      </tr><tr>
        <td>
          516
        </td>
        <td>
          7995
        </td>
        <td>
          16219
          -
          16378
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.flatMap
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.flatMap[org.apache.spark.sql.catalyst.expressions.codegen.VariableValue, Seq[org.apache.spark.sql.catalyst.expressions.codegen.VariableValue]](((x0$1: org.apache.spark.sql.catalyst.expressions.codegen.ExprValue) =&gt; x0$1 match {
  case (a @ (_: org.apache.spark.sql.catalyst.expressions.codegen.VariableValue)) =&gt; scala.this.Option.option2Iterable[org.apache.spark.sql.catalyst.expressions.codegen.VariableValue](scala.Some.apply[org.apache.spark.sql.catalyst.expressions.codegen.VariableValue](a))
  case _ =&gt; scala.this.Option.option2Iterable[Nothing](scala.None)
}))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.codegen.VariableValue])
        </td>
      </tr><tr>
        <td>
          516
        </td>
        <td>
          7994
        </td>
        <td>
          16229
          -
          16229
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.codegen.VariableValue]
        </td>
      </tr><tr>
        <td>
          518
        </td>
        <td>
          7990
        </td>
        <td>
          16344
          -
          16351
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[org.apache.spark.sql.catalyst.expressions.codegen.VariableValue](a)
        </td>
      </tr><tr>
        <td>
          518
        </td>
        <td>
          7991
        </td>
        <td>
          16344
          -
          16351
        </td>
        <td>
          ApplyImplicitView
        </td>
        <td>
          scala.Option.option2Iterable
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.this.Option.option2Iterable[org.apache.spark.sql.catalyst.expressions.codegen.VariableValue](scala.Some.apply[org.apache.spark.sql.catalyst.expressions.codegen.VariableValue](a))
        </td>
      </tr><tr>
        <td>
          519
        </td>
        <td>
          7993
        </td>
        <td>
          16368
          -
          16372
        </td>
        <td>
          ApplyImplicitView
        </td>
        <td>
          scala.Option.option2Iterable
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.this.Option.option2Iterable[Nothing](scala.None)
        </td>
      </tr><tr>
        <td>
          519
        </td>
        <td>
          7992
        </td>
        <td>
          16368
          -
          16372
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.None
        </td>
      </tr><tr>
        <td>
          522
        </td>
        <td>
          7998
        </td>
        <td>
          16384
          -
          16991
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[String, String](ordered.map[String, Seq[String]](((v: org.apache.spark.sql.catalyst.expressions.codegen.VariableValue) =&gt; {
  &lt;synthetic&gt; &lt;artifact&gt; private[this] val x$4: (String, String) = (Params.this.stripBrackets(v): (String, String) @unchecked) match {
    case (_1: String, _2: String)(String, String)((stripped @ _), (arrayInName @ _)) =&gt; scala.Tuple2.apply[String, String](stripped, arrayInName)
  };
  val stripped: String = x$4._1;
  val arrayInName: String = x$4._2;
  &lt;synthetic&gt; &lt;artifact&gt; private[this] val x$5: (String, String) = (if (v.javaType.isArray())
    scala.Tuple2.apply[String, String](scala.StringContext.apply(&quot;&quot;, &quot;&quot;).s(v.javaType.getComponentType().getName()), &quot;[]&quot;)
  else
    if (v.javaType.isPrimitive())
      scala.Tuple2.apply[String, String](v.javaType.toString(), arrayInName.replaceAll(&quot;[^\\[\\]]&quot;, &quot;&quot;))
    else
      scala.Tuple2.apply[String, String](v.javaType.getName(), arrayInName.replaceAll(&quot;[^\\[\\]]&quot;, &quot;&quot;)): (String, String) @unchecked) match {
    case (_1: String, _2: String)(String, String)((typ @ _), (array @ _)) =&gt; scala.Tuple2.apply[String, String](typ, array)
  };
  val typ: String = x$5._1;
  val array: String = x$5._2;
  scala.StringContext.apply(&quot;&quot;, &quot;&quot;, &quot; &quot;, &quot;&quot;).s(typ, array, stripped)
}))(collection.this.Seq.canBuildFrom[String]).mkString(&quot;, &quot;), ordered.map[String, Seq[String]](((v: org.apache.spark.sql.catalyst.expressions.codegen.VariableValue) =&gt; if (v.javaType.isArray().&amp;&amp;(callsKeepArrays))
  v.variableName
else
  Params.this.stripBrackets(v)._1))(collection.this.Seq.canBuildFrom[String]).mkString(&quot;, &quot;))
        </td>
      </tr><tr>
        <td>
          534
        </td>
        <td>
          7996
        </td>
        <td>
          16385
          -
          16823
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.mkString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ordered.map[String, Seq[String]](((v: org.apache.spark.sql.catalyst.expressions.codegen.VariableValue) =&gt; {
  &lt;synthetic&gt; &lt;artifact&gt; private[this] val x$4: (String, String) = (Params.this.stripBrackets(v): (String, String) @unchecked) match {
    case (_1: String, _2: String)(String, String)((stripped @ _), (arrayInName @ _)) =&gt; scala.Tuple2.apply[String, String](stripped, arrayInName)
  };
  val stripped: String = x$4._1;
  val arrayInName: String = x$4._2;
  &lt;synthetic&gt; &lt;artifact&gt; private[this] val x$5: (String, String) = (if (v.javaType.isArray())
    scala.Tuple2.apply[String, String](scala.StringContext.apply(&quot;&quot;, &quot;&quot;).s(v.javaType.getComponentType().getName()), &quot;[]&quot;)
  else
    if (v.javaType.isPrimitive())
      scala.Tuple2.apply[String, String](v.javaType.toString(), arrayInName.replaceAll(&quot;[^\\[\\]]&quot;, &quot;&quot;))
    else
      scala.Tuple2.apply[String, String](v.javaType.getName(), arrayInName.replaceAll(&quot;[^\\[\\]]&quot;, &quot;&quot;)): (String, String) @unchecked) match {
    case (_1: String, _2: String)(String, String)((typ @ _), (array @ _)) =&gt; scala.Tuple2.apply[String, String](typ, array)
  };
  val typ: String = x$5._1;
  val array: String = x$5._2;
  scala.StringContext.apply(&quot;&quot;, &quot;&quot;, &quot; &quot;, &quot;&quot;).s(typ, array, stripped)
}))(collection.this.Seq.canBuildFrom[String]).mkString(&quot;, &quot;)
        </td>
      </tr><tr>
        <td>
          540
        </td>
        <td>
          7997
        </td>
        <td>
          16832
          -
          16990
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.mkString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ordered.map[String, Seq[String]](((v: org.apache.spark.sql.catalyst.expressions.codegen.VariableValue) =&gt; if (v.javaType.isArray().&amp;&amp;(callsKeepArrays))
  v.variableName
else
  Params.this.stripBrackets(v)._1))(collection.this.Seq.canBuildFrom[String]).mkString(&quot;, &quot;)
        </td>
      </tr>
    </table>
          </div>
        </div>
      </body>
    </html>