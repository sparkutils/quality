<html>
      <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <title id="title">
          com/sparkutils/quality/impl/RuleRunnerObject.scala.html
        </title>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/css/theme.default.min.css" type="text/css"/><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/js/jquery.tablesorter.min.js"></script><link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css" type="text/css"/><script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script><script type="text/javascript">
        $(document).ready(function() {$(".tablesorter").tablesorter();});
      </script>
        <style>
          table.codegrid { font-family: monospace; font-size: 12px; width: auto!important; }table.statementlist { width: auto!important; font-size: 13px; } table.codegrid td { padding: 0!important; border: 0!important } table td.linenumber { width: 40px!important; } 
        </style>
      </head>
      <body style="font-family: monospace;">
        <ul class="nav nav-tabs">
          <li>
            <a href="#codegrid" data-toggle="tab">Codegrid</a>
          </li>
          <li>
            <a href="#statementlist" data-toggle="tab">Statement List</a>
          </li>
        </ul>
        <div class="tab-content">
          <div class="tab-pane active" id="codegrid">
            <pre style='font-size: 12pt; font-family: courier, monospace;'>1 <span style=''>package com.sparkutils.quality.impl
</span>2 <span style=''>
</span>3 <span style=''>import com.sparkutils.quality.utils.{PrintCode, StructFunctions}
</span>4 <span style=''>import com.sparkutils.quality.QualityException.qualityException
</span>5 <span style=''>import com.sparkutils.quality.impl.aggregates.AggregateExpressions
</span>6 <span style=''>import com.sparkutils.quality.impl.bloom.{BucketedArrayParquetAggregator, ParquetAggregator}
</span>7 <span style=''>import com.sparkutils.quality.impl.hash.{HashFunctionFactory, HashFunctionsExpression, MessageDigestFactory, ZALongHashFunctionFactory, ZALongTupleHashFunctionFactory}
</span>8 <span style=''>import com.sparkutils.quality.impl.id.{AsBase64Fields, AsBase64Struct, GenericLongBasedIDExpression, GuaranteedUniqueID, GuaranteedUniqueIdIDExpression, IDFromBase64, SizeOfIDString, model}
</span>9 <span style=''>import com.sparkutils.quality.impl.rng.{RandLongsWithJump, RandomBytes, RandomLongs}
</span>10 <span style=''>import com.sparkutils.quality.impl.longPair.{AsUUID, LongPairExpression, PrefixedToLongPair}
</span>11 <span style=''>import com.sparkutils.quality.impl.util.{ComparableMapConverter, ComparableMapReverser}
</span>12 <span style=''>import com.sparkutils.quality.{ExprLogic, RuleSuite, impl}
</span>13 <span style=''>import org.apache.commons.rng.simple.RandomSource
</span>14 <span style=''>import org.apache.spark.sql.QualitySparkUtils.add
</span>15 <span style=''>import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute
</span>16 <span style=''>import org.apache.spark.sql.catalyst.expressions.{Add, And, AttributeReference, EqualTo, Expression, LambdaFunction, Literal, UnresolvedNamedLambdaVariable}
</span>17 <span style=''>import org.apache.spark.sql.qualityFunctions.LambdaFunctions.processTopCallFun
</span>18 <span style=''>import org.apache.spark.sql.qualityFunctions._
</span>19 <span style=''>import org.apache.spark.sql.types._
</span>20 <span style=''>import org.apache.spark.sql.{QualitySparkUtils, SparkSession, functions}
</span>21 <span style=''>import org.apache.spark.unsafe.types.UTF8String
</span>22 <span style=''>
</span>23 <span style=''>trait RuleRunnerFunctionsImport {
</span>24 <span style=''>
</span>25 <span style=''>  import RuleRunnerFunctions._
</span>26 <span style=''>
</span>27 <span style=''>  val maxDec = </span><span style='background: #AEF1AE'>DecimalType(DecimalType.MAX_PRECISION, DecimalType.MAX_SCALE)</span><span style=''>
</span>28 <span style=''>
</span>29 <span style=''>  private val noopAdd = (dt: DataType) =&gt; </span><span style='background: #F0ADAD'>None</span><span style=''>
</span>30 <span style=''>
</span>31 <span style=''>  /**
</span>32 <span style=''>   * Provides the default monoidal add for a dataType, used for merging summed results when aggregating
</span>33 <span style=''>   *
</span>34 <span style=''>   * @param dataType
</span>35 <span style=''>   * @return
</span>36 <span style=''>   */
</span>37 <span style=''>  def defaultAdd(dataType: DataType, extension: DataType =&gt; Option[(Expression, Expression) =&gt; Expression] = noopAdd): Option[(Expression, Expression) =&gt; Expression] =
</span>38 <span style=''>    dataType match {
</span>39 <span style=''>      case _: MapType =&gt; </span><span style='background: #AEF1AE'>Some((left, right) =&gt; MapMerge(Seq(left, right), (dataType: DataType) =&gt; defaultAdd(dataType, extension)))</span><span style=''>
</span>40 <span style=''>      case _: IntegerType | LongType | DoubleType =&gt;
</span>41 <span style=''>        </span><span style='background: #AEF1AE'>Some((left, right) =&gt; add(left, right, null))</span><span style=''>
</span>42 <span style=''>      case a: DecimalType =&gt;
</span>43 <span style=''>        </span><span style='background: #AEF1AE'>Some((left, right) =&gt; add(left, right, a))</span><span style=''>
</span>44 <span style=''>      case _ =&gt; </span><span style='background: #F0ADAD'>extension(dataType)</span><span style=''>
</span>45 <span style=''>    }
</span>46 <span style=''>
</span>47 <span style=''>  /**
</span>48 <span style=''>   * Provides the default monoidal Zero for a dataType, used for defaults when aggregating
</span>49 <span style=''>   *
</span>50 <span style=''>   * @param dataType
</span>51 <span style=''>   * @return
</span>52 <span style=''>   */
</span>53 <span style=''>  def defaultZero(dataType: DataType): Option[Any] =
</span>54 <span style=''>    dataType match {
</span>55 <span style=''>      case _: MapType =&gt; </span><span style='background: #AEF1AE'>Some(EmptyMap)</span><span style=''>
</span>56 <span style=''>      case _: IntegerType | LongType =&gt; </span><span style='background: #AEF1AE'>Some(0L)</span><span style=''>
</span>57 <span style=''>      case _: DoubleType =&gt; </span><span style='background: #AEF1AE'>Some(0.0)</span><span style=''>
</span>58 <span style=''>      case d: DecimalType =&gt; </span><span style='background: #AEF1AE'>Some(Decimal.createUnsafe(0, d.precision, d.scale))</span><span style=''>
</span>59 <span style=''>      case _ =&gt; </span><span style='background: #F0ADAD'>None</span><span style=''>
</span>60 <span style=''>    }
</span>61 <span style=''>
</span>62 <span style=''>  /**
</span>63 <span style=''>   * Wrap to provide the default lookup for registerFunctions to change type parsing from DDL based to other or
</span>64 <span style=''>   * when None to add additional lookups should ddl fail
</span>65 <span style=''>   *
</span>66 <span style=''>   * @param string
</span>67 <span style=''>   * @return
</span>68 <span style=''>   */
</span>69 <span style=''>  def defaultParseTypes(string: String): Option[DataType] =
</span>70 <span style=''>    try {
</span>71 <span style=''>      </span><span style='background: #AEF1AE'>Some(
</span>72 <span style=''></span><span style='background: #AEF1AE'>        DataType.fromDDL(string)
</span>73 <span style=''></span><span style='background: #AEF1AE'>      )</span><span style=''>
</span>74 <span style=''>    } catch {
</span>75 <span style=''>      case _: Throwable =&gt; </span><span style='background: #F0ADAD'>None</span><span style=''>
</span>76 <span style=''>    }
</span>77 <span style=''>
</span>78 <span style=''>  val INC_REWRITE_GENEXP_ERR_MSG: String = </span><span style='background: #AEF1AE'>&quot;inc('DDL', generic expression) is not supported in NO_REWRITE mode, use inc(generic expression) without NO_REWRITE mode enabled&quot;</span><span style=''>
</span>79 <span style=''>
</span>80 <span style=''>  /**
</span>81 <span style=''>   * Must be called before using any functions like Passed, Failed or Probability(X)
</span>82 <span style=''>   * @param parseTypes override type parsing (e.g. DDL, defaults to defaultParseTypes / DataType.fromDDL)
</span>83 <span style=''>   * @param zero override zero creation for aggExpr (defaults to defaultZero)
</span>84 <span style=''>   * @param add override the &quot;add&quot; function for aggExpr types (defaults to defaultAdd(dataType))
</span>85 <span style=''>   * @param writer override the printCode and printExpr print writing function (defaults to println)
</span>86 <span style=''>   * @param register function to register the sql extensions
</span>87 <span style=''>   */
</span>88 <span style=''>  def registerQualityFunctions(parseTypes: String =&gt; Option[DataType] = defaultParseTypes _,
</span>89 <span style=''>                               zero: DataType =&gt; Option[Any] = defaultZero _,
</span>90 <span style=''>                               add: DataType =&gt; Option[(Expression, Expression) =&gt; Expression] = (dataType: DataType) =&gt; defaultAdd(dataType),
</span>91 <span style=''>                               mapCompare: DataType =&gt; Option[(Any, Any) =&gt; Int] = (dataType: DataType) =&gt; utils.defaultMapCompare(dataType),
</span>92 <span style=''>                               writer: String =&gt; Unit = println(_),
</span>93 <span style=''>                               register: (String, Seq[Expression] =&gt; Expression) =&gt; Unit =
</span>94 <span style=''>                                  QualitySparkUtils.registerFunction(SparkSession.getActiveSession.get.sessionState.functionRegistry) _
</span>95 <span style=''>                       ) {
</span>96 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;comparableMaps&quot;, exps =&gt; ComparableMapConverter(exps(0), mapCompare))</span><span style=''>
</span>97 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;reverseComparableMaps&quot;, exps =&gt; ComparableMapReverser(exps(0)))</span><span style=''>
</span>98 <span style=''>
</span>99 <span style=''>    val f = (exps: Seq[Expression]) =&gt; </span><span style='background: #AEF1AE'>ProbabilityExpr(exps.head)</span><span style=''>
</span>100 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;probability&quot;, f)</span><span style=''>
</span>101 <span style=''>    val ff = (exps: Seq[Expression]) =&gt; </span><span style='background: #AEF1AE'>FlattenResultsExpression(exps.head, FlattenStruct.ruleSuiteDeserializer)</span><span style=''>
</span>102 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;flattenResults&quot;, ff)</span><span style=''>
</span>103 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;flattenRuleResults&quot;, exps =&gt; FlattenRulesResultsExpression(exps(0), FlattenStruct.ruleSuiteDeserializer))</span><span style=''>
</span>104 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;flattenFolderResults&quot;, exps =&gt; FlattenFolderResultsExpression(exps(0), FlattenStruct.ruleSuiteDeserializer))</span><span style=''>
</span>105 <span style=''>
</span>106 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;passed&quot;, _ =&gt; com.sparkutils.quality.PassedExpr)</span><span style=''>
</span>107 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;failed&quot;, _ =&gt; com.sparkutils.quality.FailedExpr)</span><span style=''>
</span>108 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;softFailed&quot;, _ =&gt; com.sparkutils.quality.SoftFailedExpr)</span><span style=''>
</span>109 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;disabledRule&quot;, _ =&gt; com.sparkutils.quality.DisabledRuleExpr)</span><span style=''>
</span>110 <span style=''>
</span>111 <span style=''>    val pif = (exps: Seq[Expression]) =&gt; </span><span style='background: #AEF1AE'>Pack(exps(0), exps(1))</span><span style=''>
</span>112 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;packInts&quot;, pif)</span><span style=''>
</span>113 <span style=''>
</span>114 <span style=''>    val upif = (exps: Seq[Expression]) =&gt; </span><span style='background: #AEF1AE'>UnPack(exps(0))</span><span style=''>
</span>115 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;unpack&quot;, upif)</span><span style=''>
</span>116 <span style=''>
</span>117 <span style=''>    val uptif = (exps: Seq[Expression]) =&gt; </span><span style='background: #AEF1AE'>UnPackIdTriple(exps(0))</span><span style=''>
</span>118 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;unpackIdTriple&quot;, uptif)</span><span style=''>
</span>119 <span style=''>
</span>120 <span style=''>
</span>121 <span style=''>    val sf = (exps: Seq[Expression]) =&gt; </span><span style='background: #AEF1AE'>SoftFailExpr(exps.head)</span><span style=''>
</span>122 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;softFail&quot;, sf)</span><span style=''>
</span>123 <span style=''>
</span>124 <span style=''>    def strType(exp: Expression) = {
</span>125 <span style=''>      val Literal(str: UTF8String, StringType) = exp // only accept type as string
</span>126 <span style=''>      </span><span style='background: #AEF1AE'>str.toString</span><span style=''>
</span>127 <span style=''>    }
</span>128 <span style=''>
</span>129 <span style=''>    def parse(exp: Expression) = {
</span>130 <span style=''>      val Literal(str: UTF8String, StringType) = exp // only accept type as string
</span>131 <span style=''>      </span><span style='background: #AEF1AE'>parseTypes(str.toString).getOrElse(</span><span style='background: #F0ADAD'>qualityException(s&quot;Could not parse the type $str&quot;)</span><span style='background: #AEF1AE'>)</span><span style=''>
</span>132 <span style=''>    }
</span>133 <span style=''>
</span>134 <span style=''>    </span><span style='background: #AEF1AE'>register(LambdaFunctions.PlaceHolder, {
</span>135 <span style=''></span><span style='background: #AEF1AE'>      case Seq(e, Literal(bol: Boolean, BooleanType)) =&gt;
</span>136 <span style=''></span><span style='background: #AEF1AE'>        PlaceHolderExpression(parse(e), bol)
</span>137 <span style=''></span><span style='background: #AEF1AE'>      case Seq(e) =&gt;
</span>138 <span style=''></span><span style='background: #AEF1AE'>        PlaceHolderExpression(parse(e))
</span>139 <span style=''></span><span style='background: #AEF1AE'>      case _ =&gt;
</span>140 <span style=''></span><span style='background: #AEF1AE'>        PlaceHolderExpression(LongType)
</span>141 <span style=''></span><span style='background: #AEF1AE'>    })</span><span style=''>
</span>142 <span style=''>
</span>143 <span style=''>    /* Note - both Lambda and CallFun are only called in top level expressions,
</span>144 <span style=''>          nested calls are handled within the lambda expression
</span>145 <span style=''>          creation that &quot;calls&quot; this. */
</span>146 <span style=''>
</span>147 <span style=''>    </span><span style='background: #AEF1AE'>register(LambdaFunctions.Lambda, {
</span>148 <span style=''></span><span style='background: #AEF1AE'>      case Seq(fun: FunForward) =&gt;
</span>149 <span style=''></span><span style='background: #AEF1AE'>        val res = FunCall(fun)
</span>150 <span style=''></span><span style='background: #AEF1AE'>        res
</span>151 <span style=''></span><span style='background: #AEF1AE'>      case Seq(fun: FunN) =&gt;
</span>152 <span style=''></span><span style='background: #AEF1AE'>        // placeholders that are 1:1
</span>153 <span style=''></span><span style='background: #AEF1AE'>        val res = fun.function
</span>154 <span style=''></span><span style='background: #AEF1AE'>        res
</span>155 <span style=''></span><span style='background: #AEF1AE'>    })</span><span style=''>
</span>156 <span style=''>
</span>157 <span style=''>    </span><span style='background: #AEF1AE'>register(LambdaFunctions.CallFun, {
</span>158 <span style=''></span><span style='background: #AEF1AE'>      case (fun@ FunN(_, l@ LambdaFunction(ff : FunForward, _, _), _, _, _)) +: args =&gt;
</span>159 <span style=''></span><span style='background: #AEF1AE'>        processTopCallFun(fun, l, ff, args)
</span>160 <span style=''></span><span style='background: #AEF1AE'>      case t =&gt; </span><span style='background: #F0ADAD'>qualityException(s&quot;${LambdaFunctions.CallFun} should only be used to process partially applied functions returned by a user lambda, got $t instead&quot;)</span><span style='background: #AEF1AE'>
</span>161 <span style=''></span><span style='background: #AEF1AE'>    })</span><span style=''>
</span>162 <span style=''>
</span>163 <span style=''>    val afx = (exps: Seq[Expression]) =&gt; {
</span>164 <span style=''>      val (sumType, filter, sum, count) =
</span>165 <span style=''>        exps.size match {
</span>166 <span style=''>          case 3 =&gt;
</span>167 <span style=''>            // attempt to take a look at exps1 to identify if it's a FunN or mapWith
</span>168 <span style=''>            val typ =
</span>169 <span style=''>              exps(1) match {
</span>170 <span style=''>                case FunN(Seq(RefExpression(dataType, _, _)), _, _, _, _) =&gt; dataType // would default to long anyway
</span>171 <span style=''>                case MapTransform(RefExpression(t: MapType, _, _), _, _, _) =&gt; t
</span>172 <span style=''>                case _ =&gt; LongType
</span>173 <span style=''>              }
</span>174 <span style=''>            (typ, exps(0), exps(1), exps(2))
</span>175 <span style=''>          case 4 =&gt;
</span>176 <span style=''>            val Literal(str: UTF8String, StringType) = exps(0) // only accept type as string
</span>177 <span style=''>            if (str.toString == &quot;NO_REWRITE&quot;)
</span>178 <span style=''>              // signal not to replace types
</span>179 <span style=''>              (null, exps(1), exps(2), exps(3))
</span>180 <span style=''>            else
</span>181 <span style=''>              (parse(exps(0)), exps(1), exps(2), exps(3))
</span>182 <span style=''>        }
</span>183 <span style=''>      </span><span style='background: #AEF1AE'>AggregateExpressions(sumType, filter, sum, count, zero, add)</span><span style=''>
</span>184 <span style=''>    }
</span>185 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;aggExpr&quot;, afx)</span><span style=''>
</span>186 <span style=''>
</span>187 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;sumWith&quot;, (exps: Seq[Expression]) =&gt; {
</span>188 <span style=''></span><span style='background: #AEF1AE'>      val (dataType, origExp) = exps.size match {
</span>189 <span style=''></span><span style='background: #AEF1AE'>        case 1 =&gt; (LongType, exps(0))
</span>190 <span style=''></span><span style='background: #AEF1AE'>        // backwards compat
</span>191 <span style=''></span><span style='background: #AEF1AE'>        case 2 =&gt; (parse(exps(0)), exps(1))
</span>192 <span style=''></span><span style='background: #AEF1AE'>      }
</span>193 <span style=''></span><span style='background: #AEF1AE'>      FunN(Seq(RefExpression(dataType)), origExp, Some(&quot;sumWith&quot;))
</span>194 <span style=''></span><span style='background: #AEF1AE'>    })</span><span style=''>
</span>195 <span style=''>
</span>196 <span style=''>    val ff2 = (exps: Seq[Expression]) =&gt; {
</span>197 <span style=''>      // real type for param1 is changed by aggrExpr, but last works for all compat as well
</span>198 <span style=''>      val (sumType, exp) =
</span>199 <span style=''>        exps.size match {
</span>200 <span style=''>          case 1 =&gt; (LongType, exps(0))
</span>201 <span style=''>          case 2 =&gt; (parse(exps(0)), exps(1)) // support the NO_REWRITE override case
</span>202 <span style=''>        }
</span>203 <span style=''>
</span>204 <span style=''>      </span><span style='background: #AEF1AE'>FunN(Seq(RefExpression(sumType), RefExpression(LongType)), exp, Some(&quot;resultsWith&quot;))</span><span style=''>
</span>205 <span style=''>    }
</span>206 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;resultsWith&quot;, ff2)</span><span style=''>
</span>207 <span style=''>
</span>208 <span style=''>    val mapFX = (exps: Seq[Expression]) =&gt; </span><span style='background: #AEF1AE'>exps.size</span><span style=''> match {
</span>209 <span style=''>      case 3 =&gt;
</span>210 <span style=''>        // parse it for old sql to support backwards aggExpr
</span>211 <span style=''>        </span><span style='background: #AEF1AE'>MapTransform.create(RefExpression(parse(exps(0))), exps(1), exps(2), zero)</span><span style=''>
</span>212 <span style=''>      case 2 =&gt;
</span>213 <span style=''>        // default to LongType, aggrExpr must fix, 2nd param is key, third the func manipulating the key
</span>214 <span style=''>        </span><span style='background: #AEF1AE'>MapTransform.create(RefExpression(MapType(LongType, LongType)), exps(0), exps(1), zero)</span><span style=''>
</span>215 <span style=''>    }
</span>216 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;mapWith&quot;, mapFX)</span><span style=''>
</span>217 <span style=''>
</span>218 <span style=''>    def aggFWith(fun: String) = (what: String) =&gt; (exps: Seq[Expression]) =&gt; (
</span>219 <span style=''>      </span><span style='background: #AEF1AE'>if (exps.size == 0)
</span>220 <span style=''></span><span style='background: #AEF1AE'>        functions.expr(s&quot;$fun( $what )&quot;)
</span>221 <span style=''></span><span style='background: #AEF1AE'>      else
</span>222 <span style=''></span><span style='background: #AEF1AE'>        functions.expr(s&quot;$fun('${strType(exps(0))}', $what )&quot;)
</span>223 <span style=''></span><span style='background: #AEF1AE'>      ).expr</span><span style=''>
</span>224 <span style=''>
</span>225 <span style=''>    val retWith = </span><span style='background: #AEF1AE'>aggFWith(&quot;resultsWith&quot;)</span><span style=''>
</span>226 <span style=''>
</span>227 <span style=''>    // common cases
</span>228 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;meanF&quot;, retWith(&quot;(sum, count) -&gt; sum / count&quot;))</span><span style=''>
</span>229 <span style=''>
</span>230 <span style=''>    val sumWith = </span><span style='background: #AEF1AE'>aggFWith(&quot;sumWith&quot;)</span><span style=''>
</span>231 <span style=''>
</span>232 <span style=''>    val incX = (exps: Seq[Expression]) =&gt; exps match {
</span>233 <span style=''>      case Seq(x: AttributeReference) =&gt;
</span>234 <span style=''>        val name = </span><span style='background: #AEF1AE'>x.qualifier.mkString(&quot;.&quot;) + x.name</span><span style=''> // that is bad code man should be option
</span>235 <span style=''>        </span><span style='background: #AEF1AE'>sumWith(s&quot;sum -&gt; sum + $name&quot;)(Seq())</span><span style=''>
</span>236 <span style=''>      case Seq(Literal(str: UTF8String, StringType)) =&gt;
</span>237 <span style=''>        // case for type passing
</span>238 <span style=''>        </span><span style='background: #F0ADAD'>sumWith(&quot;sum -&gt; sum + 1&quot;)(exps)</span><span style=''>
</span>239 <span style=''>      case Seq(Literal(str: UTF8String, StringType), x: AttributeReference) =&gt;
</span>240 <span style=''>        val name = </span><span style='background: #AEF1AE'>x.qualifier.mkString(&quot;.&quot;) + x.name</span><span style=''>
</span>241 <span style=''>        </span><span style='background: #AEF1AE'>sumWith(s&quot;sum -&gt; sum + $name&quot;)(Seq(exps(0)))</span><span style=''> // keep the type, drop the attr
</span>242 <span style=''>      case Seq(Literal(str: UTF8String, StringType), y) =&gt;
</span>243 <span style=''>        </span><span style='background: #AEF1AE'>qualityException(INC_REWRITE_GENEXP_ERR_MSG)</span><span style=''>
</span>244 <span style=''>      case Seq( y ) =&gt;
</span>245 <span style=''>        val LambdaFunction(a: Add, Seq(sum: UnresolvedNamedLambdaVariable), hidden ) = functions.expr(&quot;sumWith(sum -&gt; sum + 1)&quot;).expr.children(0)
</span>246 <span style=''>        import QualitySparkUtils.{add =&gt; addf}
</span>247 <span style=''>        // could be a cast around x or three attributes plusing each other or....
</span>248 <span style=''>        </span><span style='background: #AEF1AE'>FunN(Seq(RefExpression(LongType)),
</span>249 <span style=''></span><span style='background: #AEF1AE'>          LambdaFunction(addf(a.left, y, LongType), Seq(sum), hidden )
</span>250 <span style=''></span><span style='background: #AEF1AE'>          , Some(&quot;inc&quot;))</span><span style=''> // keep the type
</span>251 <span style=''>      case Seq() =&gt; </span><span style='background: #AEF1AE'>functions.expr(s&quot;sumWith(sum -&gt; sum + 1)&quot;).expr</span><span style=''>
</span>252 <span style=''>    }
</span>253 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;inc&quot;, incX)</span><span style=''>
</span>254 <span style=''>
</span>255 <span style=''>    // return sum
</span>256 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;returnSum&quot;, retWith(&quot;(sum, count) -&gt; sum&quot;))</span><span style=''>
</span>257 <span style=''>
</span>258 <span style=''>
</span>259 <span style=''>    // random generators
</span>260 <span style=''>    val brf = (exps: Seq[Expression]) =&gt; {
</span>261 <span style=''>      def getRandom(exp: Expression) = {
</span>262 <span style=''>        val str = </span><span style='background: #AEF1AE'>getString(exp)</span><span style=''>
</span>263 <span style=''>        </span><span style='background: #AEF1AE'>RandomSource.valueOf(str)</span><span style=''>
</span>264 <span style=''>      }
</span>265 <span style=''>
</span>266 <span style=''>      //numBytes: Int, randomSource: RandomSource, seed: Long constructor but needs to use random, seed, numbytes
</span>267 <span style=''>      val (numBytes: Int, randomSource, seed: Long) =
</span>268 <span style=''>        exps.size match {
</span>269 <span style=''>          case 0 =&gt; (16, RandomSource.XO_RO_SHI_RO_128_PP, 0L)
</span>270 <span style=''>          case 1 =&gt; (16, getRandom(exps(0)), 0L)
</span>271 <span style=''>          case 2 =&gt; (16, getRandom(exps(0)), getLong(exps(1)))
</span>272 <span style=''>          case 3 =&gt; (getLong(exps(2)).toInt, getRandom(exps(0)), getLong(exps(1)))
</span>273 <span style=''>          case _ =&gt; literalsNeeded
</span>274 <span style=''>        }
</span>275 <span style=''>
</span>276 <span style=''>      </span><span style='background: #AEF1AE'>RandomBytes(numBytes, randomSource, seed)</span><span style=''>
</span>277 <span style=''>    }
</span>278 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;rngBytes&quot;, brf)</span><span style=''>
</span>279 <span style=''>    def getRandom(exp: Expression) = {
</span>280 <span style=''>      val str = </span><span style='background: #AEF1AE'>getString(exp)</span><span style=''>
</span>281 <span style=''>      </span><span style='background: #AEF1AE'>RandomSource.valueOf(str)</span><span style=''>
</span>282 <span style=''>    }
</span>283 <span style=''>
</span>284 <span style=''>    // random generators
</span>285 <span style=''>    val lrf = (exps: Seq[Expression]) =&gt; {
</span>286 <span style=''>
</span>287 <span style=''>      //randomSource: RandomSource, seed: Long constructor but needs to use random, seed, numbytes
</span>288 <span style=''>      val (randomSource, seed: Long) =
</span>289 <span style=''>        exps.size match {
</span>290 <span style=''>          case 0 =&gt; (RandomSource.XO_RO_SHI_RO_128_PP, 0L)
</span>291 <span style=''>          case 1 =&gt; (getRandom(exps(0)), 0L)
</span>292 <span style=''>          case 2 =&gt; (getRandom(exps(0)), getLong(exps(1)))
</span>293 <span style=''>          case _ =&gt; literalsNeeded
</span>294 <span style=''>        }
</span>295 <span style=''>
</span>296 <span style=''>      </span><span style='background: #AEF1AE'>RandomLongs.create(randomSource, seed)</span><span style=''>
</span>297 <span style=''>    }
</span>298 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;rng&quot;, lrf)</span><span style=''>
</span>299 <span style=''>
</span>300 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;rngUUID&quot;, (exps: Seq[Expression]) =&gt; RngUUIDExpression(exps.head))</span><span style=''>
</span>301 <span style=''>
</span>302 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;longPair&quot;, (exps: Seq[Expression]) =&gt; LongPairExpression(exps(0), exps(1)))</span><span style=''>
</span>303 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;longPairFromUUID&quot;, (exps: Seq[Expression]) =&gt; UUIDToLongsExpression(exps.head))</span><span style=''>
</span>304 <span style=''>
</span>305 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;smallBloom&quot;, (exps: Seq[Expression]) =&gt; ParquetAggregator(exps(0), exps(1), exps(2)))</span><span style=''>
</span>306 <span style=''>
</span>307 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;bigBloom&quot;, (exps: Seq[Expression]) =&gt; BucketedArrayParquetAggregator(exps(0), exps(1), exps(2), exps(3)))</span><span style=''>
</span>308 <span style=''>
</span>309 <span style=''>    val longPairEqual = (exps: Seq[Expression]) =&gt; {
</span>310 <span style=''>      val Seq(Literal(a, StringType), Literal(b, StringType)) = exps
</span>311 <span style=''>
</span>312 <span style=''>      def lower(a: Any) = </span><span style='background: #F0ADAD'>UnresolvedAttribute(s&quot;${a}_lower&quot;)</span><span style=''>
</span>313 <span style=''>
</span>314 <span style=''>      def higher(a: Any) = </span><span style='background: #F0ADAD'>UnresolvedAttribute(s&quot;${a}_higher&quot;)</span><span style=''>
</span>315 <span style=''>
</span>316 <span style=''>      </span><span style='background: #F0ADAD'>And(EqualTo(lower(a), lower(b)), EqualTo(higher(a), higher(b)))</span><span style=''>
</span>317 <span style=''>    }
</span>318 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;longPairEqual&quot;, longPairEqual)</span><span style=''>
</span>319 <span style=''>
</span>320 <span style=''>    val idEqual = (exps: Seq[Expression]) =&gt; {
</span>321 <span style=''>      val Seq(Literal(a, StringType), Literal(b, StringType)) = exps
</span>322 <span style=''>
</span>323 <span style=''>      def attr(a: Any, field: String) = </span><span style='background: #AEF1AE'>UnresolvedAttribute(s&quot;${a}_$field&quot;)</span><span style=''>
</span>324 <span style=''>
</span>325 <span style=''>      </span><span style='background: #AEF1AE'>And(And(EqualTo(attr(a,&quot;base&quot;), attr(b, &quot;base&quot;)),
</span>326 <span style=''></span><span style='background: #AEF1AE'>        EqualTo(attr(a,&quot;i0&quot;), attr(b, &quot;i0&quot;))),
</span>327 <span style=''></span><span style='background: #AEF1AE'>        EqualTo(attr(a,&quot;i1&quot;), attr(b, &quot;i1&quot;)))</span><span style=''>
</span>328 <span style=''>    }
</span>329 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;idEqual&quot;, idEqual)</span><span style=''>
</span>330 <span style=''>
</span>331 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;as_uuid&quot;, exps =&gt; AsUUID(exps(0), exps(1)))</span><span style=''>
</span>332 <span style=''>
</span>333 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;ruleSuiteResultDetails&quot;, (exps: Seq[Expression]) =&gt; impl.RuleSuiteResultDetails(exps(0)))</span><span style=''>
</span>334 <span style=''>
</span>335 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;digestToLongsStruct&quot;, digestToLongs(true))</span><span style=''>
</span>336 <span style=''>
</span>337 <span style=''>    def digestToLongs(asStruct: Boolean = true) = (exps: Seq[Expression]) =&gt; {
</span>338 <span style=''>      val digestImpl = </span><span style='background: #AEF1AE'>getString(exps.head)</span><span style=''>
</span>339 <span style=''>      </span><span style='background: #AEF1AE'>HashFunctionsExpression(exps.tail, digestImpl, asStruct, MessageDigestFactory(digestImpl))</span><span style=''>
</span>340 <span style=''>    }
</span>341 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;digestToLongs&quot;, digestToLongs(false))</span><span style=''>
</span>342 <span style=''>
</span>343 <span style=''>    def fieldBasedID(factory: String =&gt; DigestFactory) = (exps: Seq[Expression]) =&gt;
</span>344 <span style=''>        </span><span style='background: #AEF1AE'>exps.size</span><span style=''> match {
</span>345 <span style=''>          case a if </span><span style='background: #AEF1AE'>a &gt; 2</span><span style=''> =&gt;
</span>346 <span style=''>            val digestImpl = </span><span style='background: #AEF1AE'>getString(exps(1))</span><span style=''>
</span>347 <span style=''>            </span><span style='background: #AEF1AE'>GenericLongBasedIDExpression(model.FieldBasedID,
</span>348 <span style=''></span><span style='background: #AEF1AE'>              HashFunctionsExpression(exps.drop(2), digestImpl, true, factory(digestImpl)), getString(exps.head))</span><span style=''>
</span>349 <span style=''>
</span>350 <span style=''>          case _ =&gt; </span><span style='background: #F0ADAD'>literalsNeeded</span><span style=''>
</span>351 <span style=''>        }
</span>352 <span style=''>
</span>353 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;fieldBasedID&quot;, fieldBasedID(MessageDigestFactory))</span><span style=''>
</span>354 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;zaLongsFieldBasedID&quot;, fieldBasedID(ZALongTupleHashFunctionFactory))</span><span style=''>
</span>355 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;zaFieldBasedID&quot;, fieldBasedID(ZALongHashFunctionFactory))</span><span style=''>
</span>356 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;hashFieldBasedID&quot;, fieldBasedID(HashFunctionFactory(_)))</span><span style=''>
</span>357 <span style=''>
</span>358 <span style=''>    val providedID = (exps: Seq[Expression]) =&gt;
</span>359 <span style=''>      </span><span style='background: #AEF1AE'>exps.size</span><span style=''> match {
</span>360 <span style=''>        case 2 =&gt;
</span>361 <span style=''>          </span><span style='background: #AEF1AE'>GenericLongBasedIDExpression(model.ProvidedID,
</span>362 <span style=''></span><span style='background: #AEF1AE'>            exps(1), getString(exps.head))</span><span style=''>
</span>363 <span style=''>
</span>364 <span style=''>        case _ =&gt; </span><span style='background: #F0ADAD'>literalsNeeded</span><span style=''>
</span>365 <span style=''>      }
</span>366 <span style=''>
</span>367 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;providedID&quot;, providedID)</span><span style=''>
</span>368 <span style=''>
</span>369 <span style=''>    val prefixedToLongPair = (exps: Seq[Expression]) =&gt;
</span>370 <span style=''>      </span><span style='background: #AEF1AE'>exps.size</span><span style=''> match {
</span>371 <span style=''>        case 2 =&gt;
</span>372 <span style=''>          </span><span style='background: #AEF1AE'>PrefixedToLongPair(exps(1), getString(exps.head))</span><span style=''>
</span>373 <span style=''>
</span>374 <span style=''>        case _ =&gt; </span><span style='background: #F0ADAD'>literalsNeeded</span><span style=''>
</span>375 <span style=''>      }
</span>376 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;prefixedToLongPair&quot;, prefixedToLongPair)</span><span style=''>
</span>377 <span style=''>
</span>378 <span style=''>    val rngID = (exps: Seq[Expression]) =&gt; {
</span>379 <span style=''>      val (randomSource, seed: Long, prefix) =
</span>380 <span style=''>        exps.size match {
</span>381 <span style=''>          case 1 =&gt; ( RandomSource.XO_RO_SHI_RO_128_PP, 0L, getString(exps.head))
</span>382 <span style=''>          case 2 =&gt; ( getRandom(exps(1)), 0L,  getString(exps.head))
</span>383 <span style=''>          case 3 =&gt; ( getRandom(exps(1)), getLong(exps(2)),  getString(exps.head))
</span>384 <span style=''>          case _ =&gt; literalsNeeded
</span>385 <span style=''>        }
</span>386 <span style=''>
</span>387 <span style=''>      </span><span style='background: #AEF1AE'>GenericLongBasedIDExpression(model.RandomID,
</span>388 <span style=''></span><span style='background: #AEF1AE'>        RandLongsWithJump(seed, randomSource), prefix)</span><span style=''> // only jumpables work
</span>389 <span style=''>    }
</span>390 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;rngID&quot;, rngID)</span><span style=''>
</span>391 <span style=''>
</span>392 <span style=''>    val uniqueID = (exps: Seq[Expression]) =&gt; {
</span>393 <span style=''>      val (prefix) =
</span>394 <span style=''>        </span><span style='background: #AEF1AE'>exps.size</span><span style=''> match {
</span>395 <span style=''>          case 1 =&gt; </span><span style='background: #AEF1AE'>getString(exps.head)</span><span style=''>
</span>396 <span style=''>          case _ =&gt; </span><span style='background: #F0ADAD'>literalsNeeded</span><span style=''>
</span>397 <span style=''>        }
</span>398 <span style=''>
</span>399 <span style=''>      </span><span style='background: #AEF1AE'>GuaranteedUniqueIdIDExpression(
</span>400 <span style=''></span><span style='background: #AEF1AE'>        GuaranteedUniqueID()
</span>401 <span style=''></span><span style='background: #AEF1AE'>        , prefix
</span>402 <span style=''></span><span style='background: #AEF1AE'>      )</span><span style=''>
</span>403 <span style=''>    }
</span>404 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;uniqueID&quot;, uniqueID)</span><span style=''>
</span>405 <span style=''>
</span>406 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;id_size&quot;, exps =&gt; SizeOfIDString(exps.head))</span><span style=''>
</span>407 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;id_base64&quot;, {
</span>408 <span style=''></span><span style='background: #AEF1AE'>      case Seq(e) =&gt; AsBase64Struct(e)
</span>409 <span style=''></span><span style='background: #AEF1AE'>      case s =&gt; AsBase64Fields(s)
</span>410 <span style=''></span><span style='background: #AEF1AE'>    })</span><span style=''>
</span>411 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;id_from_base64&quot;, {
</span>412 <span style=''></span><span style='background: #AEF1AE'>      case Seq(e) =&gt; IDFromBase64(e, 2) // default assumption
</span>413 <span style=''></span><span style='background: #AEF1AE'>      case Seq(e, s) =&gt; IDFromBase64(e, getInteger(s))
</span>414 <span style=''></span><span style='background: #AEF1AE'>    })</span><span style=''>
</span>415 <span style=''>
</span>416 <span style=''>    val Murmur3_128_64 = (exps: Seq[Expression]) =&gt; {
</span>417 <span style=''>      val (prefix) =
</span>418 <span style=''>        </span><span style='background: #AEF1AE'>exps.size</span><span style=''> match {
</span>419 <span style=''>          case a if </span><span style='background: #AEF1AE'>a &lt; 2</span><span style=''> =&gt; </span><span style='background: #F0ADAD'>literalsNeeded</span><span style=''>
</span>420 <span style=''>          case _ =&gt; </span><span style='background: #AEF1AE'>getString(exps.head)</span><span style=''>
</span>421 <span style=''>        }
</span>422 <span style=''>      </span><span style='background: #AEF1AE'>GenericLongBasedIDExpression(model.FieldBasedID,
</span>423 <span style=''></span><span style='background: #AEF1AE'>        HashFunctionsExpression(exps.tail, &quot;IGNORED&quot;, true, HashFunctionFactory(&quot;IGNORED&quot;)), prefix)</span><span style=''>
</span>424 <span style=''>    }
</span>425 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;murmur3ID&quot;, Murmur3_128_64)</span><span style=''>
</span>426 <span style=''>
</span>427 <span style=''>    def hashWithF(asStruct: Boolean) = (exps: Seq[Expression]) =&gt; {
</span>428 <span style=''>      val (impl) =
</span>429 <span style=''>        </span><span style='background: #AEF1AE'>exps.size</span><span style=''> match {
</span>430 <span style=''>          case a if </span><span style='background: #AEF1AE'>a &lt; 2</span><span style=''> =&gt; </span><span style='background: #F0ADAD'>literalsNeeded</span><span style=''>
</span>431 <span style=''>          case _ =&gt; </span><span style='background: #AEF1AE'>getString(exps.head)</span><span style=''>
</span>432 <span style=''>        }
</span>433 <span style=''>      </span><span style='background: #AEF1AE'>HashFunctionsExpression(exps.tail, impl, asStruct, HashFunctionFactory(impl))</span><span style=''>
</span>434 <span style=''>    }
</span>435 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;hashWith&quot;, hashWithF(false))</span><span style=''>
</span>436 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;hashWithStruct&quot;, hashWithF(true))</span><span style=''>
</span>437 <span style=''>
</span>438 <span style=''>    def zahashF(asStruct: Boolean) = (exps: Seq[Expression]) =&gt; {
</span>439 <span style=''>      val (digestImpl) =
</span>440 <span style=''>        </span><span style='background: #AEF1AE'>exps.size</span><span style=''> match {
</span>441 <span style=''>          case a if </span><span style='background: #AEF1AE'>a &lt; 2</span><span style=''> =&gt; </span><span style='background: #F0ADAD'>literalsNeeded</span><span style=''>
</span>442 <span style=''>          case _ =&gt; </span><span style='background: #AEF1AE'>getString(exps.head)</span><span style=''>
</span>443 <span style=''>        }
</span>444 <span style=''>      </span><span style='background: #AEF1AE'>HashFunctionsExpression(exps.tail, digestImpl, asStruct, ZALongHashFunctionFactory(digestImpl))</span><span style=''>
</span>445 <span style=''>    }
</span>446 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;zaHashWith&quot;, zahashF(false))</span><span style=''> // 64bit only, not a great id choice
</span>447 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;zaHashWithStruct&quot;, zahashF(true))</span><span style=''> // 64bit only, not a great id choice
</span>448 <span style=''>
</span>449 <span style=''>    def zaTuplehashF(asStruct: Boolean) = (exps: Seq[Expression]) =&gt; {
</span>450 <span style=''>      val (digestImpl) =
</span>451 <span style=''>        </span><span style='background: #AEF1AE'>exps.size</span><span style=''> match {
</span>452 <span style=''>          case a if </span><span style='background: #AEF1AE'>a &lt; 2</span><span style=''> =&gt; </span><span style='background: #F0ADAD'>literalsNeeded</span><span style=''>
</span>453 <span style=''>          case _ =&gt; </span><span style='background: #AEF1AE'>getString(exps.head)</span><span style=''>
</span>454 <span style=''>        }
</span>455 <span style=''>      </span><span style='background: #AEF1AE'>HashFunctionsExpression(exps.tail, digestImpl, asStruct, ZALongTupleHashFunctionFactory(digestImpl))</span><span style=''>
</span>456 <span style=''>    }
</span>457 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;zaHashLongsWith&quot;, zaTuplehashF(false))</span><span style=''>
</span>458 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;zaHashLongsWithStruct&quot;, zaTuplehashF(true))</span><span style=''>
</span>459 <span style=''>
</span>460 <span style=''>    // here to stop these functions being used and allow validation
</span>461 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;coalesceIfAttributesMissing&quot;, _ =&gt; </span><span style='background: #F0ADAD'>qualityException(&quot;coalesceIf functions cannot be created&quot;)</span><span style='background: #AEF1AE'> )</span><span style=''>
</span>462 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;coalesceIfAttributesMissingDisable&quot;, _ =&gt; </span><span style='background: #F0ADAD'>qualityException(&quot;coalesceIf functions cannot be created&quot;)</span><span style='background: #AEF1AE'> )</span><span style=''>
</span>463 <span style=''>
</span>464 <span style=''>    // The MSE library adds this lens functionality, 3.1.1 introduces this to dsl but neither does an sql interface
</span>465 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;updateField&quot;, StructFunctions.withFieldFunction)</span><span style=''>
</span>466 <span style=''>
</span>467 <span style=''>    def msgAndExpr(msgDefault: String, exps: Seq[Expression]) = exps match {
</span>468 <span style=''>      case Seq(Literal(str: UTF8String, StringType), e: Expression) =&gt;
</span>469 <span style=''>        </span><span style='background: #AEF1AE'>(str.toString, e)</span><span style=''>
</span>470 <span style=''>      case Seq(e: Expression) =&gt;
</span>471 <span style=''>        </span><span style='background: #AEF1AE'>(msgDefault, e)</span><span style=''>
</span>472 <span style=''>    }
</span>473 <span style=''>
</span>474 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;printCode&quot;, (exps: Seq[Expression]) =&gt; {
</span>475 <span style=''></span><span style='background: #AEF1AE'>      val (msg, exp) = msgAndExpr(PrintCode(exps(0)).msg, exps)
</span>476 <span style=''></span><span style='background: #AEF1AE'>      PrintCode(exp, msg, writer)
</span>477 <span style=''></span><span style='background: #AEF1AE'>    })</span><span style=''>
</span>478 <span style=''>    </span><span style='background: #AEF1AE'>register(&quot;printExpr&quot;, (exps: Seq[Expression]) =&gt; {
</span>479 <span style=''></span><span style='background: #AEF1AE'>      val (msg, exp) = msgAndExpr(&quot;Expression toStr is -&gt;&quot;, exps)
</span>480 <span style=''></span><span style='background: #AEF1AE'>      writer(s&quot;$msg $exp .  Sql is ${exp.sql}&quot;)
</span>481 <span style=''></span><span style='background: #AEF1AE'>      exp
</span>482 <span style=''></span><span style='background: #AEF1AE'>    })</span><span style=''>
</span>483 <span style=''>  }
</span>484 <span style=''>
</span>485 <span style=''>}
</span>486 <span style=''>
</span>487 <span style=''>object RuleRunnerFunctions {
</span>488 <span style=''>
</span>489 <span style=''>  protected[quality] def literalsNeeded = </span><span style='background: #F0ADAD'>qualityException(&quot;Cannot setup expression with non-literals&quot;)</span><span style=''>
</span>490 <span style=''>  protected[quality] def getLong(exp: Expression) =
</span>491 <span style=''>    exp match {
</span>492 <span style=''>      case Literal(seed: Long, LongType) =&gt; seed
</span>493 <span style=''>      case _ =&gt; </span><span style='background: #F0ADAD'>literalsNeeded</span><span style=''>
</span>494 <span style=''>    }
</span>495 <span style=''>  protected[quality] def getInteger(exp: Expression) =
</span>496 <span style=''>    exp match {
</span>497 <span style=''>      case Literal(seed: Int, IntegerType) =&gt; seed
</span>498 <span style=''>      case _ =&gt; </span><span style='background: #F0ADAD'>literalsNeeded</span><span style=''>
</span>499 <span style=''>    }
</span>500 <span style=''>  protected[quality] def getString(exp: Expression) =
</span>501 <span style=''>    exp match {
</span>502 <span style=''>      case Literal(str: UTF8String, StringType) =&gt; </span><span style='background: #AEF1AE'>str.toString()</span><span style=''>
</span>503 <span style=''>      case _ =&gt; </span><span style='background: #F0ADAD'>literalsNeeded</span><span style=''>
</span>504 <span style=''>    }
</span>505 <span style=''>
</span>506 <span style=''>  protected[quality] def flattenExpressions(ruleSuite: RuleSuite): Seq[Expression] =
</span>507 <span style=''>    </span><span style='background: #AEF1AE'>ruleSuite.ruleSets.flatMap( ruleSet =&gt; ruleSet.rules.map(rule =&gt;
</span>508 <span style=''></span><span style='background: #AEF1AE'>      rule.expression match {
</span>509 <span style=''></span><span style='background: #AEF1AE'>        case r: ExprLogic =&gt; r.expr // only ExprLogic are possible here
</span>510 <span style=''></span><span style='background: #AEF1AE'>      }))</span><span style=''>
</span>511 <span style=''>
</span>512 <span style=''>  val qualityFunctions = </span><span style='background: #AEF1AE'>Set(&quot;murmur3ID&quot;,&quot;uniqueID&quot;,&quot;rngID&quot;,&quot;providedID&quot;,&quot;fieldBasedID&quot;,
</span>513 <span style=''></span><span style='background: #AEF1AE'>    &quot;digestToLongs&quot;,&quot;digestToLongsStruct&quot;,&quot;ruleSuiteResultDetails&quot;,&quot;idEqual&quot;,&quot;longPairEqual&quot;,&quot;bigBloom&quot;,&quot;smallBloom&quot;,
</span>514 <span style=''></span><span style='background: #AEF1AE'>    &quot;longPairFromUUID&quot;,&quot;longPair&quot;,&quot;rngUUID&quot;,&quot;rng&quot;,&quot;rngBytes&quot;,&quot;returnSum&quot;,&quot;sumWith&quot;,&quot;resultsWith&quot;,
</span>515 <span style=''></span><span style='background: #AEF1AE'>    &quot;inc&quot;,&quot;meanF&quot;,&quot;aggExpr&quot;,&quot;passed&quot;,&quot;failed&quot;,&quot;softFailed&quot;,&quot;disabledRule&quot;,&quot;packInts&quot;,&quot;unpack&quot;,
</span>516 <span style=''></span><span style='background: #AEF1AE'>    &quot;unpackIdTriple&quot;,&quot;softFail&quot;,&quot;probability&quot;,&quot;flattenResults&quot;,&quot;flattenRuleResults&quot;, &quot;flattenFolderResults&quot;, &quot;probabilityIn&quot;,
</span>517 <span style=''></span><span style='background: #AEF1AE'>    &quot;mapLookup&quot;,&quot;mapContains&quot;,&quot;saferLongPair&quot;,&quot;hashWith&quot;,&quot;hashWithStruct&quot;,&quot;zaHashWith&quot;, &quot;zaHashLongsWith&quot;,
</span>518 <span style=''></span><span style='background: #AEF1AE'>    &quot;hashFieldBasedID&quot;,&quot;zaLongsFieldBasedID&quot;,&quot;zaHashLongsWithStruct&quot;, &quot;zaHashWithStruct&quot;, &quot;zaFieldBasedID&quot;, &quot;prefixedToLongPair&quot;,
</span>519 <span style=''></span><span style='background: #AEF1AE'>    &quot;coalesceIfAttributesMissing&quot;, &quot;coalesceIfAttributesMissingDisable&quot;, &quot;updateField&quot;, LambdaFunctions.PlaceHolder,
</span>520 <span style=''></span><span style='background: #AEF1AE'>    LambdaFunctions.Lambda, LambdaFunctions.CallFun, &quot;printExpr&quot;, &quot;printCode&quot;, &quot;comparableMaps&quot;, &quot;reverseComparableMaps&quot;, &quot;as_uuid&quot;,
</span>521 <span style=''></span><span style='background: #AEF1AE'>    &quot;id_size&quot;, &quot;id_base64&quot;, &quot;id_from_base64&quot;
</span>522 <span style=''></span><span style='background: #AEF1AE'>  )</span><span style=''>
</span>523 <span style=''>}
</span></pre>
          </div>
          <div class="tab-pane" id="statementlist">
            <table cellspacing="0" cellpadding="0" class="table statementlist">
      <tr>
        <th>Line</th>
        <th>Stmt Id</th>
        <th>Pos</th>
        <th>Tree</th>
        <th>Symbol</th>
        <th>Tests</th>
        <th>Code</th>
      </tr><tr>
        <td>
          27
        </td>
        <td>
          2066
        </td>
        <td>
          1698
          -
          1759
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.DecimalType.apply(org.apache.spark.sql.types.DecimalType.MAX_PRECISION, org.apache.spark.sql.types.DecimalType.MAX_SCALE)
        </td>
      </tr><tr>
        <td>
          27
        </td>
        <td>
          2065
        </td>
        <td>
          1737
          -
          1758
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.MAX_SCALE
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.DecimalType.MAX_SCALE
        </td>
      </tr><tr>
        <td>
          27
        </td>
        <td>
          2064
        </td>
        <td>
          1710
          -
          1735
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.MAX_PRECISION
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.DecimalType.MAX_PRECISION
        </td>
      </tr><tr>
        <td>
          29
        </td>
        <td>
          2067
        </td>
        <td>
          1803
          -
          1807
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.None
        </td>
      </tr><tr>
        <td>
          39
        </td>
        <td>
          2069
        </td>
        <td>
          2251
          -
          2282
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.defaultAdd
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRunnerFunctionsImport.this.defaultAdd(dataType, extension)
        </td>
      </tr><tr>
        <td>
          39
        </td>
        <td>
          2071
        </td>
        <td>
          2178
          -
          2284
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[(org.apache.spark.sql.catalyst.expressions.Expression, org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.qualityFunctions.MapMerge](((left: org.apache.spark.sql.catalyst.expressions.Expression, right: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.qualityFunctions.MapMerge.apply(scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](left, right), ((dataType: org.apache.spark.sql.types.DataType) =&gt; RuleRunnerFunctionsImport.this.defaultAdd(dataType, extension)))))
        </td>
      </tr><tr>
        <td>
          39
        </td>
        <td>
          2068
        </td>
        <td>
          2209
          -
          2225
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](left, right)
        </td>
      </tr><tr>
        <td>
          39
        </td>
        <td>
          2070
        </td>
        <td>
          2200
          -
          2283
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.MapMerge.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.MapMerge.apply(scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](left, right), ((dataType: org.apache.spark.sql.types.DataType) =&gt; RuleRunnerFunctionsImport.this.defaultAdd(dataType, extension)))
        </td>
      </tr><tr>
        <td>
          41
        </td>
        <td>
          2073
        </td>
        <td>
          2346
          -
          2391
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[(org.apache.spark.sql.catalyst.expressions.Expression, org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.catalyst.expressions.Expression](((left: org.apache.spark.sql.catalyst.expressions.Expression, right: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.QualitySparkUtils.add(left, right, null)))
        </td>
      </tr><tr>
        <td>
          41
        </td>
        <td>
          2072
        </td>
        <td>
          2368
          -
          2390
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.add
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.QualitySparkUtils.add(left, right, null)
        </td>
      </tr><tr>
        <td>
          43
        </td>
        <td>
          2075
        </td>
        <td>
          2429
          -
          2471
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[(org.apache.spark.sql.catalyst.expressions.Expression, org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.catalyst.expressions.Expression](((left: org.apache.spark.sql.catalyst.expressions.Expression, right: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; org.apache.spark.sql.QualitySparkUtils.add(left, right, a)))
        </td>
      </tr><tr>
        <td>
          43
        </td>
        <td>
          2074
        </td>
        <td>
          2451
          -
          2470
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.add
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.QualitySparkUtils.add(left, right, a)
        </td>
      </tr><tr>
        <td>
          44
        </td>
        <td>
          2076
        </td>
        <td>
          2488
          -
          2507
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function1.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          extension.apply(dataType)
        </td>
      </tr><tr>
        <td>
          55
        </td>
        <td>
          2077
        </td>
        <td>
          2756
          -
          2770
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[com.sparkutils.quality.impl.EmptyMap.type](EmptyMap)
        </td>
      </tr><tr>
        <td>
          56
        </td>
        <td>
          2078
        </td>
        <td>
          2811
          -
          2819
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[Long](0L)
        </td>
      </tr><tr>
        <td>
          57
        </td>
        <td>
          2079
        </td>
        <td>
          2848
          -
          2857
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[Double](0.0)
        </td>
      </tr><tr>
        <td>
          58
        </td>
        <td>
          2084
        </td>
        <td>
          2887
          -
          2938
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[org.apache.spark.sql.types.Decimal](org.apache.spark.sql.types.Decimal.createUnsafe(0L, d.precision, d.scale))
        </td>
      </tr><tr>
        <td>
          58
        </td>
        <td>
          2080
        </td>
        <td>
          2913
          -
          2914
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0L
        </td>
      </tr><tr>
        <td>
          58
        </td>
        <td>
          2083
        </td>
        <td>
          2892
          -
          2937
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.Decimal.createUnsafe
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.Decimal.createUnsafe(0L, d.precision, d.scale)
        </td>
      </tr><tr>
        <td>
          58
        </td>
        <td>
          2082
        </td>
        <td>
          2929
          -
          2936
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.scale
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          d.scale
        </td>
      </tr><tr>
        <td>
          58
        </td>
        <td>
          2081
        </td>
        <td>
          2916
          -
          2927
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.precision
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          d.precision
        </td>
      </tr><tr>
        <td>
          59
        </td>
        <td>
          2085
        </td>
        <td>
          2955
          -
          2959
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.None
        </td>
      </tr><tr>
        <td>
          71
        </td>
        <td>
          2087
        </td>
        <td>
          3261
          -
          3307
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[org.apache.spark.sql.types.DataType](org.apache.spark.sql.types.DataType.fromDDL(string))
        </td>
      </tr><tr>
        <td>
          71
        </td>
        <td>
          2088
        </td>
        <td>
          3261
          -
          3307
        </td>
        <td>
          Block
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[org.apache.spark.sql.types.DataType](org.apache.spark.sql.types.DataType.fromDDL(string))
        </td>
      </tr><tr>
        <td>
          72
        </td>
        <td>
          2086
        </td>
        <td>
          3275
          -
          3299
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.DataType.fromDDL
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.DataType.fromDDL(string)
        </td>
      </tr><tr>
        <td>
          75
        </td>
        <td>
          2089
        </td>
        <td>
          3349
          -
          3353
        </td>
        <td>
          Select
        </td>
        <td>
          scala.None
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.None
        </td>
      </tr><tr>
        <td>
          78
        </td>
        <td>
          2090
        </td>
        <td>
          3404
          -
          3533
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;inc(\'DDL\', generic expression) is not supported in NO_REWRITE mode, use inc(generic expression) without NO_REWRITE mode enabled&quot;
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          2093
        </td>
        <td>
          4892
          -
          4935
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ComparableMapConverter.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.util.ComparableMapConverter.apply(exps.apply(0), mapCompare)
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          2092
        </td>
        <td>
          4915
          -
          4922
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          2091
        </td>
        <td>
          4866
          -
          4882
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;comparableMaps&quot;
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          2094
        </td>
        <td>
          4857
          -
          4936
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;comparableMaps&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.impl.util.ComparableMapConverter.apply(exps.apply(0), mapCompare)))
        </td>
      </tr><tr>
        <td>
          97
        </td>
        <td>
          2096
        </td>
        <td>
          5005
          -
          5012
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          97
        </td>
        <td>
          2095
        </td>
        <td>
          4950
          -
          4973
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;reverseComparableMaps&quot;
        </td>
      </tr><tr>
        <td>
          97
        </td>
        <td>
          2098
        </td>
        <td>
          4941
          -
          5014
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;reverseComparableMaps&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.impl.util.ComparableMapReverser.apply(exps.apply(0))))
        </td>
      </tr><tr>
        <td>
          97
        </td>
        <td>
          2097
        </td>
        <td>
          4983
          -
          5013
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.util.ComparableMapReverser.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.util.ComparableMapReverser.apply(exps.apply(0))
        </td>
      </tr><tr>
        <td>
          99
        </td>
        <td>
          2100
        </td>
        <td>
          5055
          -
          5081
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.ProbabilityExpr.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ProbabilityExpr.apply(exps.head)
        </td>
      </tr><tr>
        <td>
          99
        </td>
        <td>
          2099
        </td>
        <td>
          5071
          -
          5080
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          100
        </td>
        <td>
          2101
        </td>
        <td>
          5086
          -
          5112
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;probability&quot;, f)
        </td>
      </tr><tr>
        <td>
          101
        </td>
        <td>
          2102
        </td>
        <td>
          5178
          -
          5187
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          101
        </td>
        <td>
          2104
        </td>
        <td>
          5153
          -
          5225
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.FlattenResultsExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          FlattenResultsExpression.apply(exps.head, FlattenStruct.ruleSuiteDeserializer)
        </td>
      </tr><tr>
        <td>
          101
        </td>
        <td>
          2103
        </td>
        <td>
          5189
          -
          5224
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.FlattenStruct.ruleSuiteDeserializer
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          FlattenStruct.ruleSuiteDeserializer
        </td>
      </tr><tr>
        <td>
          102
        </td>
        <td>
          2105
        </td>
        <td>
          5230
          -
          5260
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;flattenResults&quot;, ff)
        </td>
      </tr><tr>
        <td>
          103
        </td>
        <td>
          2107
        </td>
        <td>
          5334
          -
          5341
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          103
        </td>
        <td>
          2110
        </td>
        <td>
          5265
          -
          5380
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;flattenRuleResults&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; FlattenRulesResultsExpression.apply(exps.apply(0), FlattenStruct.ruleSuiteDeserializer)))
        </td>
      </tr><tr>
        <td>
          103
        </td>
        <td>
          2109
        </td>
        <td>
          5304
          -
          5379
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.FlattenRulesResultsExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          FlattenRulesResultsExpression.apply(exps.apply(0), FlattenStruct.ruleSuiteDeserializer)
        </td>
      </tr><tr>
        <td>
          103
        </td>
        <td>
          2106
        </td>
        <td>
          5274
          -
          5294
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;flattenRuleResults&quot;
        </td>
      </tr><tr>
        <td>
          103
        </td>
        <td>
          2108
        </td>
        <td>
          5343
          -
          5378
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.FlattenStruct.ruleSuiteDeserializer
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          FlattenStruct.ruleSuiteDeserializer
        </td>
      </tr><tr>
        <td>
          104
        </td>
        <td>
          2111
        </td>
        <td>
          5394
          -
          5416
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;flattenFolderResults&quot;
        </td>
      </tr><tr>
        <td>
          104
        </td>
        <td>
          2113
        </td>
        <td>
          5466
          -
          5501
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.FlattenStruct.ruleSuiteDeserializer
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          FlattenStruct.ruleSuiteDeserializer
        </td>
      </tr><tr>
        <td>
          104
        </td>
        <td>
          2112
        </td>
        <td>
          5457
          -
          5464
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          104
        </td>
        <td>
          2115
        </td>
        <td>
          5385
          -
          5503
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;flattenFolderResults&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; FlattenFolderResultsExpression.apply(exps.apply(0), FlattenStruct.ruleSuiteDeserializer)))
        </td>
      </tr><tr>
        <td>
          104
        </td>
        <td>
          2114
        </td>
        <td>
          5426
          -
          5502
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.FlattenFolderResultsExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          FlattenFolderResultsExpression.apply(exps.apply(0), FlattenStruct.ruleSuiteDeserializer)
        </td>
      </tr><tr>
        <td>
          106
        </td>
        <td>
          2116
        </td>
        <td>
          5518
          -
          5526
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;passed&quot;
        </td>
      </tr><tr>
        <td>
          106
        </td>
        <td>
          2118
        </td>
        <td>
          5509
          -
          5567
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;passed&quot;, ((x$2: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.`package`.PassedExpr))
        </td>
      </tr><tr>
        <td>
          106
        </td>
        <td>
          2117
        </td>
        <td>
          5533
          -
          5566
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerImports.PassedExpr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.`package`.PassedExpr
        </td>
      </tr><tr>
        <td>
          107
        </td>
        <td>
          2120
        </td>
        <td>
          5596
          -
          5629
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerImports.FailedExpr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.`package`.FailedExpr
        </td>
      </tr><tr>
        <td>
          107
        </td>
        <td>
          2119
        </td>
        <td>
          5581
          -
          5589
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;failed&quot;
        </td>
      </tr><tr>
        <td>
          107
        </td>
        <td>
          2121
        </td>
        <td>
          5572
          -
          5630
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;failed&quot;, ((x$3: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.`package`.FailedExpr))
        </td>
      </tr><tr>
        <td>
          108
        </td>
        <td>
          2122
        </td>
        <td>
          5644
          -
          5656
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;softFailed&quot;
        </td>
      </tr><tr>
        <td>
          108
        </td>
        <td>
          2124
        </td>
        <td>
          5635
          -
          5701
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;softFailed&quot;, ((x$4: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.`package`.SoftFailedExpr))
        </td>
      </tr><tr>
        <td>
          108
        </td>
        <td>
          2123
        </td>
        <td>
          5663
          -
          5700
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerImports.SoftFailedExpr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.`package`.SoftFailedExpr
        </td>
      </tr><tr>
        <td>
          109
        </td>
        <td>
          2125
        </td>
        <td>
          5715
          -
          5729
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;disabledRule&quot;
        </td>
      </tr><tr>
        <td>
          109
        </td>
        <td>
          2127
        </td>
        <td>
          5706
          -
          5776
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;disabledRule&quot;, ((x$5: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.`package`.DisabledRuleExpr))
        </td>
      </tr><tr>
        <td>
          109
        </td>
        <td>
          2126
        </td>
        <td>
          5736
          -
          5775
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerImports.DisabledRuleExpr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.`package`.DisabledRuleExpr
        </td>
      </tr><tr>
        <td>
          111
        </td>
        <td>
          2129
        </td>
        <td>
          5833
          -
          5840
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          111
        </td>
        <td>
          2128
        </td>
        <td>
          5824
          -
          5831
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          111
        </td>
        <td>
          2130
        </td>
        <td>
          5819
          -
          5841
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.Pack.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          Pack.apply(exps.apply(0), exps.apply(1))
        </td>
      </tr><tr>
        <td>
          112
        </td>
        <td>
          2131
        </td>
        <td>
          5846
          -
          5871
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;packInts&quot;, pif)
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          2133
        </td>
        <td>
          5915
          -
          5930
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.UnPack.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          UnPack.apply(exps.apply(0))
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          2132
        </td>
        <td>
          5922
          -
          5929
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          115
        </td>
        <td>
          2134
        </td>
        <td>
          5935
          -
          5959
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;unpack&quot;, upif)
        </td>
      </tr><tr>
        <td>
          117
        </td>
        <td>
          2136
        </td>
        <td>
          6004
          -
          6027
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.UnPackIdTriple.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          UnPackIdTriple.apply(exps.apply(0))
        </td>
      </tr><tr>
        <td>
          117
        </td>
        <td>
          2135
        </td>
        <td>
          6019
          -
          6026
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          118
        </td>
        <td>
          2137
        </td>
        <td>
          6032
          -
          6065
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;unpackIdTriple&quot;, uptif)
        </td>
      </tr><tr>
        <td>
          121
        </td>
        <td>
          2138
        </td>
        <td>
          6121
          -
          6130
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          121
        </td>
        <td>
          2139
        </td>
        <td>
          6108
          -
          6131
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.SoftFailExpr.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SoftFailExpr.apply(exps.head)
        </td>
      </tr><tr>
        <td>
          122
        </td>
        <td>
          2140
        </td>
        <td>
          6136
          -
          6160
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;softFail&quot;, sf)
        </td>
      </tr><tr>
        <td>
          126
        </td>
        <td>
          2141
        </td>
        <td>
          6288
          -
          6300
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.unsafe.types.UTF8String.toString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          str.toString()
        </td>
      </tr><tr>
        <td>
          131
        </td>
        <td>
          2146
        </td>
        <td>
          6432
          -
          6518
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Option.getOrElse
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          parseTypes.apply(str.toString()).getOrElse[org.apache.spark.sql.types.DataType](com.sparkutils.quality.QualityException.qualityException(scala.StringContext.apply(&quot;Could not parse the type &quot;, &quot;&quot;).s(str), com.sparkutils.quality.QualityException.qualityException$default$2))
        </td>
      </tr><tr>
        <td>
          131
        </td>
        <td>
          2143
        </td>
        <td>
          6484
          -
          6516
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;Could not parse the type &quot;, &quot;&quot;).s(str)
        </td>
      </tr><tr>
        <td>
          131
        </td>
        <td>
          2145
        </td>
        <td>
          6467
          -
          6517
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.sparkutils.quality.QualityException.qualityException(scala.StringContext.apply(&quot;Could not parse the type &quot;, &quot;&quot;).s(str), com.sparkutils.quality.QualityException.qualityException$default$2)
        </td>
      </tr><tr>
        <td>
          131
        </td>
        <td>
          2142
        </td>
        <td>
          6443
          -
          6455
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.unsafe.types.UTF8String.toString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          str.toString()
        </td>
      </tr><tr>
        <td>
          131
        </td>
        <td>
          2144
        </td>
        <td>
          6467
          -
          6467
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
      </tr><tr>
        <td>
          134
        </td>
        <td>
          2156
        </td>
        <td>
          6530
          -
          6795
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(org.apache.spark.sql.qualityFunctions.LambdaFunctions.PlaceHolder, ((x0$1: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; x0$1 match {
  case scala.collection.Seq.unapplySeq[org.apache.spark.sql.catalyst.expressions.Expression](&lt;unapply-selector&gt;) &lt;unapply&gt; ((e @ _), (value: Any, dataType: org.apache.spark.sql.types.DataType)org.apache.spark.sql.catalyst.expressions.Literal((bol @ (_: Boolean)), org.apache.spark.sql.types.BooleanType)) =&gt; org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply(parse(e), bol)
  case scala.collection.Seq.unapplySeq[org.apache.spark.sql.catalyst.expressions.Expression](&lt;unapply-selector&gt;) &lt;unapply&gt; ((e @ _)) =&gt; org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply(parse(e), org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply$default$2)
  case _ =&gt; org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply$default$2)
}))
        </td>
      </tr><tr>
        <td>
          134
        </td>
        <td>
          2147
        </td>
        <td>
          6539
          -
          6566
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.PlaceHolder
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.PlaceHolder
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          2149
        </td>
        <td>
          6635
          -
          6671
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply(parse(e), bol)
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          2148
        </td>
        <td>
          6657
          -
          6665
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.parse
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          parse(e)
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          2152
        </td>
        <td>
          6701
          -
          6732
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply(parse(e), org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply$default$2)
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          2151
        </td>
        <td>
          6701
          -
          6701
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply$default$2
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          2150
        </td>
        <td>
          6723
          -
          6731
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.parse
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          parse(e)
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          2155
        </td>
        <td>
          6757
          -
          6788
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply$default$2)
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          2154
        </td>
        <td>
          6757
          -
          6757
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.PlaceHolderExpression.apply$default$2
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          2153
        </td>
        <td>
          6779
          -
          6787
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          147
        </td>
        <td>
          2160
        </td>
        <td>
          6987
          -
          7215
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(org.apache.spark.sql.qualityFunctions.LambdaFunctions.Lambda, ((x0$2: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; x0$2 match {
  case scala.collection.Seq.unapplySeq[org.apache.spark.sql.catalyst.expressions.Expression](&lt;unapply-selector&gt;) &lt;unapply&gt; ((fun @ (_: org.apache.spark.sql.qualityFunctions.FunForward))) =&gt; {
    val res: org.apache.spark.sql.catalyst.expressions.Expression = org.apache.spark.sql.qualityFunctions.FunCall.apply(fun);
    res
  }
  case scala.collection.Seq.unapplySeq[org.apache.spark.sql.catalyst.expressions.Expression](&lt;unapply-selector&gt;) &lt;unapply&gt; ((fun @ (_: org.apache.spark.sql.qualityFunctions.FunN))) =&gt; {
    val res: org.apache.spark.sql.catalyst.expressions.Expression = fun.function;
    res
  }
}))
        </td>
      </tr><tr>
        <td>
          147
        </td>
        <td>
          2157
        </td>
        <td>
          6996
          -
          7018
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.Lambda
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.Lambda
        </td>
      </tr><tr>
        <td>
          149
        </td>
        <td>
          2158
        </td>
        <td>
          7075
          -
          7087
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunCall.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunCall.apply(fun)
        </td>
      </tr><tr>
        <td>
          153
        </td>
        <td>
          2159
        </td>
        <td>
          7184
          -
          7196
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.function
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          fun.function
        </td>
      </tr><tr>
        <td>
          157
        </td>
        <td>
          2161
        </td>
        <td>
          7230
          -
          7253
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun
        </td>
      </tr><tr>
        <td>
          157
        </td>
        <td>
          2170
        </td>
        <td>
          7221
          -
          7560
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun, ((x0$3: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; x0$3 match {
  case scala.`package`.+:.unapply[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](&lt;unapply-selector&gt;) &lt;unapply&gt; ((fun @ (arguments: Seq[org.apache.spark.sql.catalyst.expressions.Expression], function: org.apache.spark.sql.catalyst.expressions.Expression, name: Option[String], processed: Boolean, attemptCodeGen: Boolean)org.apache.spark.sql.qualityFunctions.FunN(_, (l @ (function: org.apache.spark.sql.catalyst.expressions.Expression, arguments: Seq[org.apache.spark.sql.catalyst.expressions.NamedExpression], hidden: Boolean)org.apache.spark.sql.catalyst.expressions.LambdaFunction((ff @ (_: org.apache.spark.sql.qualityFunctions.FunForward)), _, _)), _, _, _)), (args @ _)) =&gt; org.apache.spark.sql.qualityFunctions.LambdaFunctions.processTopCallFun(fun, l, ff, args)
  case (t @ _) =&gt; com.sparkutils.quality.QualityException.qualityException(scala.StringContext.apply(&quot;&quot;, &quot; should only be used to process partially applied functions returned by a user lambda, got &quot;, &quot; instead&quot;).s(org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun, t), com.sparkutils.quality.QualityException.qualityException$default$2)
}))
        </td>
      </tr><tr>
        <td>
          159
        </td>
        <td>
          2162
        </td>
        <td>
          7353
          -
          7388
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.processTopCallFun
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.processTopCallFun(fun, l, ff, args)
        </td>
      </tr><tr>
        <td>
          160
        </td>
        <td>
          2165
        </td>
        <td>
          7543
          -
          7552
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; instead&quot;
        </td>
      </tr><tr>
        <td>
          160
        </td>
        <td>
          2164
        </td>
        <td>
          7450
          -
          7542
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; should only be used to process partially applied functions returned by a user lambda, got &quot;
        </td>
      </tr><tr>
        <td>
          160
        </td>
        <td>
          2167
        </td>
        <td>
          7422
          -
          7552
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot; should only be used to process partially applied functions returned by a user lambda, got &quot;, &quot; instead&quot;).s(org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun, t)
        </td>
      </tr><tr>
        <td>
          160
        </td>
        <td>
          2169
        </td>
        <td>
          7405
          -
          7553
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.sparkutils.quality.QualityException.qualityException(scala.StringContext.apply(&quot;&quot;, &quot; should only be used to process partially applied functions returned by a user lambda, got &quot;, &quot; instead&quot;).s(org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun, t), com.sparkutils.quality.QualityException.qualityException$default$2)
        </td>
      </tr><tr>
        <td>
          160
        </td>
        <td>
          2163
        </td>
        <td>
          7424
          -
          7425
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          160
        </td>
        <td>
          2166
        </td>
        <td>
          7426
          -
          7449
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun
        </td>
      </tr><tr>
        <td>
          160
        </td>
        <td>
          2168
        </td>
        <td>
          7405
          -
          7405
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
      </tr><tr>
        <td>
          164
        </td>
        <td>
          2173
        </td>
        <td>
          7633
          -
          7633
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple4._3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$6._3
        </td>
      </tr><tr>
        <td>
          164
        </td>
        <td>
          2172
        </td>
        <td>
          7625
          -
          7625
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple4._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$6._2
        </td>
      </tr><tr>
        <td>
          164
        </td>
        <td>
          2174
        </td>
        <td>
          7638
          -
          7638
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple4._4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$6._4
        </td>
      </tr><tr>
        <td>
          164
        </td>
        <td>
          2171
        </td>
        <td>
          7616
          -
          7616
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple4._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$6._1
        </td>
      </tr><tr>
        <td>
          183
        </td>
        <td>
          2175
        </td>
        <td>
          8468
          -
          8528
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.aggregates.AggregateExpressions.apply(sumType, filter, sum, count, zero, add)
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          2176
        </td>
        <td>
          8539
          -
          8563
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;aggExpr&quot;, afx)
        </td>
      </tr><tr>
        <td>
          187
        </td>
        <td>
          2188
        </td>
        <td>
          8569
          -
          8859
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;sumWith&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; {
  &lt;synthetic&gt; &lt;artifact&gt; private[this] val x$7: (org.apache.spark.sql.types.DataType, org.apache.spark.sql.catalyst.expressions.Expression) = (exps.size match {
    case 1 =&gt; scala.Tuple2.apply[org.apache.spark.sql.types.LongType.type, org.apache.spark.sql.catalyst.expressions.Expression](org.apache.spark.sql.types.LongType, exps.apply(0))
    case 2 =&gt; scala.Tuple2.apply[org.apache.spark.sql.types.DataType, org.apache.spark.sql.catalyst.expressions.Expression](parse(exps.apply(0)), exps.apply(1))
  }: (org.apache.spark.sql.types.DataType, org.apache.spark.sql.catalyst.expressions.Expression) @unchecked) match {
    case (_1: org.apache.spark.sql.types.DataType, _2: org.apache.spark.sql.catalyst.expressions.Expression)(org.apache.spark.sql.types.DataType, org.apache.spark.sql.catalyst.expressions.Expression)((dataType @ _), (origExp @ _)) =&gt; scala.Tuple2.apply[org.apache.spark.sql.types.DataType, org.apache.spark.sql.catalyst.expressions.Expression](dataType, origExp)
  };
  val dataType: org.apache.spark.sql.types.DataType = x$7._1;
  val origExp: org.apache.spark.sql.catalyst.expressions.Expression = x$7._2;
  org.apache.spark.sql.qualityFunctions.FunN.apply(scala.collection.Seq.apply[org.apache.spark.sql.qualityFunctions.RefExpression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(dataType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)), origExp, scala.Some.apply[String](&quot;sumWith&quot;), org.apache.spark.sql.qualityFunctions.FunN.apply$default$4, org.apache.spark.sql.qualityFunctions.FunN.apply$default$5)
}))
        </td>
      </tr><tr>
        <td>
          187
        </td>
        <td>
          2177
        </td>
        <td>
          8578
          -
          8587
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;sumWith&quot;
        </td>
      </tr><tr>
        <td>
          188
        </td>
        <td>
          2179
        </td>
        <td>
          8639
          -
          8639
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$7._2
        </td>
      </tr><tr>
        <td>
          188
        </td>
        <td>
          2178
        </td>
        <td>
          8629
          -
          8629
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$7._1
        </td>
      </tr><tr>
        <td>
          193
        </td>
        <td>
          2182
        </td>
        <td>
          8801
          -
          8824
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply(dataType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)
        </td>
      </tr><tr>
        <td>
          193
        </td>
        <td>
          2185
        </td>
        <td>
          8792
          -
          8792
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$4
        </td>
      </tr><tr>
        <td>
          193
        </td>
        <td>
          2187
        </td>
        <td>
          8792
          -
          8852
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply(scala.collection.Seq.apply[org.apache.spark.sql.qualityFunctions.RefExpression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(dataType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)), origExp, scala.Some.apply[String](&quot;sumWith&quot;), org.apache.spark.sql.qualityFunctions.FunN.apply$default$4, org.apache.spark.sql.qualityFunctions.FunN.apply$default$5)
        </td>
      </tr><tr>
        <td>
          193
        </td>
        <td>
          2181
        </td>
        <td>
          8801
          -
          8801
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
      </tr><tr>
        <td>
          193
        </td>
        <td>
          2184
        </td>
        <td>
          8836
          -
          8851
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[String](&quot;sumWith&quot;)
        </td>
      </tr><tr>
        <td>
          193
        </td>
        <td>
          2183
        </td>
        <td>
          8797
          -
          8825
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.qualityFunctions.RefExpression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(dataType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3))
        </td>
      </tr><tr>
        <td>
          193
        </td>
        <td>
          2186
        </td>
        <td>
          8792
          -
          8792
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$5
        </td>
      </tr><tr>
        <td>
          193
        </td>
        <td>
          2180
        </td>
        <td>
          8801
          -
          8801
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          2190
        </td>
        <td>
          9016
          -
          9016
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$8._2
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          2189
        </td>
        <td>
          9007
          -
          9007
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$8._1
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          2197
        </td>
        <td>
          9225
          -
          9248
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          2200
        </td>
        <td>
          9192
          -
          9192
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$4
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          2191
        </td>
        <td>
          9201
          -
          9201
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          2194
        </td>
        <td>
          9239
          -
          9247
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          2196
        </td>
        <td>
          9225
          -
          9225
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          2199
        </td>
        <td>
          9256
          -
          9275
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[String](&quot;resultsWith&quot;)
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          2202
        </td>
        <td>
          9192
          -
          9276
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply(scala.collection.Seq.apply[org.apache.spark.sql.qualityFunctions.RefExpression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(sumType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3), org.apache.spark.sql.qualityFunctions.RefExpression.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)), exp, scala.Some.apply[String](&quot;resultsWith&quot;), org.apache.spark.sql.qualityFunctions.FunN.apply$default$4, org.apache.spark.sql.qualityFunctions.FunN.apply$default$5)
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          2193
        </td>
        <td>
          9201
          -
          9223
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply(sumType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          2192
        </td>
        <td>
          9201
          -
          9201
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          2201
        </td>
        <td>
          9192
          -
          9192
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$5
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          2195
        </td>
        <td>
          9225
          -
          9225
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          2198
        </td>
        <td>
          9197
          -
          9249
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.qualityFunctions.RefExpression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(sumType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3), org.apache.spark.sql.qualityFunctions.RefExpression.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3))
        </td>
      </tr><tr>
        <td>
          206
        </td>
        <td>
          2203
        </td>
        <td>
          9287
          -
          9315
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;resultsWith&quot;, ff2)
        </td>
      </tr><tr>
        <td>
          208
        </td>
        <td>
          2204
        </td>
        <td>
          9360
          -
          9369
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size
        </td>
      </tr><tr>
        <td>
          211
        </td>
        <td>
          2206
        </td>
        <td>
          9497
          -
          9511
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.parse
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          parse(exps.apply(0))
        </td>
      </tr><tr>
        <td>
          211
        </td>
        <td>
          2209
        </td>
        <td>
          9483
          -
          9512
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply(parse(exps.apply(0)), org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)
        </td>
      </tr><tr>
        <td>
          211
        </td>
        <td>
          2212
        </td>
        <td>
          9463
          -
          9537
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.MapTransform.create
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.MapTransform.create(org.apache.spark.sql.qualityFunctions.RefExpression.apply(parse(exps.apply(0)), org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3), exps.apply(1), exps.apply(2), zero)
        </td>
      </tr><tr>
        <td>
          211
        </td>
        <td>
          2211
        </td>
        <td>
          9523
          -
          9530
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(2)
        </td>
      </tr><tr>
        <td>
          211
        </td>
        <td>
          2205
        </td>
        <td>
          9503
          -
          9510
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          211
        </td>
        <td>
          2208
        </td>
        <td>
          9483
          -
          9483
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
      </tr><tr>
        <td>
          211
        </td>
        <td>
          2210
        </td>
        <td>
          9514
          -
          9521
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          211
        </td>
        <td>
          2207
        </td>
        <td>
          9483
          -
          9483
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
      </tr><tr>
        <td>
          214
        </td>
        <td>
          2215
        </td>
        <td>
          9701
          -
          9728
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.MapType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.MapType.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.types.LongType)
        </td>
      </tr><tr>
        <td>
          214
        </td>
        <td>
          2218
        </td>
        <td>
          9687
          -
          9729
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply(org.apache.spark.sql.types.MapType.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.types.LongType), org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)
        </td>
      </tr><tr>
        <td>
          214
        </td>
        <td>
          2221
        </td>
        <td>
          9667
          -
          9754
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.MapTransform.create
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.MapTransform.create(org.apache.spark.sql.qualityFunctions.RefExpression.apply(org.apache.spark.sql.types.MapType.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.types.LongType), org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3), exps.apply(0), exps.apply(1), zero)
        </td>
      </tr><tr>
        <td>
          214
        </td>
        <td>
          2220
        </td>
        <td>
          9740
          -
          9747
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          214
        </td>
        <td>
          2214
        </td>
        <td>
          9719
          -
          9727
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          214
        </td>
        <td>
          2217
        </td>
        <td>
          9687
          -
          9687
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
      </tr><tr>
        <td>
          214
        </td>
        <td>
          2219
        </td>
        <td>
          9731
          -
          9738
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          214
        </td>
        <td>
          2213
        </td>
        <td>
          9709
          -
          9717
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          214
        </td>
        <td>
          2216
        </td>
        <td>
          9687
          -
          9687
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
      </tr><tr>
        <td>
          216
        </td>
        <td>
          2222
        </td>
        <td>
          9765
          -
          9791
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;mapWith&quot;, mapFX)
        </td>
      </tr><tr>
        <td>
          219
        </td>
        <td>
          2223
        </td>
        <td>
          9882
          -
          9896
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size.==(0)
        </td>
      </tr><tr>
        <td>
          220
        </td>
        <td>
          2224
        </td>
        <td>
          9921
          -
          9937
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;&quot;, &quot;( &quot;, &quot; )&quot;).s(fun, what)
        </td>
      </tr><tr>
        <td>
          220
        </td>
        <td>
          2226
        </td>
        <td>
          9906
          -
          9938
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.functions.expr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.functions.expr(scala.StringContext.apply(&quot;&quot;, &quot;( &quot;, &quot; )&quot;).s(fun, what))
        </td>
      </tr><tr>
        <td>
          220
        </td>
        <td>
          2225
        </td>
        <td>
          9906
          -
          9938
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.functions.expr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.functions.expr(scala.StringContext.apply(&quot;&quot;, &quot;( &quot;, &quot; )&quot;).s(fun, what))
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          2233
        </td>
        <td>
          9973
          -
          10011
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;&quot;, &quot;(\'&quot;, &quot;\', &quot;, &quot; )&quot;).s(fun, strType(exps.apply(0)), what)
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          2227
        </td>
        <td>
          9975
          -
          9976
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          2230
        </td>
        <td>
          10008
          -
          10011
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot; )&quot;
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          2229
        </td>
        <td>
          10000
          -
          10004
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;\', &quot;
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          2232
        </td>
        <td>
          9983
          -
          9999
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.strType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          strType(exps.apply(0))
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          2235
        </td>
        <td>
          9958
          -
          10012
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.functions.expr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.functions.expr(scala.StringContext.apply(&quot;&quot;, &quot;(\'&quot;, &quot;\', &quot;, &quot; )&quot;).s(fun, strType(exps.apply(0)), what))
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          2234
        </td>
        <td>
          9958
          -
          10012
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.functions.expr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.functions.expr(scala.StringContext.apply(&quot;&quot;, &quot;(\'&quot;, &quot;\', &quot;, &quot; )&quot;).s(fun, strType(exps.apply(0)), what))
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          2228
        </td>
        <td>
          9979
          -
          9982
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;(\'&quot;
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          2231
        </td>
        <td>
          9991
          -
          9998
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          223
        </td>
        <td>
          2236
        </td>
        <td>
          9878
          -
          10025
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.Column.expr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          if (exps.size.==(0))
  org.apache.spark.sql.functions.expr(scala.StringContext.apply(&quot;&quot;, &quot;( &quot;, &quot; )&quot;).s(fun, what))
else
  org.apache.spark.sql.functions.expr(scala.StringContext.apply(&quot;&quot;, &quot;(\'&quot;, &quot;\', &quot;, &quot; )&quot;).s(fun, strType(exps.apply(0)), what)).expr
        </td>
      </tr><tr>
        <td>
          225
        </td>
        <td>
          2237
        </td>
        <td>
          10045
          -
          10068
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.aggFWith
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          aggFWith(&quot;resultsWith&quot;)
        </td>
      </tr><tr>
        <td>
          228
        </td>
        <td>
          2239
        </td>
        <td>
          10112
          -
          10150
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function1.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          retWith.apply(&quot;(sum, count) -&gt; sum / count&quot;)
        </td>
      </tr><tr>
        <td>
          228
        </td>
        <td>
          2238
        </td>
        <td>
          10103
          -
          10110
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;meanF&quot;
        </td>
      </tr><tr>
        <td>
          228
        </td>
        <td>
          2240
        </td>
        <td>
          10094
          -
          10151
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;meanF&quot;, retWith.apply(&quot;(sum, count) -&gt; sum / count&quot;))
        </td>
      </tr><tr>
        <td>
          230
        </td>
        <td>
          2241
        </td>
        <td>
          10171
          -
          10190
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.aggFWith
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          aggFWith(&quot;sumWith&quot;)
        </td>
      </tr><tr>
        <td>
          234
        </td>
        <td>
          2242
        </td>
        <td>
          10328
          -
          10331
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;.&quot;
        </td>
      </tr><tr>
        <td>
          234
        </td>
        <td>
          2244
        </td>
        <td>
          10307
          -
          10341
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x.qualifier.mkString(&quot;.&quot;).+(x.name)
        </td>
      </tr><tr>
        <td>
          234
        </td>
        <td>
          2243
        </td>
        <td>
          10335
          -
          10341
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.AttributeReference.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x.name
        </td>
      </tr><tr>
        <td>
          235
        </td>
        <td>
          2245
        </td>
        <td>
          10399
          -
          10420
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;sum -&gt; sum + &quot;, &quot;&quot;).s(name)
        </td>
      </tr><tr>
        <td>
          235
        </td>
        <td>
          2247
        </td>
        <td>
          10391
          -
          10428
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function1.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          sumWith.apply(scala.StringContext.apply(&quot;sum -&gt; sum + &quot;, &quot;&quot;).s(name)).apply(scala.collection.Seq.apply[Nothing]())
        </td>
      </tr><tr>
        <td>
          235
        </td>
        <td>
          2246
        </td>
        <td>
          10422
          -
          10427
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[Nothing]()
        </td>
      </tr><tr>
        <td>
          238
        </td>
        <td>
          2248
        </td>
        <td>
          10526
          -
          10557
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function1.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          sumWith.apply(&quot;sum -&gt; sum + 1&quot;).apply(exps)
        </td>
      </tr><tr>
        <td>
          240
        </td>
        <td>
          2251
        </td>
        <td>
          10656
          -
          10690
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x.qualifier.mkString(&quot;.&quot;).+(x.name)
        </td>
      </tr><tr>
        <td>
          240
        </td>
        <td>
          2250
        </td>
        <td>
          10684
          -
          10690
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.AttributeReference.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x.name
        </td>
      </tr><tr>
        <td>
          240
        </td>
        <td>
          2249
        </td>
        <td>
          10677
          -
          10680
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;.&quot;
        </td>
      </tr><tr>
        <td>
          241
        </td>
        <td>
          2254
        </td>
        <td>
          10730
          -
          10742
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](exps.apply(0))
        </td>
      </tr><tr>
        <td>
          241
        </td>
        <td>
          2253
        </td>
        <td>
          10734
          -
          10741
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          241
        </td>
        <td>
          2252
        </td>
        <td>
          10707
          -
          10728
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;sum -&gt; sum + &quot;, &quot;&quot;).s(name)
        </td>
      </tr><tr>
        <td>
          241
        </td>
        <td>
          2255
        </td>
        <td>
          10699
          -
          10743
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function1.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          sumWith.apply(scala.StringContext.apply(&quot;sum -&gt; sum + &quot;, &quot;&quot;).s(name)).apply(scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.Expression](exps.apply(0)))
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          2257
        </td>
        <td>
          10843
          -
          10843
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          2256
        </td>
        <td>
          10860
          -
          10886
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.INC_REWRITE_GENEXP_ERR_MSG
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRunnerFunctionsImport.this.INC_REWRITE_GENEXP_ERR_MSG
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          2258
        </td>
        <td>
          10843
          -
          10887
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.QualityException.qualityException(RuleRunnerFunctionsImport.this.INC_REWRITE_GENEXP_ERR_MSG, com.sparkutils.quality.QualityException.qualityException$default$2)
        </td>
      </tr><tr>
        <td>
          245
        </td>
        <td>
          2260
        </td>
        <td>
          10950
          -
          10950
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$9._2
        </td>
      </tr><tr>
        <td>
          245
        </td>
        <td>
          2259
        </td>
        <td>
          10938
          -
          10938
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$9._1
        </td>
      </tr><tr>
        <td>
          245
        </td>
        <td>
          2261
        </td>
        <td>
          10987
          -
          10987
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$9._3
        </td>
      </tr><tr>
        <td>
          248
        </td>
        <td>
          2266
        </td>
        <td>
          11199
          -
          11227
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.qualityFunctions.RefExpression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3))
        </td>
      </tr><tr>
        <td>
          248
        </td>
        <td>
          2275
        </td>
        <td>
          11194
          -
          11324
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply(scala.collection.Seq.apply[org.apache.spark.sql.qualityFunctions.RefExpression](org.apache.spark.sql.qualityFunctions.RefExpression.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)), org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply(org.apache.spark.sql.QualitySparkUtils.add(a.left, y, org.apache.spark.sql.types.LongType), scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.UnresolvedNamedLambdaVariable](sum), hidden), scala.Some.apply[String](&quot;inc&quot;), org.apache.spark.sql.qualityFunctions.FunN.apply$default$4, org.apache.spark.sql.qualityFunctions.FunN.apply$default$5)
        </td>
      </tr><tr>
        <td>
          248
        </td>
        <td>
          2263
        </td>
        <td>
          11203
          -
          11203
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2
        </td>
      </tr><tr>
        <td>
          248
        </td>
        <td>
          2265
        </td>
        <td>
          11203
          -
          11226
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply(org.apache.spark.sql.types.LongType, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$2, org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3)
        </td>
      </tr><tr>
        <td>
          248
        </td>
        <td>
          2274
        </td>
        <td>
          11194
          -
          11194
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$5
        </td>
      </tr><tr>
        <td>
          248
        </td>
        <td>
          2262
        </td>
        <td>
          11217
          -
          11225
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          248
        </td>
        <td>
          2264
        </td>
        <td>
          11203
          -
          11203
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.RefExpression.apply$default$3
        </td>
      </tr><tr>
        <td>
          248
        </td>
        <td>
          2273
        </td>
        <td>
          11194
          -
          11194
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.FunN.apply$default$4
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          2269
        </td>
        <td>
          11254
          -
          11279
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.add
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.QualitySparkUtils.add(a.left, y, org.apache.spark.sql.types.LongType)
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          2271
        </td>
        <td>
          11239
          -
          11299
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.LambdaFunction.apply(org.apache.spark.sql.QualitySparkUtils.add(a.left, y, org.apache.spark.sql.types.LongType), scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.UnresolvedNamedLambdaVariable](sum), hidden)
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          2268
        </td>
        <td>
          11270
          -
          11278
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          2270
        </td>
        <td>
          11281
          -
          11289
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.Seq.apply[org.apache.spark.sql.catalyst.expressions.UnresolvedNamedLambdaVariable](sum)
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          2267
        </td>
        <td>
          11259
          -
          11265
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Add.left
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.left
        </td>
      </tr><tr>
        <td>
          250
        </td>
        <td>
          2272
        </td>
        <td>
          11312
          -
          11323
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Some.apply[String](&quot;inc&quot;)
        </td>
      </tr><tr>
        <td>
          251
        </td>
        <td>
          2277
        </td>
        <td>
          11362
          -
          11409
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.Column.expr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.functions.expr(scala.StringContext.apply(&quot;sumWith(sum -&gt; sum + 1)&quot;).s()).expr
        </td>
      </tr><tr>
        <td>
          251
        </td>
        <td>
          2276
        </td>
        <td>
          11377
          -
          11403
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;sumWith(sum -&gt; sum + 1)&quot;).s()
        </td>
      </tr><tr>
        <td>
          253
        </td>
        <td>
          2278
        </td>
        <td>
          11420
          -
          11441
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;inc&quot;, incX)
        </td>
      </tr><tr>
        <td>
          256
        </td>
        <td>
          2281
        </td>
        <td>
          11465
          -
          11518
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;returnSum&quot;, retWith.apply(&quot;(sum, count) -&gt; sum&quot;))
        </td>
      </tr><tr>
        <td>
          256
        </td>
        <td>
          2280
        </td>
        <td>
          11487
          -
          11517
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function1.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          retWith.apply(&quot;(sum, count) -&gt; sum&quot;)
        </td>
      </tr><tr>
        <td>
          256
        </td>
        <td>
          2279
        </td>
        <td>
          11474
          -
          11485
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;returnSum&quot;
        </td>
      </tr><tr>
        <td>
          262
        </td>
        <td>
          2282
        </td>
        <td>
          11648
          -
          11662
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRunnerFunctions.getString(exp)
        </td>
      </tr><tr>
        <td>
          263
        </td>
        <td>
          2283
        </td>
        <td>
          11671
          -
          11696
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.commons.rng.simple.RandomSource.valueOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.commons.rng.simple.RandomSource.valueOf(str)
        </td>
      </tr><tr>
        <td>
          267
        </td>
        <td>
          2284
        </td>
        <td>
          11831
          -
          11831
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$10._1
        </td>
      </tr><tr>
        <td>
          267
        </td>
        <td>
          2286
        </td>
        <td>
          11860
          -
          11860
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$10._3
        </td>
      </tr><tr>
        <td>
          267
        </td>
        <td>
          2285
        </td>
        <td>
          11846
          -
          11846
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$10._2
        </td>
      </tr><tr>
        <td>
          276
        </td>
        <td>
          2287
        </td>
        <td>
          12210
          -
          12251
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.rng.RandomBytes.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.rng.RandomBytes.apply(numBytes, randomSource, seed)
        </td>
      </tr><tr>
        <td>
          278
        </td>
        <td>
          2288
        </td>
        <td>
          12262
          -
          12287
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;rngBytes&quot;, brf)
        </td>
      </tr><tr>
        <td>
          280
        </td>
        <td>
          2289
        </td>
        <td>
          12343
          -
          12357
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRunnerFunctions.getString(exp)
        </td>
      </tr><tr>
        <td>
          281
        </td>
        <td>
          2290
        </td>
        <td>
          12364
          -
          12389
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.commons.rng.simple.RandomSource.valueOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.commons.rng.simple.RandomSource.valueOf(str)
        </td>
      </tr><tr>
        <td>
          288
        </td>
        <td>
          2292
        </td>
        <td>
          12590
          -
          12590
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$11._2
        </td>
      </tr><tr>
        <td>
          288
        </td>
        <td>
          2291
        </td>
        <td>
          12576
          -
          12576
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$11._1
        </td>
      </tr><tr>
        <td>
          296
        </td>
        <td>
          2293
        </td>
        <td>
          12845
          -
          12883
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.rng.RandomLongs.create
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.rng.RandomLongs.create(randomSource, seed)
        </td>
      </tr><tr>
        <td>
          298
        </td>
        <td>
          2294
        </td>
        <td>
          12894
          -
          12914
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;rng&quot;, lrf)
        </td>
      </tr><tr>
        <td>
          300
        </td>
        <td>
          2296
        </td>
        <td>
          12985
          -
          12994
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          300
        </td>
        <td>
          2298
        </td>
        <td>
          12920
          -
          12996
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;rngUUID&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; RngUUIDExpression.apply(exps.head)))
        </td>
      </tr><tr>
        <td>
          300
        </td>
        <td>
          2295
        </td>
        <td>
          12929
          -
          12938
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;rngUUID&quot;
        </td>
      </tr><tr>
        <td>
          300
        </td>
        <td>
          2297
        </td>
        <td>
          12967
          -
          12995
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RngUUIDExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RngUUIDExpression.apply(exps.head)
        </td>
      </tr><tr>
        <td>
          302
        </td>
        <td>
          2302
        </td>
        <td>
          13050
          -
          13086
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.longPair.LongPairExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.longPair.LongPairExpression.apply(exps.apply(0), exps.apply(1))
        </td>
      </tr><tr>
        <td>
          302
        </td>
        <td>
          2299
        </td>
        <td>
          13011
          -
          13021
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;longPair&quot;
        </td>
      </tr><tr>
        <td>
          302
        </td>
        <td>
          2301
        </td>
        <td>
          13078
          -
          13085
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          302
        </td>
        <td>
          2303
        </td>
        <td>
          13002
          -
          13087
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;longPair&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.impl.longPair.LongPairExpression.apply(exps.apply(0), exps.apply(1))))
        </td>
      </tr><tr>
        <td>
          302
        </td>
        <td>
          2300
        </td>
        <td>
          13069
          -
          13076
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          303
        </td>
        <td>
          2305
        </td>
        <td>
          13170
          -
          13179
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          303
        </td>
        <td>
          2307
        </td>
        <td>
          13092
          -
          13181
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;longPairFromUUID&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; UUIDToLongsExpression.apply(exps.head)))
        </td>
      </tr><tr>
        <td>
          303
        </td>
        <td>
          2304
        </td>
        <td>
          13101
          -
          13119
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;longPairFromUUID&quot;
        </td>
      </tr><tr>
        <td>
          303
        </td>
        <td>
          2306
        </td>
        <td>
          13148
          -
          13180
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.UUIDToLongsExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          UUIDToLongsExpression.apply(exps.head)
        </td>
      </tr><tr>
        <td>
          305
        </td>
        <td>
          2311
        </td>
        <td>
          13273
          -
          13280
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(2)
        </td>
      </tr><tr>
        <td>
          305
        </td>
        <td>
          2314
        </td>
        <td>
          13237
          -
          13281
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.bloom.ParquetAggregator.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.bloom.ParquetAggregator.apply(exps.apply(0), exps.apply(1), exps.apply(2), com.sparkutils.quality.impl.bloom.ParquetAggregator.apply$default$4, com.sparkutils.quality.impl.bloom.ParquetAggregator.apply$default$5)
        </td>
      </tr><tr>
        <td>
          305
        </td>
        <td>
          2308
        </td>
        <td>
          13196
          -
          13208
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;smallBloom&quot;
        </td>
      </tr><tr>
        <td>
          305
        </td>
        <td>
          2310
        </td>
        <td>
          13264
          -
          13271
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          305
        </td>
        <td>
          2313
        </td>
        <td>
          13237
          -
          13237
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.bloom.ParquetAggregator.apply$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.bloom.ParquetAggregator.apply$default$5
        </td>
      </tr><tr>
        <td>
          305
        </td>
        <td>
          2312
        </td>
        <td>
          13237
          -
          13237
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.bloom.ParquetAggregator.apply$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.bloom.ParquetAggregator.apply$default$4
        </td>
      </tr><tr>
        <td>
          305
        </td>
        <td>
          2315
        </td>
        <td>
          13187
          -
          13282
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;smallBloom&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.impl.bloom.ParquetAggregator.apply(exps.apply(0), exps.apply(1), exps.apply(2), com.sparkutils.quality.impl.bloom.ParquetAggregator.apply$default$4, com.sparkutils.quality.impl.bloom.ParquetAggregator.apply$default$5)))
        </td>
      </tr><tr>
        <td>
          305
        </td>
        <td>
          2309
        </td>
        <td>
          13255
          -
          13262
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          307
        </td>
        <td>
          2317
        </td>
        <td>
          13367
          -
          13374
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          307
        </td>
        <td>
          2320
        </td>
        <td>
          13394
          -
          13401
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(3)
        </td>
      </tr><tr>
        <td>
          307
        </td>
        <td>
          2323
        </td>
        <td>
          13336
          -
          13336
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$7
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$7
        </td>
      </tr><tr>
        <td>
          307
        </td>
        <td>
          2316
        </td>
        <td>
          13297
          -
          13307
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;bigBloom&quot;
        </td>
      </tr><tr>
        <td>
          307
        </td>
        <td>
          2325
        </td>
        <td>
          13288
          -
          13403
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;bigBloom&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply(exps.apply(0), exps.apply(1), exps.apply(2), exps.apply(3), com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$5, com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$6, com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$7)))
        </td>
      </tr><tr>
        <td>
          307
        </td>
        <td>
          2319
        </td>
        <td>
          13385
          -
          13392
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(2)
        </td>
      </tr><tr>
        <td>
          307
        </td>
        <td>
          2322
        </td>
        <td>
          13336
          -
          13336
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$6
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$6
        </td>
      </tr><tr>
        <td>
          307
        </td>
        <td>
          2321
        </td>
        <td>
          13336
          -
          13336
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$5
        </td>
      </tr><tr>
        <td>
          307
        </td>
        <td>
          2324
        </td>
        <td>
          13336
          -
          13402
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply(exps.apply(0), exps.apply(1), exps.apply(2), exps.apply(3), com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$5, com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$6, com.sparkutils.quality.impl.bloom.BucketedArrayParquetAggregator.apply$default$7)
        </td>
      </tr><tr>
        <td>
          307
        </td>
        <td>
          2318
        </td>
        <td>
          13376
          -
          13383
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          310
        </td>
        <td>
          2326
        </td>
        <td>
          13480
          -
          13480
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$12._1
        </td>
      </tr><tr>
        <td>
          310
        </td>
        <td>
          2327
        </td>
        <td>
          13504
          -
          13504
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$12._2
        </td>
      </tr><tr>
        <td>
          312
        </td>
        <td>
          2329
        </td>
        <td>
          13554
          -
          13588
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.apply(scala.StringContext.apply(&quot;&quot;, &quot;_lower&quot;).s(a))
        </td>
      </tr><tr>
        <td>
          312
        </td>
        <td>
          2328
        </td>
        <td>
          13574
          -
          13587
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot;_lower&quot;).s(a)
        </td>
      </tr><tr>
        <td>
          314
        </td>
        <td>
          2331
        </td>
        <td>
          13617
          -
          13652
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.apply(scala.StringContext.apply(&quot;&quot;, &quot;_higher&quot;).s(a))
        </td>
      </tr><tr>
        <td>
          314
        </td>
        <td>
          2330
        </td>
        <td>
          13637
          -
          13651
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot;_higher&quot;).s(a)
        </td>
      </tr><tr>
        <td>
          316
        </td>
        <td>
          2335
        </td>
        <td>
          13701
          -
          13710
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.higher
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          higher(a)
        </td>
      </tr><tr>
        <td>
          316
        </td>
        <td>
          2338
        </td>
        <td>
          13660
          -
          13723
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.And.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.And.apply(org.apache.spark.sql.catalyst.expressions.EqualTo.apply(lower(a), lower(b)), org.apache.spark.sql.catalyst.expressions.EqualTo.apply(higher(a), higher(b)))
        </td>
      </tr><tr>
        <td>
          316
        </td>
        <td>
          2332
        </td>
        <td>
          13672
          -
          13680
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.lower
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          lower(a)
        </td>
      </tr><tr>
        <td>
          316
        </td>
        <td>
          2334
        </td>
        <td>
          13664
          -
          13691
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.EqualTo.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.EqualTo.apply(lower(a), lower(b))
        </td>
      </tr><tr>
        <td>
          316
        </td>
        <td>
          2337
        </td>
        <td>
          13693
          -
          13722
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.EqualTo.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.EqualTo.apply(higher(a), higher(b))
        </td>
      </tr><tr>
        <td>
          316
        </td>
        <td>
          2333
        </td>
        <td>
          13682
          -
          13690
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.lower
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          lower(b)
        </td>
      </tr><tr>
        <td>
          316
        </td>
        <td>
          2336
        </td>
        <td>
          13712
          -
          13721
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.higher
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          higher(b)
        </td>
      </tr><tr>
        <td>
          318
        </td>
        <td>
          2339
        </td>
        <td>
          13734
          -
          13774
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;longPairEqual&quot;, longPairEqual)
        </td>
      </tr><tr>
        <td>
          321
        </td>
        <td>
          2341
        </td>
        <td>
          13869
          -
          13869
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$13._2
        </td>
      </tr><tr>
        <td>
          321
        </td>
        <td>
          2340
        </td>
        <td>
          13845
          -
          13845
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$13._1
        </td>
      </tr><tr>
        <td>
          323
        </td>
        <td>
          2343
        </td>
        <td>
          13933
          -
          13968
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.apply(scala.StringContext.apply(&quot;&quot;, &quot;_&quot;, &quot;&quot;).s(a, field))
        </td>
      </tr><tr>
        <td>
          323
        </td>
        <td>
          2342
        </td>
        <td>
          13953
          -
          13967
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;&quot;, &quot;_&quot;, &quot;&quot;).s(a, field)
        </td>
      </tr><tr>
        <td>
          325
        </td>
        <td>
          2344
        </td>
        <td>
          13992
          -
          14006
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.attr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          attr(a, &quot;base&quot;)
        </td>
      </tr><tr>
        <td>
          325
        </td>
        <td>
          2350
        </td>
        <td>
          13980
          -
          14071
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.And.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.And.apply(org.apache.spark.sql.catalyst.expressions.EqualTo.apply(attr(a, &quot;base&quot;), attr(b, &quot;base&quot;)), org.apache.spark.sql.catalyst.expressions.EqualTo.apply(attr(a, &quot;i0&quot;), attr(b, &quot;i0&quot;)))
        </td>
      </tr><tr>
        <td>
          325
        </td>
        <td>
          2346
        </td>
        <td>
          13984
          -
          14024
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.EqualTo.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.EqualTo.apply(attr(a, &quot;base&quot;), attr(b, &quot;base&quot;))
        </td>
      </tr><tr>
        <td>
          325
        </td>
        <td>
          2354
        </td>
        <td>
          13976
          -
          14118
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.And.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.And.apply(org.apache.spark.sql.catalyst.expressions.And.apply(org.apache.spark.sql.catalyst.expressions.EqualTo.apply(attr(a, &quot;base&quot;), attr(b, &quot;base&quot;)), org.apache.spark.sql.catalyst.expressions.EqualTo.apply(attr(a, &quot;i0&quot;), attr(b, &quot;i0&quot;))), org.apache.spark.sql.catalyst.expressions.EqualTo.apply(attr(a, &quot;i1&quot;), attr(b, &quot;i1&quot;)))
        </td>
      </tr><tr>
        <td>
          325
        </td>
        <td>
          2345
        </td>
        <td>
          14008
          -
          14023
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.attr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          attr(b, &quot;base&quot;)
        </td>
      </tr><tr>
        <td>
          326
        </td>
        <td>
          2347
        </td>
        <td>
          14042
          -
          14054
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.attr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          attr(a, &quot;i0&quot;)
        </td>
      </tr><tr>
        <td>
          326
        </td>
        <td>
          2349
        </td>
        <td>
          14034
          -
          14070
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.EqualTo.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.EqualTo.apply(attr(a, &quot;i0&quot;), attr(b, &quot;i0&quot;))
        </td>
      </tr><tr>
        <td>
          326
        </td>
        <td>
          2348
        </td>
        <td>
          14056
          -
          14069
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.attr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          attr(b, &quot;i0&quot;)
        </td>
      </tr><tr>
        <td>
          327
        </td>
        <td>
          2353
        </td>
        <td>
          14081
          -
          14117
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.EqualTo.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.EqualTo.apply(attr(a, &quot;i1&quot;), attr(b, &quot;i1&quot;))
        </td>
      </tr><tr>
        <td>
          327
        </td>
        <td>
          2352
        </td>
        <td>
          14103
          -
          14116
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.attr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          attr(b, &quot;i1&quot;)
        </td>
      </tr><tr>
        <td>
          327
        </td>
        <td>
          2351
        </td>
        <td>
          14089
          -
          14101
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.attr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          attr(a, &quot;i1&quot;)
        </td>
      </tr><tr>
        <td>
          329
        </td>
        <td>
          2355
        </td>
        <td>
          14129
          -
          14157
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;idEqual&quot;, idEqual)
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          2356
        </td>
        <td>
          14172
          -
          14181
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;as_uuid&quot;
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          2359
        </td>
        <td>
          14191
          -
          14215
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.longPair.AsUUID.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.longPair.AsUUID.apply(exps.apply(0), exps.apply(1))
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          2358
        </td>
        <td>
          14207
          -
          14214
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          2357
        </td>
        <td>
          14198
          -
          14205
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          2360
        </td>
        <td>
          14163
          -
          14216
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;as_uuid&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.impl.longPair.AsUUID.apply(exps.apply(0), exps.apply(1))))
        </td>
      </tr><tr>
        <td>
          333
        </td>
        <td>
          2362
        </td>
        <td>
          14312
          -
          14319
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(0)
        </td>
      </tr><tr>
        <td>
          333
        </td>
        <td>
          2361
        </td>
        <td>
          14231
          -
          14255
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;ruleSuiteResultDetails&quot;
        </td>
      </tr><tr>
        <td>
          333
        </td>
        <td>
          2364
        </td>
        <td>
          14222
          -
          14321
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;ruleSuiteResultDetails&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.impl.RuleSuiteResultDetails.apply(exps.apply(0))))
        </td>
      </tr><tr>
        <td>
          333
        </td>
        <td>
          2363
        </td>
        <td>
          14284
          -
          14320
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleSuiteResultDetails.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.RuleSuiteResultDetails.apply(exps.apply(0))
        </td>
      </tr><tr>
        <td>
          335
        </td>
        <td>
          2365
        </td>
        <td>
          14336
          -
          14357
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;digestToLongsStruct&quot;
        </td>
      </tr><tr>
        <td>
          335
        </td>
        <td>
          2367
        </td>
        <td>
          14327
          -
          14379
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;digestToLongsStruct&quot;, digestToLongs(true))
        </td>
      </tr><tr>
        <td>
          335
        </td>
        <td>
          2366
        </td>
        <td>
          14359
          -
          14378
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.digestToLongs
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          digestToLongs(true)
        </td>
      </tr><tr>
        <td>
          338
        </td>
        <td>
          2368
        </td>
        <td>
          14493
          -
          14502
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          338
        </td>
        <td>
          2369
        </td>
        <td>
          14483
          -
          14503
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRunnerFunctions.getString(exps.head)
        </td>
      </tr><tr>
        <td>
          339
        </td>
        <td>
          2371
        </td>
        <td>
          14567
          -
          14599
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.MessageDigestFactory.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.hash.MessageDigestFactory.apply(digestImpl)
        </td>
      </tr><tr>
        <td>
          339
        </td>
        <td>
          2370
        </td>
        <td>
          14534
          -
          14543
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableLike.tail
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.tail
        </td>
      </tr><tr>
        <td>
          339
        </td>
        <td>
          2372
        </td>
        <td>
          14510
          -
          14600
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashFunctionsExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.hash.HashFunctionsExpression.apply(exps.tail, digestImpl, asStruct, com.sparkutils.quality.impl.hash.MessageDigestFactory.apply(digestImpl))
        </td>
      </tr><tr>
        <td>
          341
        </td>
        <td>
          2374
        </td>
        <td>
          14637
          -
          14657
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.digestToLongs
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          digestToLongs(false)
        </td>
      </tr><tr>
        <td>
          341
        </td>
        <td>
          2373
        </td>
        <td>
          14620
          -
          14635
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;digestToLongs&quot;
        </td>
      </tr><tr>
        <td>
          341
        </td>
        <td>
          2375
        </td>
        <td>
          14611
          -
          14658
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;digestToLongs&quot;, digestToLongs(false))
        </td>
      </tr><tr>
        <td>
          344
        </td>
        <td>
          2376
        </td>
        <td>
          14752
          -
          14761
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size
        </td>
      </tr><tr>
        <td>
          345
        </td>
        <td>
          2377
        </td>
        <td>
          14790
          -
          14795
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.&gt;(2)
        </td>
      </tr><tr>
        <td>
          346
        </td>
        <td>
          2379
        </td>
        <td>
          14828
          -
          14846
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRunnerFunctions.getString(exps.apply(1))
        </td>
      </tr><tr>
        <td>
          346
        </td>
        <td>
          2378
        </td>
        <td>
          14838
          -
          14845
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          347
        </td>
        <td>
          2380
        </td>
        <td>
          14888
          -
          14906
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.id.model.FieldBasedID
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.model.FieldBasedID
        </td>
      </tr><tr>
        <td>
          347
        </td>
        <td>
          2387
        </td>
        <td>
          14859
          -
          15021
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.id.GenericLongBasedIDExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.GenericLongBasedIDExpression.apply(com.sparkutils.quality.impl.id.model.FieldBasedID, com.sparkutils.quality.impl.hash.HashFunctionsExpression.apply(exps.drop(2), digestImpl, true, factory.apply(digestImpl)), RuleRunnerFunctions.getString(exps.head))
        </td>
      </tr><tr>
        <td>
          348
        </td>
        <td>
          2383
        </td>
        <td>
          14978
          -
          14997
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function1.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          factory.apply(digestImpl)
        </td>
      </tr><tr>
        <td>
          348
        </td>
        <td>
          2385
        </td>
        <td>
          15010
          -
          15019
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          348
        </td>
        <td>
          2382
        </td>
        <td>
          14972
          -
          14976
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          348
        </td>
        <td>
          2381
        </td>
        <td>
          14946
          -
          14958
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.IterableLike.drop
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.drop(2)
        </td>
      </tr><tr>
        <td>
          348
        </td>
        <td>
          2384
        </td>
        <td>
          14922
          -
          14998
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashFunctionsExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.hash.HashFunctionsExpression.apply(exps.drop(2), digestImpl, true, factory.apply(digestImpl))
        </td>
      </tr><tr>
        <td>
          348
        </td>
        <td>
          2386
        </td>
        <td>
          15000
          -
          15020
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRunnerFunctions.getString(exps.head)
        </td>
      </tr><tr>
        <td>
          350
        </td>
        <td>
          2388
        </td>
        <td>
          15043
          -
          15057
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.literalsNeeded
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          RuleRunnerFunctions.literalsNeeded
        </td>
      </tr><tr>
        <td>
          353
        </td>
        <td>
          2389
        </td>
        <td>
          15082
          -
          15096
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;fieldBasedID&quot;
        </td>
      </tr><tr>
        <td>
          353
        </td>
        <td>
          2392
        </td>
        <td>
          15073
          -
          15133
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;fieldBasedID&quot;, fieldBasedID(com.sparkutils.quality.impl.hash.MessageDigestFactory))
        </td>
      </tr><tr>
        <td>
          353
        </td>
        <td>
          2391
        </td>
        <td>
          15098
          -
          15132
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.fieldBasedID
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          fieldBasedID(com.sparkutils.quality.impl.hash.MessageDigestFactory)
        </td>
      </tr><tr>
        <td>
          353
        </td>
        <td>
          2390
        </td>
        <td>
          15111
          -
          15131
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.hash.MessageDigestFactory
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.hash.MessageDigestFactory
        </td>
      </tr><tr>
        <td>
          354
        </td>
        <td>
          2394
        </td>
        <td>
          15183
          -
          15213
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.hash.ZALongTupleHashFunctionFactory
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.hash.ZALongTupleHashFunctionFactory
        </td>
      </tr><tr>
        <td>
          354
        </td>
        <td>
          2393
        </td>
        <td>
          15147
          -
          15168
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;zaLongsFieldBasedID&quot;
        </td>
      </tr><tr>
        <td>
          354
        </td>
        <td>
          2396
        </td>
        <td>
          15138
          -
          15215
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;zaLongsFieldBasedID&quot;, fieldBasedID(com.sparkutils.quality.impl.hash.ZALongTupleHashFunctionFactory))
        </td>
      </tr><tr>
        <td>
          354
        </td>
        <td>
          2395
        </td>
        <td>
          15170
          -
          15214
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.fieldBasedID
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          fieldBasedID(com.sparkutils.quality.impl.hash.ZALongTupleHashFunctionFactory)
        </td>
      </tr><tr>
        <td>
          355
        </td>
        <td>
          2398
        </td>
        <td>
          15260
          -
          15285
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.hash.ZALongHashFunctionFactory
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.hash.ZALongHashFunctionFactory
        </td>
      </tr><tr>
        <td>
          355
        </td>
        <td>
          2400
        </td>
        <td>
          15220
          -
          15287
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;zaFieldBasedID&quot;, fieldBasedID(com.sparkutils.quality.impl.hash.ZALongHashFunctionFactory))
        </td>
      </tr><tr>
        <td>
          355
        </td>
        <td>
          2397
        </td>
        <td>
          15229
          -
          15245
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;zaFieldBasedID&quot;
        </td>
      </tr><tr>
        <td>
          355
        </td>
        <td>
          2399
        </td>
        <td>
          15247
          -
          15286
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.fieldBasedID
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          fieldBasedID(com.sparkutils.quality.impl.hash.ZALongHashFunctionFactory)
        </td>
      </tr><tr>
        <td>
          356
        </td>
        <td>
          2401
        </td>
        <td>
          15301
          -
          15319
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;hashFieldBasedID&quot;
        </td>
      </tr><tr>
        <td>
          356
        </td>
        <td>
          2403
        </td>
        <td>
          15321
          -
          15357
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.fieldBasedID
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          fieldBasedID(((x$14: String) =&gt; com.sparkutils.quality.impl.hash.HashFunctionFactory.apply(x$14)))
        </td>
      </tr><tr>
        <td>
          356
        </td>
        <td>
          2402
        </td>
        <td>
          15334
          -
          15356
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashFunctionFactory.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.hash.HashFunctionFactory.apply(x$14)
        </td>
      </tr><tr>
        <td>
          356
        </td>
        <td>
          2404
        </td>
        <td>
          15292
          -
          15358
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;hashFieldBasedID&quot;, fieldBasedID(((x$14: String) =&gt; com.sparkutils.quality.impl.hash.HashFunctionFactory.apply(x$14))))
        </td>
      </tr><tr>
        <td>
          359
        </td>
        <td>
          2405
        </td>
        <td>
          15414
          -
          15423
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size
        </td>
      </tr><tr>
        <td>
          361
        </td>
        <td>
          2410
        </td>
        <td>
          15460
          -
          15549
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.id.GenericLongBasedIDExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.GenericLongBasedIDExpression.apply(com.sparkutils.quality.impl.id.model.ProvidedID, exps.apply(1), RuleRunnerFunctions.getString(exps.head))
        </td>
      </tr><tr>
        <td>
          361
        </td>
        <td>
          2406
        </td>
        <td>
          15489
          -
          15505
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.id.model.ProvidedID
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.model.ProvidedID
        </td>
      </tr><tr>
        <td>
          362
        </td>
        <td>
          2407
        </td>
        <td>
          15519
          -
          15526
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          362
        </td>
        <td>
          2409
        </td>
        <td>
          15528
          -
          15548
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRunnerFunctions.getString(exps.head)
        </td>
      </tr><tr>
        <td>
          362
        </td>
        <td>
          2408
        </td>
        <td>
          15538
          -
          15547
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          364
        </td>
        <td>
          2411
        </td>
        <td>
          15569
          -
          15583
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.literalsNeeded
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          RuleRunnerFunctions.literalsNeeded
        </td>
      </tr><tr>
        <td>
          367
        </td>
        <td>
          2412
        </td>
        <td>
          15597
          -
          15631
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;providedID&quot;, providedID)
        </td>
      </tr><tr>
        <td>
          370
        </td>
        <td>
          2413
        </td>
        <td>
          15695
          -
          15704
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size
        </td>
      </tr><tr>
        <td>
          372
        </td>
        <td>
          2416
        </td>
        <td>
          15769
          -
          15789
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRunnerFunctions.getString(exps.head)
        </td>
      </tr><tr>
        <td>
          372
        </td>
        <td>
          2415
        </td>
        <td>
          15779
          -
          15788
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          372
        </td>
        <td>
          2414
        </td>
        <td>
          15760
          -
          15767
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.apply(1)
        </td>
      </tr><tr>
        <td>
          372
        </td>
        <td>
          2417
        </td>
        <td>
          15741
          -
          15790
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.longPair.PrefixedToLongPair.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.longPair.PrefixedToLongPair.apply(exps.apply(1), RuleRunnerFunctions.getString(exps.head))
        </td>
      </tr><tr>
        <td>
          374
        </td>
        <td>
          2418
        </td>
        <td>
          15810
          -
          15824
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.literalsNeeded
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          RuleRunnerFunctions.literalsNeeded
        </td>
      </tr><tr>
        <td>
          376
        </td>
        <td>
          2419
        </td>
        <td>
          15837
          -
          15887
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;prefixedToLongPair&quot;, prefixedToLongPair)
        </td>
      </tr><tr>
        <td>
          379
        </td>
        <td>
          2421
        </td>
        <td>
          15959
          -
          15959
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$15._2
        </td>
      </tr><tr>
        <td>
          379
        </td>
        <td>
          2420
        </td>
        <td>
          15945
          -
          15945
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$15._1
        </td>
      </tr><tr>
        <td>
          379
        </td>
        <td>
          2422
        </td>
        <td>
          15971
          -
          15971
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$15._3
        </td>
      </tr><tr>
        <td>
          387
        </td>
        <td>
          2425
        </td>
        <td>
          16293
          -
          16392
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.id.GenericLongBasedIDExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.GenericLongBasedIDExpression.apply(com.sparkutils.quality.impl.id.model.RandomID, com.sparkutils.quality.impl.rng.RandLongsWithJump.apply(seed, randomSource), prefix)
        </td>
      </tr><tr>
        <td>
          387
        </td>
        <td>
          2423
        </td>
        <td>
          16322
          -
          16336
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.id.model.RandomID
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.model.RandomID
        </td>
      </tr><tr>
        <td>
          388
        </td>
        <td>
          2424
        </td>
        <td>
          16346
          -
          16383
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.rng.RandLongsWithJump.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.rng.RandLongsWithJump.apply(seed, randomSource)
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          2426
        </td>
        <td>
          16426
          -
          16450
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;rngID&quot;, rngID)
        </td>
      </tr><tr>
        <td>
          394
        </td>
        <td>
          2427
        </td>
        <td>
          16529
          -
          16538
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size
        </td>
      </tr><tr>
        <td>
          395
        </td>
        <td>
          2428
        </td>
        <td>
          16577
          -
          16586
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          395
        </td>
        <td>
          2429
        </td>
        <td>
          16567
          -
          16587
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRunnerFunctions.getString(exps.head)
        </td>
      </tr><tr>
        <td>
          396
        </td>
        <td>
          2430
        </td>
        <td>
          16608
          -
          16622
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.literalsNeeded
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          RuleRunnerFunctions.literalsNeeded
        </td>
      </tr><tr>
        <td>
          399
        </td>
        <td>
          2436
        </td>
        <td>
          16640
          -
          16725
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.id.GuaranteedUniqueIdIDExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.GuaranteedUniqueIdIDExpression.apply(com.sparkutils.quality.impl.id.GuaranteedUniqueID.apply(com.sparkutils.quality.impl.id.GuaranteedUniqueID.apply$default$1, com.sparkutils.quality.impl.id.GuaranteedUniqueID.apply$default$2, com.sparkutils.quality.impl.id.GuaranteedUniqueID.apply$default$3, com.sparkutils.quality.impl.id.GuaranteedUniqueID.apply$default$4), prefix)
        </td>
      </tr><tr>
        <td>
          400
        </td>
        <td>
          2434
        </td>
        <td>
          16680
          -
          16680
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.id.GuaranteedUniqueID.apply$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.GuaranteedUniqueID.apply$default$4
        </td>
      </tr><tr>
        <td>
          400
        </td>
        <td>
          2433
        </td>
        <td>
          16680
          -
          16680
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.id.GuaranteedUniqueID.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.GuaranteedUniqueID.apply$default$3
        </td>
      </tr><tr>
        <td>
          400
        </td>
        <td>
          2432
        </td>
        <td>
          16680
          -
          16680
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.id.GuaranteedUniqueID.apply$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.GuaranteedUniqueID.apply$default$2
        </td>
      </tr><tr>
        <td>
          400
        </td>
        <td>
          2435
        </td>
        <td>
          16680
          -
          16700
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.id.GuaranteedUniqueID.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.GuaranteedUniqueID.apply(com.sparkutils.quality.impl.id.GuaranteedUniqueID.apply$default$1, com.sparkutils.quality.impl.id.GuaranteedUniqueID.apply$default$2, com.sparkutils.quality.impl.id.GuaranteedUniqueID.apply$default$3, com.sparkutils.quality.impl.id.GuaranteedUniqueID.apply$default$4)
        </td>
      </tr><tr>
        <td>
          400
        </td>
        <td>
          2431
        </td>
        <td>
          16680
          -
          16680
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.id.GuaranteedUniqueID.apply$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.GuaranteedUniqueID.apply$default$1
        </td>
      </tr><tr>
        <td>
          404
        </td>
        <td>
          2437
        </td>
        <td>
          16736
          -
          16766
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;uniqueID&quot;, uniqueID)
        </td>
      </tr><tr>
        <td>
          406
        </td>
        <td>
          2439
        </td>
        <td>
          16815
          -
          16824
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          406
        </td>
        <td>
          2441
        </td>
        <td>
          16772
          -
          16826
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;id_size&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.impl.id.SizeOfIDString.apply(exps.head)))
        </td>
      </tr><tr>
        <td>
          406
        </td>
        <td>
          2438
        </td>
        <td>
          16781
          -
          16790
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;id_size&quot;
        </td>
      </tr><tr>
        <td>
          406
        </td>
        <td>
          2440
        </td>
        <td>
          16800
          -
          16825
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.id.SizeOfIDString.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.SizeOfIDString.apply(exps.head)
        </td>
      </tr><tr>
        <td>
          407
        </td>
        <td>
          2445
        </td>
        <td>
          16831
          -
          16934
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;id_base64&quot;, ((x0$4: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; x0$4 match {
  case scala.collection.Seq.unapplySeq[org.apache.spark.sql.catalyst.expressions.Expression](&lt;unapply-selector&gt;) &lt;unapply&gt; ((e @ _)) =&gt; com.sparkutils.quality.impl.id.AsBase64Struct.apply(e)
  case (s @ _) =&gt; com.sparkutils.quality.impl.id.AsBase64Fields.apply(s)
}))
        </td>
      </tr><tr>
        <td>
          407
        </td>
        <td>
          2442
        </td>
        <td>
          16840
          -
          16851
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;id_base64&quot;
        </td>
      </tr><tr>
        <td>
          408
        </td>
        <td>
          2443
        </td>
        <td>
          16876
          -
          16893
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.id.AsBase64Struct.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.AsBase64Struct.apply(e)
        </td>
      </tr><tr>
        <td>
          409
        </td>
        <td>
          2444
        </td>
        <td>
          16910
          -
          16927
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.id.AsBase64Fields.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.AsBase64Fields.apply(s)
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          2450
        </td>
        <td>
          16939
          -
          17091
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;id_from_base64&quot;, ((x0$5: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; x0$5 match {
  case scala.collection.Seq.unapplySeq[org.apache.spark.sql.catalyst.expressions.Expression](&lt;unapply-selector&gt;) &lt;unapply&gt; ((e @ _)) =&gt; com.sparkutils.quality.impl.id.IDFromBase64.apply(e, 2)
  case scala.collection.Seq.unapplySeq[org.apache.spark.sql.catalyst.expressions.Expression](&lt;unapply-selector&gt;) &lt;unapply&gt; ((e @ _), (s @ _)) =&gt; com.sparkutils.quality.impl.id.IDFromBase64.apply(e, RuleRunnerFunctions.getInteger(s))
}))
        </td>
      </tr><tr>
        <td>
          411
        </td>
        <td>
          2446
        </td>
        <td>
          16948
          -
          16964
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;id_from_base64&quot;
        </td>
      </tr><tr>
        <td>
          412
        </td>
        <td>
          2447
        </td>
        <td>
          16989
          -
          17007
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.id.IDFromBase64.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.IDFromBase64.apply(e, 2)
        </td>
      </tr><tr>
        <td>
          413
        </td>
        <td>
          2448
        </td>
        <td>
          17070
          -
          17083
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.getInteger
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRunnerFunctions.getInteger(s)
        </td>
      </tr><tr>
        <td>
          413
        </td>
        <td>
          2449
        </td>
        <td>
          17054
          -
          17084
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.id.IDFromBase64.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.IDFromBase64.apply(e, RuleRunnerFunctions.getInteger(s))
        </td>
      </tr><tr>
        <td>
          418
        </td>
        <td>
          2451
        </td>
        <td>
          17176
          -
          17185
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size
        </td>
      </tr><tr>
        <td>
          419
        </td>
        <td>
          2452
        </td>
        <td>
          17214
          -
          17219
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.&lt;(2)
        </td>
      </tr><tr>
        <td>
          419
        </td>
        <td>
          2453
        </td>
        <td>
          17223
          -
          17237
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.literalsNeeded
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          RuleRunnerFunctions.literalsNeeded
        </td>
      </tr><tr>
        <td>
          420
        </td>
        <td>
          2454
        </td>
        <td>
          17268
          -
          17277
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          420
        </td>
        <td>
          2455
        </td>
        <td>
          17258
          -
          17278
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRunnerFunctions.getString(exps.head)
        </td>
      </tr><tr>
        <td>
          422
        </td>
        <td>
          2462
        </td>
        <td>
          17295
          -
          17444
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.id.GenericLongBasedIDExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.GenericLongBasedIDExpression.apply(com.sparkutils.quality.impl.id.model.FieldBasedID, com.sparkutils.quality.impl.hash.HashFunctionsExpression.apply(exps.tail, &quot;IGNORED&quot;, true, com.sparkutils.quality.impl.hash.HashFunctionFactory.apply(&quot;IGNORED&quot;)), prefix)
        </td>
      </tr><tr>
        <td>
          422
        </td>
        <td>
          2456
        </td>
        <td>
          17324
          -
          17342
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.id.model.FieldBasedID
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.id.model.FieldBasedID
        </td>
      </tr><tr>
        <td>
          423
        </td>
        <td>
          2461
        </td>
        <td>
          17352
          -
          17435
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashFunctionsExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.hash.HashFunctionsExpression.apply(exps.tail, &quot;IGNORED&quot;, true, com.sparkutils.quality.impl.hash.HashFunctionFactory.apply(&quot;IGNORED&quot;))
        </td>
      </tr><tr>
        <td>
          423
        </td>
        <td>
          2460
        </td>
        <td>
          17404
          -
          17434
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashFunctionFactory.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.hash.HashFunctionFactory.apply(&quot;IGNORED&quot;)
        </td>
      </tr><tr>
        <td>
          423
        </td>
        <td>
          2457
        </td>
        <td>
          17376
          -
          17385
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableLike.tail
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.tail
        </td>
      </tr><tr>
        <td>
          423
        </td>
        <td>
          2459
        </td>
        <td>
          17398
          -
          17402
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          423
        </td>
        <td>
          2458
        </td>
        <td>
          17387
          -
          17396
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;IGNORED&quot;
        </td>
      </tr><tr>
        <td>
          425
        </td>
        <td>
          2463
        </td>
        <td>
          17455
          -
          17492
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;murmur3ID&quot;, Murmur3_128_64)
        </td>
      </tr><tr>
        <td>
          429
        </td>
        <td>
          2464
        </td>
        <td>
          17589
          -
          17598
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size
        </td>
      </tr><tr>
        <td>
          430
        </td>
        <td>
          2466
        </td>
        <td>
          17636
          -
          17650
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.literalsNeeded
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          RuleRunnerFunctions.literalsNeeded
        </td>
      </tr><tr>
        <td>
          430
        </td>
        <td>
          2465
        </td>
        <td>
          17627
          -
          17632
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.&lt;(2)
        </td>
      </tr><tr>
        <td>
          431
        </td>
        <td>
          2468
        </td>
        <td>
          17671
          -
          17691
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRunnerFunctions.getString(exps.head)
        </td>
      </tr><tr>
        <td>
          431
        </td>
        <td>
          2467
        </td>
        <td>
          17681
          -
          17690
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          433
        </td>
        <td>
          2470
        </td>
        <td>
          17759
          -
          17784
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashFunctionFactory.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.hash.HashFunctionFactory.apply(impl)
        </td>
      </tr><tr>
        <td>
          433
        </td>
        <td>
          2469
        </td>
        <td>
          17732
          -
          17741
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableLike.tail
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.tail
        </td>
      </tr><tr>
        <td>
          433
        </td>
        <td>
          2471
        </td>
        <td>
          17708
          -
          17785
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashFunctionsExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.hash.HashFunctionsExpression.apply(exps.tail, impl, asStruct, com.sparkutils.quality.impl.hash.HashFunctionFactory.apply(impl))
        </td>
      </tr><tr>
        <td>
          435
        </td>
        <td>
          2472
        </td>
        <td>
          17805
          -
          17815
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;hashWith&quot;
        </td>
      </tr><tr>
        <td>
          435
        </td>
        <td>
          2474
        </td>
        <td>
          17796
          -
          17834
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;hashWith&quot;, hashWithF(false))
        </td>
      </tr><tr>
        <td>
          435
        </td>
        <td>
          2473
        </td>
        <td>
          17817
          -
          17833
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.hashWithF
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          hashWithF(false)
        </td>
      </tr><tr>
        <td>
          436
        </td>
        <td>
          2475
        </td>
        <td>
          17848
          -
          17864
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;hashWithStruct&quot;
        </td>
      </tr><tr>
        <td>
          436
        </td>
        <td>
          2477
        </td>
        <td>
          17839
          -
          17882
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;hashWithStruct&quot;, hashWithF(true))
        </td>
      </tr><tr>
        <td>
          436
        </td>
        <td>
          2476
        </td>
        <td>
          17866
          -
          17881
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.hashWithF
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          hashWithF(true)
        </td>
      </tr><tr>
        <td>
          440
        </td>
        <td>
          2478
        </td>
        <td>
          17983
          -
          17992
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size
        </td>
      </tr><tr>
        <td>
          441
        </td>
        <td>
          2479
        </td>
        <td>
          18021
          -
          18026
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.&lt;(2)
        </td>
      </tr><tr>
        <td>
          441
        </td>
        <td>
          2480
        </td>
        <td>
          18030
          -
          18044
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.literalsNeeded
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          RuleRunnerFunctions.literalsNeeded
        </td>
      </tr><tr>
        <td>
          442
        </td>
        <td>
          2481
        </td>
        <td>
          18075
          -
          18084
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          442
        </td>
        <td>
          2482
        </td>
        <td>
          18065
          -
          18085
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRunnerFunctions.getString(exps.head)
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          2484
        </td>
        <td>
          18159
          -
          18196
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.ZALongHashFunctionFactory.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.hash.ZALongHashFunctionFactory.apply(digestImpl)
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          2483
        </td>
        <td>
          18126
          -
          18135
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableLike.tail
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.tail
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          2485
        </td>
        <td>
          18102
          -
          18197
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashFunctionsExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.hash.HashFunctionsExpression.apply(exps.tail, digestImpl, asStruct, com.sparkutils.quality.impl.hash.ZALongHashFunctionFactory.apply(digestImpl))
        </td>
      </tr><tr>
        <td>
          446
        </td>
        <td>
          2488
        </td>
        <td>
          18208
          -
          18246
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;zaHashWith&quot;, zahashF(false))
        </td>
      </tr><tr>
        <td>
          446
        </td>
        <td>
          2487
        </td>
        <td>
          18231
          -
          18245
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.zahashF
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          zahashF(false)
        </td>
      </tr><tr>
        <td>
          446
        </td>
        <td>
          2486
        </td>
        <td>
          18217
          -
          18229
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;zaHashWith&quot;
        </td>
      </tr><tr>
        <td>
          447
        </td>
        <td>
          2490
        </td>
        <td>
          18317
          -
          18330
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.zahashF
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          zahashF(true)
        </td>
      </tr><tr>
        <td>
          447
        </td>
        <td>
          2489
        </td>
        <td>
          18297
          -
          18315
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;zaHashWithStruct&quot;
        </td>
      </tr><tr>
        <td>
          447
        </td>
        <td>
          2491
        </td>
        <td>
          18288
          -
          18331
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;zaHashWithStruct&quot;, zahashF(true))
        </td>
      </tr><tr>
        <td>
          451
        </td>
        <td>
          2492
        </td>
        <td>
          18474
          -
          18483
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.size
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.size
        </td>
      </tr><tr>
        <td>
          452
        </td>
        <td>
          2493
        </td>
        <td>
          18512
          -
          18517
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          a.&lt;(2)
        </td>
      </tr><tr>
        <td>
          452
        </td>
        <td>
          2494
        </td>
        <td>
          18521
          -
          18535
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.literalsNeeded
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          RuleRunnerFunctions.literalsNeeded
        </td>
      </tr><tr>
        <td>
          453
        </td>
        <td>
          2496
        </td>
        <td>
          18556
          -
          18576
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.getString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          RuleRunnerFunctions.getString(exps.head)
        </td>
      </tr><tr>
        <td>
          453
        </td>
        <td>
          2495
        </td>
        <td>
          18566
          -
          18575
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.IterableLike.head
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.head
        </td>
      </tr><tr>
        <td>
          455
        </td>
        <td>
          2499
        </td>
        <td>
          18593
          -
          18693
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.HashFunctionsExpression.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.hash.HashFunctionsExpression.apply(exps.tail, digestImpl, asStruct, com.sparkutils.quality.impl.hash.ZALongTupleHashFunctionFactory.apply(digestImpl))
        </td>
      </tr><tr>
        <td>
          455
        </td>
        <td>
          2498
        </td>
        <td>
          18650
          -
          18692
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.hash.ZALongTupleHashFunctionFactory.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.hash.ZALongTupleHashFunctionFactory.apply(digestImpl)
        </td>
      </tr><tr>
        <td>
          455
        </td>
        <td>
          2497
        </td>
        <td>
          18617
          -
          18626
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableLike.tail
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exps.tail
        </td>
      </tr><tr>
        <td>
          457
        </td>
        <td>
          2502
        </td>
        <td>
          18704
          -
          18752
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;zaHashLongsWith&quot;, zaTuplehashF(false))
        </td>
      </tr><tr>
        <td>
          457
        </td>
        <td>
          2501
        </td>
        <td>
          18732
          -
          18751
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.zaTuplehashF
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          zaTuplehashF(false)
        </td>
      </tr><tr>
        <td>
          457
        </td>
        <td>
          2500
        </td>
        <td>
          18713
          -
          18730
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;zaHashLongsWith&quot;
        </td>
      </tr><tr>
        <td>
          458
        </td>
        <td>
          2505
        </td>
        <td>
          18757
          -
          18810
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;zaHashLongsWithStruct&quot;, zaTuplehashF(true))
        </td>
      </tr><tr>
        <td>
          458
        </td>
        <td>
          2504
        </td>
        <td>
          18791
          -
          18809
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctionsImport.zaTuplehashF
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          zaTuplehashF(true)
        </td>
      </tr><tr>
        <td>
          458
        </td>
        <td>
          2503
        </td>
        <td>
          18766
          -
          18789
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;zaHashLongsWithStruct&quot;
        </td>
      </tr><tr>
        <td>
          461
        </td>
        <td>
          2508
        </td>
        <td>
          18929
          -
          18929
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
      </tr><tr>
        <td>
          461
        </td>
        <td>
          2510
        </td>
        <td>
          18884
          -
          18989
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;coalesceIfAttributesMissing&quot;, ((x$16: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.QualityException.qualityException(&quot;coalesceIf functions cannot be created&quot;, com.sparkutils.quality.QualityException.qualityException$default$2)))
        </td>
      </tr><tr>
        <td>
          461
        </td>
        <td>
          2507
        </td>
        <td>
          18946
          -
          18986
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;coalesceIf functions cannot be created&quot;
        </td>
      </tr><tr>
        <td>
          461
        </td>
        <td>
          2506
        </td>
        <td>
          18893
          -
          18922
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;coalesceIfAttributesMissing&quot;
        </td>
      </tr><tr>
        <td>
          461
        </td>
        <td>
          2509
        </td>
        <td>
          18929
          -
          18987
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.sparkutils.quality.QualityException.qualityException(&quot;coalesceIf functions cannot be created&quot;, com.sparkutils.quality.QualityException.qualityException$default$2)
        </td>
      </tr><tr>
        <td>
          462
        </td>
        <td>
          2511
        </td>
        <td>
          19003
          -
          19039
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;coalesceIfAttributesMissingDisable&quot;
        </td>
      </tr><tr>
        <td>
          462
        </td>
        <td>
          2514
        </td>
        <td>
          19046
          -
          19104
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.sparkutils.quality.QualityException.qualityException(&quot;coalesceIf functions cannot be created&quot;, com.sparkutils.quality.QualityException.qualityException$default$2)
        </td>
      </tr><tr>
        <td>
          462
        </td>
        <td>
          2513
        </td>
        <td>
          19046
          -
          19046
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
      </tr><tr>
        <td>
          462
        </td>
        <td>
          2515
        </td>
        <td>
          18994
          -
          19106
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;coalesceIfAttributesMissingDisable&quot;, ((x$17: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; com.sparkutils.quality.QualityException.qualityException(&quot;coalesceIf functions cannot be created&quot;, com.sparkutils.quality.QualityException.qualityException$default$2)))
        </td>
      </tr><tr>
        <td>
          462
        </td>
        <td>
          2512
        </td>
        <td>
          19063
          -
          19103
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;coalesceIf functions cannot be created&quot;
        </td>
      </tr><tr>
        <td>
          465
        </td>
        <td>
          2517
        </td>
        <td>
          19252
          -
          19285
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.utils.StructFunctions.withFieldFunction
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.utils.StructFunctions.withFieldFunction
        </td>
      </tr><tr>
        <td>
          465
        </td>
        <td>
          2516
        </td>
        <td>
          19237
          -
          19250
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;updateField&quot;
        </td>
      </tr><tr>
        <td>
          465
        </td>
        <td>
          2518
        </td>
        <td>
          19228
          -
          19286
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;updateField&quot;, com.sparkutils.quality.utils.StructFunctions.withFieldFunction)
        </td>
      </tr><tr>
        <td>
          469
        </td>
        <td>
          2520
        </td>
        <td>
          19444
          -
          19461
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[String, org.apache.spark.sql.catalyst.expressions.Expression](str.toString(), e)
        </td>
      </tr><tr>
        <td>
          469
        </td>
        <td>
          2519
        </td>
        <td>
          19445
          -
          19457
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.unsafe.types.UTF8String.toString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          str.toString()
        </td>
      </tr><tr>
        <td>
          471
        </td>
        <td>
          2521
        </td>
        <td>
          19503
          -
          19518
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[String, org.apache.spark.sql.catalyst.expressions.Expression](msgDefault, e)
        </td>
      </tr><tr>
        <td>
          474
        </td>
        <td>
          2526
        </td>
        <td>
          19530
          -
          19685
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;printCode&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; {
  &lt;synthetic&gt; &lt;artifact&gt; private[this] val x$18: (String, org.apache.spark.sql.catalyst.expressions.Expression) = (msgAndExpr(com.sparkutils.quality.utils.PrintCode.apply(exps.apply(0), com.sparkutils.quality.utils.PrintCode.apply$default$2, com.sparkutils.quality.utils.PrintCode.apply$default$3).msg, exps): (String, org.apache.spark.sql.catalyst.expressions.Expression) @unchecked) match {
    case (_1: String, _2: org.apache.spark.sql.catalyst.expressions.Expression)(String, org.apache.spark.sql.catalyst.expressions.Expression)((msg @ _), (exp @ _)) =&gt; scala.Tuple2.apply[String, org.apache.spark.sql.catalyst.expressions.Expression](msg, exp)
  };
  val msg: String = x$18._1;
  val exp: org.apache.spark.sql.catalyst.expressions.Expression = x$18._2;
  com.sparkutils.quality.utils.PrintCode.apply(exp, msg, writer)
}))
        </td>
      </tr><tr>
        <td>
          474
        </td>
        <td>
          2522
        </td>
        <td>
          19539
          -
          19550
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;printCode&quot;
        </td>
      </tr><tr>
        <td>
          475
        </td>
        <td>
          2523
        </td>
        <td>
          19592
          -
          19592
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$18._1
        </td>
      </tr><tr>
        <td>
          475
        </td>
        <td>
          2524
        </td>
        <td>
          19597
          -
          19597
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$18._2
        </td>
      </tr><tr>
        <td>
          476
        </td>
        <td>
          2525
        </td>
        <td>
          19651
          -
          19678
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.utils.PrintCode.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.utils.PrintCode.apply(exp, msg, writer)
        </td>
      </tr><tr>
        <td>
          478
        </td>
        <td>
          2537
        </td>
        <td>
          19690
          -
          19871
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          register.apply(&quot;printExpr&quot;, ((exps: Seq[org.apache.spark.sql.catalyst.expressions.Expression]) =&gt; {
  &lt;synthetic&gt; &lt;artifact&gt; private[this] val x$19: (String, org.apache.spark.sql.catalyst.expressions.Expression) = (msgAndExpr(&quot;Expression toStr is -&gt;&quot;, exps): (String, org.apache.spark.sql.catalyst.expressions.Expression) @unchecked) match {
    case (_1: String, _2: org.apache.spark.sql.catalyst.expressions.Expression)(String, org.apache.spark.sql.catalyst.expressions.Expression)((msg @ _), (exp @ _)) =&gt; scala.Tuple2.apply[String, org.apache.spark.sql.catalyst.expressions.Expression](msg, exp)
  };
  val msg: String = x$19._1;
  val exp: org.apache.spark.sql.catalyst.expressions.Expression = x$19._2;
  writer.apply(scala.StringContext.apply(&quot;&quot;, &quot; &quot;, &quot; .  Sql is &quot;, &quot;&quot;).s(msg, exp, exp.sql));
  exp
}))
        </td>
      </tr><tr>
        <td>
          478
        </td>
        <td>
          2527
        </td>
        <td>
          19699
          -
          19710
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;printExpr&quot;
        </td>
      </tr><tr>
        <td>
          479
        </td>
        <td>
          2529
        </td>
        <td>
          19757
          -
          19757
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$19._2
        </td>
      </tr><tr>
        <td>
          479
        </td>
        <td>
          2528
        </td>
        <td>
          19752
          -
          19752
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$19._1
        </td>
      </tr><tr>
        <td>
          480
        </td>
        <td>
          2532
        </td>
        <td>
          19831
          -
          19843
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot; .  Sql is &quot;
        </td>
      </tr><tr>
        <td>
          480
        </td>
        <td>
          2535
        </td>
        <td>
          19820
          -
          19853
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;&quot;, &quot; &quot;, &quot; .  Sql is &quot;, &quot;&quot;).s(msg, exp, exp.sql)
        </td>
      </tr><tr>
        <td>
          480
        </td>
        <td>
          2534
        </td>
        <td>
          19844
          -
          19851
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.sql
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          exp.sql
        </td>
      </tr><tr>
        <td>
          480
        </td>
        <td>
          2531
        </td>
        <td>
          19826
          -
          19828
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot; &quot;
        </td>
      </tr><tr>
        <td>
          480
        </td>
        <td>
          2533
        </td>
        <td>
          19852
          -
          19853
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          480
        </td>
        <td>
          2536
        </td>
        <td>
          19813
          -
          19854
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Function1.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          writer.apply(scala.StringContext.apply(&quot;&quot;, &quot; &quot;, &quot; .  Sql is &quot;, &quot;&quot;).s(msg, exp, exp.sql))
        </td>
      </tr><tr>
        <td>
          480
        </td>
        <td>
          2530
        </td>
        <td>
          19822
          -
          19823
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          489
        </td>
        <td>
          2538
        </td>
        <td>
          19969
          -
          20012
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;Cannot setup expression with non-literals&quot;
        </td>
      </tr><tr>
        <td>
          489
        </td>
        <td>
          2540
        </td>
        <td>
          19952
          -
          20013
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.sparkutils.quality.QualityException.qualityException(&quot;Cannot setup expression with non-literals&quot;, com.sparkutils.quality.QualityException.qualityException$default$2)
        </td>
      </tr><tr>
        <td>
          489
        </td>
        <td>
          2539
        </td>
        <td>
          19952
          -
          19952
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.sparkutils.quality.QualityException.qualityException$default$2
        </td>
      </tr><tr>
        <td>
          493
        </td>
        <td>
          2541
        </td>
        <td>
          20147
          -
          20161
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.literalsNeeded
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          RuleRunnerFunctions.this.literalsNeeded
        </td>
      </tr><tr>
        <td>
          498
        </td>
        <td>
          2542
        </td>
        <td>
          20306
          -
          20320
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.literalsNeeded
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          RuleRunnerFunctions.this.literalsNeeded
        </td>
      </tr><tr>
        <td>
          502
        </td>
        <td>
          2543
        </td>
        <td>
          20448
          -
          20462
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.unsafe.types.UTF8String.toString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          str.toString()
        </td>
      </tr><tr>
        <td>
          503
        </td>
        <td>
          2544
        </td>
        <td>
          20479
          -
          20493
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunnerFunctions.literalsNeeded
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          RuleRunnerFunctions.this.literalsNeeded
        </td>
      </tr><tr>
        <td>
          507
        </td>
        <td>
          2547
        </td>
        <td>
          20646
          -
          20646
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]
        </td>
      </tr><tr>
        <td>
          507
        </td>
        <td>
          2550
        </td>
        <td>
          20590
          -
          20766
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.flatMap
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ruleSuite.ruleSets.flatMap[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](((ruleSet: com.sparkutils.quality.RuleSet) =&gt; ruleSet.rules.map[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](((rule: com.sparkutils.quality.Rule) =&gt; rule.expression match {
  case (r @ (_: com.sparkutils.quality.ExprLogic)) =&gt; r.expr
}))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression])))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression])
        </td>
      </tr><tr>
        <td>
          507
        </td>
        <td>
          2549
        </td>
        <td>
          20616
          -
          20616
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]
        </td>
      </tr><tr>
        <td>
          507
        </td>
        <td>
          2548
        </td>
        <td>
          20629
          -
          20765
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ruleSet.rules.map[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]](((rule: com.sparkutils.quality.Rule) =&gt; rule.expression match {
  case (r @ (_: com.sparkutils.quality.ExprLogic)) =&gt; r.expr
}))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression])
        </td>
      </tr><tr>
        <td>
          508
        </td>
        <td>
          2545
        </td>
        <td>
          20661
          -
          20676
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.Rule.expression
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          rule.expression
        </td>
      </tr><tr>
        <td>
          509
        </td>
        <td>
          2546
        </td>
        <td>
          20714
          -
          20720
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.HasExpr.expr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          r.expr
        </td>
      </tr><tr>
        <td>
          512
        </td>
        <td>
          2553
        </td>
        <td>
          20820
          -
          20827
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;rngID&quot;
        </td>
      </tr><tr>
        <td>
          512
        </td>
        <td>
          2552
        </td>
        <td>
          20809
          -
          20819
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;uniqueID&quot;
        </td>
      </tr><tr>
        <td>
          512
        </td>
        <td>
          2555
        </td>
        <td>
          20841
          -
          20855
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;fieldBasedID&quot;
        </td>
      </tr><tr>
        <td>
          512
        </td>
        <td>
          2551
        </td>
        <td>
          20797
          -
          20808
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;murmur3ID&quot;
        </td>
      </tr><tr>
        <td>
          512
        </td>
        <td>
          2614
        </td>
        <td>
          20793
          -
          21829
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.Set.apply[String](&quot;murmur3ID&quot;, &quot;uniqueID&quot;, &quot;rngID&quot;, &quot;providedID&quot;, &quot;fieldBasedID&quot;, &quot;digestToLongs&quot;, &quot;digestToLongsStruct&quot;, &quot;ruleSuiteResultDetails&quot;, &quot;idEqual&quot;, &quot;longPairEqual&quot;, &quot;bigBloom&quot;, &quot;smallBloom&quot;, &quot;longPairFromUUID&quot;, &quot;longPair&quot;, &quot;rngUUID&quot;, &quot;rng&quot;, &quot;rngBytes&quot;, &quot;returnSum&quot;, &quot;sumWith&quot;, &quot;resultsWith&quot;, &quot;inc&quot;, &quot;meanF&quot;, &quot;aggExpr&quot;, &quot;passed&quot;, &quot;failed&quot;, &quot;softFailed&quot;, &quot;disabledRule&quot;, &quot;packInts&quot;, &quot;unpack&quot;, &quot;unpackIdTriple&quot;, &quot;softFail&quot;, &quot;probability&quot;, &quot;flattenResults&quot;, &quot;flattenRuleResults&quot;, &quot;flattenFolderResults&quot;, &quot;probabilityIn&quot;, &quot;mapLookup&quot;, &quot;mapContains&quot;, &quot;saferLongPair&quot;, &quot;hashWith&quot;, &quot;hashWithStruct&quot;, &quot;zaHashWith&quot;, &quot;zaHashLongsWith&quot;, &quot;hashFieldBasedID&quot;, &quot;zaLongsFieldBasedID&quot;, &quot;zaHashLongsWithStruct&quot;, &quot;zaHashWithStruct&quot;, &quot;zaFieldBasedID&quot;, &quot;prefixedToLongPair&quot;, &quot;coalesceIfAttributesMissing&quot;, &quot;coalesceIfAttributesMissingDisable&quot;, &quot;updateField&quot;, org.apache.spark.sql.qualityFunctions.LambdaFunctions.PlaceHolder, org.apache.spark.sql.qualityFunctions.LambdaFunctions.Lambda, org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun, &quot;printExpr&quot;, &quot;printCode&quot;, &quot;comparableMaps&quot;, &quot;reverseComparableMaps&quot;, &quot;as_uuid&quot;, &quot;id_size&quot;, &quot;id_base64&quot;, &quot;id_from_base64&quot;)
        </td>
      </tr><tr>
        <td>
          512
        </td>
        <td>
          2554
        </td>
        <td>
          20828
          -
          20840
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;providedID&quot;
        </td>
      </tr><tr>
        <td>
          513
        </td>
        <td>
          2556
        </td>
        <td>
          20861
          -
          20876
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;digestToLongs&quot;
        </td>
      </tr><tr>
        <td>
          513
        </td>
        <td>
          2559
        </td>
        <td>
          20924
          -
          20933
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;idEqual&quot;
        </td>
      </tr><tr>
        <td>
          513
        </td>
        <td>
          2562
        </td>
        <td>
          20961
          -
          20973
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;smallBloom&quot;
        </td>
      </tr><tr>
        <td>
          513
        </td>
        <td>
          2561
        </td>
        <td>
          20950
          -
          20960
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;bigBloom&quot;
        </td>
      </tr><tr>
        <td>
          513
        </td>
        <td>
          2558
        </td>
        <td>
          20899
          -
          20923
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;ruleSuiteResultDetails&quot;
        </td>
      </tr><tr>
        <td>
          513
        </td>
        <td>
          2557
        </td>
        <td>
          20877
          -
          20898
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;digestToLongsStruct&quot;
        </td>
      </tr><tr>
        <td>
          513
        </td>
        <td>
          2560
        </td>
        <td>
          20934
          -
          20949
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;longPairEqual&quot;
        </td>
      </tr><tr>
        <td>
          514
        </td>
        <td>
          2565
        </td>
        <td>
          21009
          -
          21018
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;rngUUID&quot;
        </td>
      </tr><tr>
        <td>
          514
        </td>
        <td>
          2568
        </td>
        <td>
          21036
          -
          21047
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;returnSum&quot;
        </td>
      </tr><tr>
        <td>
          514
        </td>
        <td>
          2570
        </td>
        <td>
          21058
          -
          21071
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;resultsWith&quot;
        </td>
      </tr><tr>
        <td>
          514
        </td>
        <td>
          2564
        </td>
        <td>
          20998
          -
          21008
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;longPair&quot;
        </td>
      </tr><tr>
        <td>
          514
        </td>
        <td>
          2567
        </td>
        <td>
          21025
          -
          21035
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;rngBytes&quot;
        </td>
      </tr><tr>
        <td>
          514
        </td>
        <td>
          2566
        </td>
        <td>
          21019
          -
          21024
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;rng&quot;
        </td>
      </tr><tr>
        <td>
          514
        </td>
        <td>
          2569
        </td>
        <td>
          21048
          -
          21057
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;sumWith&quot;
        </td>
      </tr><tr>
        <td>
          514
        </td>
        <td>
          2563
        </td>
        <td>
          20979
          -
          20997
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;longPairFromUUID&quot;
        </td>
      </tr><tr>
        <td>
          515
        </td>
        <td>
          2571
        </td>
        <td>
          21077
          -
          21082
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;inc&quot;
        </td>
      </tr><tr>
        <td>
          515
        </td>
        <td>
          2574
        </td>
        <td>
          21101
          -
          21109
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;passed&quot;
        </td>
      </tr><tr>
        <td>
          515
        </td>
        <td>
          2577
        </td>
        <td>
          21132
          -
          21146
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;disabledRule&quot;
        </td>
      </tr><tr>
        <td>
          515
        </td>
        <td>
          2579
        </td>
        <td>
          21158
          -
          21166
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;unpack&quot;
        </td>
      </tr><tr>
        <td>
          515
        </td>
        <td>
          2573
        </td>
        <td>
          21091
          -
          21100
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;aggExpr&quot;
        </td>
      </tr><tr>
        <td>
          515
        </td>
        <td>
          2576
        </td>
        <td>
          21119
          -
          21131
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;softFailed&quot;
        </td>
      </tr><tr>
        <td>
          515
        </td>
        <td>
          2575
        </td>
        <td>
          21110
          -
          21118
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;failed&quot;
        </td>
      </tr><tr>
        <td>
          515
        </td>
        <td>
          2578
        </td>
        <td>
          21147
          -
          21157
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;packInts&quot;
        </td>
      </tr><tr>
        <td>
          515
        </td>
        <td>
          2572
        </td>
        <td>
          21083
          -
          21090
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;meanF&quot;
        </td>
      </tr><tr>
        <td>
          516
        </td>
        <td>
          2580
        </td>
        <td>
          21172
          -
          21188
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;unpackIdTriple&quot;
        </td>
      </tr><tr>
        <td>
          516
        </td>
        <td>
          2583
        </td>
        <td>
          21214
          -
          21230
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;flattenResults&quot;
        </td>
      </tr><tr>
        <td>
          516
        </td>
        <td>
          2586
        </td>
        <td>
          21277
          -
          21292
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;probabilityIn&quot;
        </td>
      </tr><tr>
        <td>
          516
        </td>
        <td>
          2582
        </td>
        <td>
          21200
          -
          21213
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;probability&quot;
        </td>
      </tr><tr>
        <td>
          516
        </td>
        <td>
          2585
        </td>
        <td>
          21253
          -
          21275
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;flattenFolderResults&quot;
        </td>
      </tr><tr>
        <td>
          516
        </td>
        <td>
          2584
        </td>
        <td>
          21231
          -
          21251
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;flattenRuleResults&quot;
        </td>
      </tr><tr>
        <td>
          516
        </td>
        <td>
          2581
        </td>
        <td>
          21189
          -
          21199
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;softFail&quot;
        </td>
      </tr><tr>
        <td>
          517
        </td>
        <td>
          2589
        </td>
        <td>
          21324
          -
          21339
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;saferLongPair&quot;
        </td>
      </tr><tr>
        <td>
          517
        </td>
        <td>
          2592
        </td>
        <td>
          21368
          -
          21380
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;zaHashWith&quot;
        </td>
      </tr><tr>
        <td>
          517
        </td>
        <td>
          2588
        </td>
        <td>
          21310
          -
          21323
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;mapContains&quot;
        </td>
      </tr><tr>
        <td>
          517
        </td>
        <td>
          2591
        </td>
        <td>
          21351
          -
          21367
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;hashWithStruct&quot;
        </td>
      </tr><tr>
        <td>
          517
        </td>
        <td>
          2593
        </td>
        <td>
          21382
          -
          21399
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;zaHashLongsWith&quot;
        </td>
      </tr><tr>
        <td>
          517
        </td>
        <td>
          2587
        </td>
        <td>
          21298
          -
          21309
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;mapLookup&quot;
        </td>
      </tr><tr>
        <td>
          517
        </td>
        <td>
          2590
        </td>
        <td>
          21340
          -
          21350
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;hashWith&quot;
        </td>
      </tr><tr>
        <td>
          518
        </td>
        <td>
          2598
        </td>
        <td>
          21491
          -
          21507
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;zaFieldBasedID&quot;
        </td>
      </tr><tr>
        <td>
          518
        </td>
        <td>
          2595
        </td>
        <td>
          21424
          -
          21445
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;zaLongsFieldBasedID&quot;
        </td>
      </tr><tr>
        <td>
          518
        </td>
        <td>
          2594
        </td>
        <td>
          21405
          -
          21423
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;hashFieldBasedID&quot;
        </td>
      </tr><tr>
        <td>
          518
        </td>
        <td>
          2597
        </td>
        <td>
          21471
          -
          21489
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;zaHashWithStruct&quot;
        </td>
      </tr><tr>
        <td>
          518
        </td>
        <td>
          2596
        </td>
        <td>
          21446
          -
          21469
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;zaHashLongsWithStruct&quot;
        </td>
      </tr><tr>
        <td>
          518
        </td>
        <td>
          2599
        </td>
        <td>
          21509
          -
          21529
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;prefixedToLongPair&quot;
        </td>
      </tr><tr>
        <td>
          519
        </td>
        <td>
          2601
        </td>
        <td>
          21566
          -
          21602
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;coalesceIfAttributesMissingDisable&quot;
        </td>
      </tr><tr>
        <td>
          519
        </td>
        <td>
          2603
        </td>
        <td>
          21619
          -
          21646
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.PlaceHolder
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.PlaceHolder
        </td>
      </tr><tr>
        <td>
          519
        </td>
        <td>
          2600
        </td>
        <td>
          21535
          -
          21564
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;coalesceIfAttributesMissing&quot;
        </td>
      </tr><tr>
        <td>
          519
        </td>
        <td>
          2602
        </td>
        <td>
          21604
          -
          21617
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;updateField&quot;
        </td>
      </tr><tr>
        <td>
          520
        </td>
        <td>
          2607
        </td>
        <td>
          21714
          -
          21725
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;printCode&quot;
        </td>
      </tr><tr>
        <td>
          520
        </td>
        <td>
          2610
        </td>
        <td>
          21770
          -
          21779
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;as_uuid&quot;
        </td>
      </tr><tr>
        <td>
          520
        </td>
        <td>
          2604
        </td>
        <td>
          21652
          -
          21674
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.Lambda
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.Lambda
        </td>
      </tr><tr>
        <td>
          520
        </td>
        <td>
          2606
        </td>
        <td>
          21701
          -
          21712
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;printExpr&quot;
        </td>
      </tr><tr>
        <td>
          520
        </td>
        <td>
          2609
        </td>
        <td>
          21745
          -
          21768
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;reverseComparableMaps&quot;
        </td>
      </tr><tr>
        <td>
          520
        </td>
        <td>
          2605
        </td>
        <td>
          21676
          -
          21699
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.qualityFunctions.LambdaFunctions.CallFun
        </td>
      </tr><tr>
        <td>
          520
        </td>
        <td>
          2608
        </td>
        <td>
          21727
          -
          21743
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;comparableMaps&quot;
        </td>
      </tr><tr>
        <td>
          521
        </td>
        <td>
          2613
        </td>
        <td>
          21809
          -
          21825
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;id_from_base64&quot;
        </td>
      </tr><tr>
        <td>
          521
        </td>
        <td>
          2612
        </td>
        <td>
          21796
          -
          21807
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;id_base64&quot;
        </td>
      </tr><tr>
        <td>
          521
        </td>
        <td>
          2611
        </td>
        <td>
          21785
          -
          21794
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;id_size&quot;
        </td>
      </tr>
    </table>
          </div>
        </div>
      </body>
    </html>