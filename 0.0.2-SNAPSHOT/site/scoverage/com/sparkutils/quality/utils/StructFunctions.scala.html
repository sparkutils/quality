<html>
      <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <title id="title">
          com/sparkutils/quality/utils/StructFunctions.scala.html
        </title>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/css/theme.default.min.css" type="text/css"/><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/js/jquery.tablesorter.min.js"></script><link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css" type="text/css"/><script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script><script type="text/javascript">
        $(document).ready(function() {$(".tablesorter").tablesorter();});
      </script>
        <style>
          table.codegrid { font-family: monospace; font-size: 12px; width: auto!important; }table.statementlist { width: auto!important; font-size: 13px; } table.codegrid td { padding: 0!important; border: 0!important } table td.linenumber { width: 40px!important; } 
        </style>
      </head>
      <body style="font-family: monospace;">
        <ul class="nav nav-tabs">
          <li>
            <a href="#codegrid" data-toggle="tab">Codegrid</a>
          </li>
          <li>
            <a href="#statementlist" data-toggle="tab">Statement List</a>
          </li>
        </ul>
        <div class="tab-content">
          <div class="tab-pane active" id="codegrid">
            <pre style='font-size: 12pt; font-family: courier, monospace;'>1 <span style=''>package com.sparkutils.quality.utils
</span>2 <span style=''>
</span>3 <span style=''>import org.apache.spark.sql.catalyst.expressions.{Expression, ExpressionDescription, GenericInternalRow}
</span>4 <span style=''>
</span>5 <span style=''>/**
</span>6 <span style=''> * The simple UpdateFields(exps(0), RuleRunnerFunctions.getString(exps(1)), exps(2)) does not work on 3.1.1
</span>7 <span style=''> * As such the sme approach must be taken.
</span>8 <span style=''> */
</span>9 <span style=''>object StructFunctions {
</span>10 <span style=''>  val withFieldFunction = (exps: Seq[Expression]) =&gt; {
</span>11 <span style=''>    </span><span style='background: #AEF1AE'>AddFields(exps)</span><span style=''>
</span>12 <span style=''>    //UpdateFields(exps(0), RuleRunnerFunctions.getString(exps(1)), exps(2))
</span>13 <span style=''>  }
</span>14 <span style=''>}
</span>15 <span style=''>
</span>16 <span style=''>// below c+p'd from https://raw.githubusercontent.com/fqaiser94/mse/master/src/main/scala/org/apache/spark/sql/catalyst/expressions/AddFields.scala , apache 2 but only for 2.4
</span>17 <span style=''>// only withNewChildrenInternal is added for 3.2 support
</span>18 <span style=''>
</span>19 <span style=''>import org.apache.spark.sql.catalyst.InternalRow
</span>20 <span style=''>import org.apache.spark.sql.catalyst.analysis.TypeCheckResult
</span>21 <span style=''>import org.apache.spark.sql.catalyst.expressions.codegen.Block._
</span>22 <span style=''>import org.apache.spark.sql.catalyst.expressions.codegen.{CodeGenerator, CodegenContext, ExprCode, FalseLiteral}
</span>23 <span style=''>import org.apache.spark.sql.types.{StringType, StructField, StructType}
</span>24 <span style=''>import org.apache.spark.unsafe.types.UTF8String
</span>25 <span style=''>
</span>26 <span style=''>/**
</span>27 <span style=''> * Adds/replaces fields in a struct.
</span>28 <span style=''> * Returns null if struct is null.
</span>29 <span style=''> * If multiple fields already exist with the one of the given fieldNames, they will all be replaced.
</span>30 <span style=''> */
</span>31 <span style=''>// scalastyle:off line.size.limit
</span>32 <span style=''>@ExpressionDescription(
</span>33 <span style=''>  usage = &quot;_FUNC_(struct, name1, val1, name2, val2, ...) - Adds/replaces fields in struct by name.&quot;,
</span>34 <span style=''>  examples = &quot;&quot;&quot;
</span>35 <span style=''>    Examples:
</span>36 <span style=''>      &gt; SELECT _FUNC_({&quot;a&quot;:1}, &quot;b&quot;, 2, &quot;c&quot;, 3);
</span>37 <span style=''>       {&quot;a&quot;:1,&quot;b&quot;:2,&quot;c&quot;:3}
</span>38 <span style=''>  &quot;&quot;&quot;)
</span>39 <span style=''>// scalastyle:on line.size.limit
</span>40 <span style=''>case class AddFields(children: Seq[Expression]) extends Expression {
</span>41 <span style=''>
</span>42 <span style=''>  private lazy val struct: Expression = children.head
</span>43 <span style=''>  private lazy val (nameExprs, valExprs) = children.drop(1).grouped(2).map {
</span>44 <span style=''>    case Seq(name, value) =&gt; (name, value)
</span>45 <span style=''>  }.toList.unzip
</span>46 <span style=''>  private lazy val fieldNames = nameExprs.map(_.eval().asInstanceOf[UTF8String].toString)
</span>47 <span style=''>  private lazy val pairs = fieldNames.zip(valExprs)
</span>48 <span style=''>
</span>49 <span style=''>  override def nullable: Boolean = </span><span style='background: #AEF1AE'>struct.nullable</span><span style=''>
</span>50 <span style=''>
</span>51 <span style=''>  private lazy val ogStructType: StructType =
</span>52 <span style=''>    struct.dataType.asInstanceOf[StructType]
</span>53 <span style=''>
</span>54 <span style=''>  override lazy val dataType: StructType = {
</span>55 <span style=''>    val existingFields = ogStructType.fields.map { x =&gt; (x.name, x) }
</span>56 <span style=''>    val addOrReplaceFields = pairs.map { case (fieldName, field) =&gt;
</span>57 <span style=''>      (fieldName, StructField(fieldName, field.dataType, field.nullable))
</span>58 <span style=''>    }
</span>59 <span style=''>    val newFields = loop(existingFields, addOrReplaceFields).map(_._2)
</span>60 <span style=''>    StructType(newFields)
</span>61 <span style=''>  }
</span>62 <span style=''>
</span>63 <span style=''>  override def checkInputDataTypes(): TypeCheckResult = {
</span>64 <span style=''>    if (</span><span style='background: #AEF1AE'>children.size % 2 == 0</span><span style=''>) {
</span>65 <span style=''>      </span><span style='background: #F0ADAD'>return TypeCheckResult.TypeCheckFailure(s&quot;$prettyName expects an odd number of arguments.&quot;)</span><span style=''>
</span>66 <span style=''>    }
</span>67 <span style=''>
</span>68 <span style=''>    val typeName = </span><span style='background: #AEF1AE'>struct.dataType.typeName</span><span style=''>
</span>69 <span style=''>    val expectedStructType = </span><span style='background: #AEF1AE'>StructType(Nil).typeName</span><span style=''>
</span>70 <span style=''>    if (</span><span style='background: #AEF1AE'>typeName != expectedStructType</span><span style=''>) {
</span>71 <span style=''>      </span><span style='background: #F0ADAD'>return TypeCheckResult.TypeCheckFailure(
</span>72 <span style=''></span><span style='background: #F0ADAD'>        s&quot;Only $expectedStructType is allowed to appear at first position, got: $typeName.&quot;)</span><span style=''>
</span>73 <span style=''>    }
</span>74 <span style=''>
</span>75 <span style=''>    if (</span><span style='background: #AEF1AE'>nameExprs.contains(null) || nameExprs.exists(e =&gt; !(e.foldable &amp;&amp; e.dataType == StringType))</span><span style=''>) {
</span>76 <span style=''>      </span><span style='background: #F0ADAD'>return TypeCheckResult.TypeCheckFailure(
</span>77 <span style=''></span><span style='background: #F0ADAD'>        s&quot;Only non-null foldable ${StringType.catalogString} expressions are allowed to appear at even position.&quot;)</span><span style=''>
</span>78 <span style=''>    }
</span>79 <span style=''>
</span>80 <span style=''>    if (</span><span style='background: #AEF1AE'>valExprs.contains(null)</span><span style=''>) {
</span>81 <span style=''>      </span><span style='background: #F0ADAD'>return TypeCheckResult.TypeCheckFailure(
</span>82 <span style=''></span><span style='background: #F0ADAD'>        s&quot;Only non-null expressions are allowed to appear at odd positions after first position.&quot;)</span><span style=''>
</span>83 <span style=''>    }
</span>84 <span style=''>
</span>85 <span style=''>    </span><span style='background: #AEF1AE'>TypeCheckResult.TypeCheckSuccess</span><span style=''>
</span>86 <span style=''>  }
</span>87 <span style=''>
</span>88 <span style=''>  override def eval(input: InternalRow): Any = {
</span>89 <span style=''>    val structValue = </span><span style='background: #AEF1AE'>struct.eval(input)</span><span style=''>
</span>90 <span style=''>    if (</span><span style='background: #AEF1AE'>structValue == null</span><span style=''>) {
</span>91 <span style=''>      </span><span style='background: #F0ADAD'>null</span><span style=''>
</span>92 <span style=''>    } else </span><span style='background: #AEF1AE'>{
</span>93 <span style=''></span><span style='background: #AEF1AE'>      val existingValues: Seq[(FieldName, Any)] =
</span>94 <span style=''></span><span style='background: #AEF1AE'>        ogStructType.fieldNames.zip(structValue.asInstanceOf[InternalRow].toSeq(ogStructType))
</span>95 <span style=''></span><span style='background: #AEF1AE'>      val addOrReplaceValues: Seq[(FieldName, Any)] =
</span>96 <span style=''></span><span style='background: #AEF1AE'>        pairs.map { case (fieldName, expression) =&gt; (fieldName, expression.eval(input)) }
</span>97 <span style=''></span><span style='background: #AEF1AE'>      val newValues = loop(existingValues, addOrReplaceValues).map(_._2)
</span>98 <span style=''></span><span style='background: #AEF1AE'>      InternalRow.fromSeq(newValues)
</span>99 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>100 <span style=''>  }
</span>101 <span style=''>
</span>102 <span style=''>  override def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {
</span>103 <span style=''>    val structGen = </span><span style='background: #AEF1AE'>struct.genCode(ctx)</span><span style=''>
</span>104 <span style=''>    val addOrReplaceFieldsGens = </span><span style='background: #AEF1AE'>valExprs.map(_.genCode(ctx))</span><span style=''>
</span>105 <span style=''>    val resultCode: String = {
</span>106 <span style=''>      val structVar = </span><span style='background: #AEF1AE'>structGen.value</span><span style=''>
</span>107 <span style=''>      type NullCheck = String
</span>108 <span style=''>      type NonNullValue = String
</span>109 <span style=''>      val existingFieldsCode: Seq[(FieldName, (NullCheck, NonNullValue))] =
</span>110 <span style=''>        </span><span style='background: #AEF1AE'>ogStructType.fields.zipWithIndex.map {
</span>111 <span style=''></span><span style='background: #AEF1AE'>          case (structField, i) =&gt;
</span>112 <span style=''></span><span style='background: #AEF1AE'>            val nullCheck = s&quot;$structVar.isNullAt($i)&quot;
</span>113 <span style=''></span><span style='background: #AEF1AE'>            val nonNullValue = CodeGenerator.getValue(structVar, structField.dataType, i.toString)
</span>114 <span style=''></span><span style='background: #AEF1AE'>            (structField.name, (nullCheck, nonNullValue))
</span>115 <span style=''></span><span style='background: #AEF1AE'>        }</span><span style=''>
</span>116 <span style=''>      val addOrReplaceFieldsCode: Seq[(FieldName, (NullCheck, NonNullValue))] =
</span>117 <span style=''>        </span><span style='background: #AEF1AE'>fieldNames.zip(addOrReplaceFieldsGens).map {
</span>118 <span style=''></span><span style='background: #AEF1AE'>          case (fieldName, fieldExprCode) =&gt;
</span>119 <span style=''></span><span style='background: #AEF1AE'>            val nullCheck = fieldExprCode.isNull.code
</span>120 <span style=''></span><span style='background: #AEF1AE'>            val nonNullValue = fieldExprCode.value.code
</span>121 <span style=''></span><span style='background: #AEF1AE'>            (fieldName, (nullCheck, nonNullValue))
</span>122 <span style=''></span><span style='background: #AEF1AE'>        }</span><span style=''>
</span>123 <span style=''>      val newFieldsCode = </span><span style='background: #AEF1AE'>loop(existingFieldsCode, addOrReplaceFieldsCode)</span><span style=''>
</span>124 <span style=''>      val rowClass = </span><span style='background: #AEF1AE'>classOf[GenericInternalRow].getName</span><span style=''>
</span>125 <span style=''>      val rowValuesVar = </span><span style='background: #AEF1AE'>ctx.freshName(&quot;rowValues&quot;)</span><span style=''>
</span>126 <span style=''>      val populateRowValuesVar = </span><span style='background: #AEF1AE'>newFieldsCode.zipWithIndex.map {
</span>127 <span style=''></span><span style='background: #AEF1AE'>        case ((_, (nullCheck, nonNullValue)), i) =&gt;
</span>128 <span style=''></span><span style='background: #AEF1AE'>          s&quot;&quot;&quot;
</span>129 <span style=''></span><span style='background: #AEF1AE'>             |if ($nullCheck) {
</span>130 <span style=''></span><span style='background: #AEF1AE'>             | $rowValuesVar[$i] = null;
</span>131 <span style=''></span><span style='background: #AEF1AE'>             |} else {
</span>132 <span style=''></span><span style='background: #AEF1AE'>             | $rowValuesVar[$i] = $nonNullValue;
</span>133 <span style=''></span><span style='background: #AEF1AE'>             |}&quot;&quot;&quot;.stripMargin
</span>134 <span style=''></span><span style='background: #AEF1AE'>      }.mkString(&quot;\n|&quot;)</span><span style=''>
</span>135 <span style=''>
</span>136 <span style=''>      </span><span style='background: #AEF1AE'>s&quot;&quot;&quot;
</span>137 <span style=''></span><span style='background: #AEF1AE'>         |Object[] $rowValuesVar = new Object[${dataType.length}];
</span>138 <span style=''></span><span style='background: #AEF1AE'>         |
</span>139 <span style=''></span><span style='background: #AEF1AE'>         |${addOrReplaceFieldsGens.map(_.code).mkString(&quot;\n&quot;)}
</span>140 <span style=''></span><span style='background: #AEF1AE'>         |$populateRowValuesVar
</span>141 <span style=''></span><span style='background: #AEF1AE'>         |
</span>142 <span style=''></span><span style='background: #AEF1AE'>         |${ev.value} = new $rowClass($rowValuesVar);
</span>143 <span style=''></span><span style='background: #AEF1AE'>          &quot;&quot;&quot;.stripMargin</span><span style=''>
</span>144 <span style=''>    }
</span>145 <span style=''>
</span>146 <span style=''>    if (</span><span style='background: #AEF1AE'>nullable</span><span style=''>) </span><span style='background: #AEF1AE'>{
</span>147 <span style=''></span><span style='background: #AEF1AE'>      val nullSafeEval =
</span>148 <span style=''></span><span style='background: #AEF1AE'>        structGen.code + ctx.nullSafeExec(struct.nullable, structGen.isNull) {
</span>149 <span style=''></span><span style='background: #AEF1AE'>          s&quot;&quot;&quot;
</span>150 <span style=''></span><span style='background: #AEF1AE'>             |${ev.isNull} = false; // resultCode could change nullability.
</span>151 <span style=''></span><span style='background: #AEF1AE'>             |$resultCode
</span>152 <span style=''></span><span style='background: #AEF1AE'>             |&quot;&quot;&quot;.stripMargin
</span>153 <span style=''></span><span style='background: #AEF1AE'>        }
</span>154 <span style=''></span><span style='background: #AEF1AE'>
</span>155 <span style=''></span><span style='background: #AEF1AE'>      ev.copy(code =
</span>156 <span style=''></span><span style='background: #AEF1AE'>        code&quot;&quot;&quot;
</span>157 <span style=''></span><span style='background: #AEF1AE'>          boolean ${ev.isNull} = true;
</span>158 <span style=''></span><span style='background: #AEF1AE'>          ${CodeGenerator.javaType(dataType)} ${ev.value} = ${CodeGenerator.defaultValue(dataType)};
</span>159 <span style=''></span><span style='background: #AEF1AE'>          $nullSafeEval
</span>160 <span style=''></span><span style='background: #AEF1AE'>          &quot;&quot;&quot;)
</span>161 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''> else {
</span>162 <span style=''>      </span><span style='background: #F0ADAD'>ev.copy(code =
</span>163 <span style=''></span><span style='background: #F0ADAD'>        code&quot;&quot;&quot;
</span>164 <span style=''></span><span style='background: #F0ADAD'>          ${structGen.code}
</span>165 <span style=''></span><span style='background: #F0ADAD'>          ${CodeGenerator.javaType(dataType)} ${ev.value} = ${CodeGenerator.defaultValue(dataType)};
</span>166 <span style=''></span><span style='background: #F0ADAD'>          $resultCode
</span>167 <span style=''></span><span style='background: #F0ADAD'>          &quot;&quot;&quot;, isNull = FalseLiteral)</span><span style=''>
</span>168 <span style=''>    }
</span>169 <span style=''>  }
</span>170 <span style=''>
</span>171 <span style=''>  override def prettyName: String = </span><span style='background: #AEF1AE'>&quot;add_fields&quot;</span><span style=''>
</span>172 <span style=''>
</span>173 <span style=''>  private type FieldName = String
</span>174 <span style=''>
</span>175 <span style=''>  /**
</span>176 <span style=''>   * Recursively loop through addOrReplaceFields, adding or replacing fields by FieldName.
</span>177 <span style=''>   */
</span>178 <span style=''>  @scala.annotation.tailrec
</span>179 <span style=''>  private def loop[V](existingFields: Seq[(String, V)],
</span>180 <span style=''>                      addOrReplaceFields: Seq[(String, V)]): Seq[(String, V)] = {
</span>181 <span style=''>    if (</span><span style='background: #AEF1AE'>addOrReplaceFields.nonEmpty</span><span style=''>) </span><span style='background: #AEF1AE'>{
</span>182 <span style=''></span><span style='background: #AEF1AE'>      val existingFieldNames = existingFields.map(_._1)
</span>183 <span style=''></span><span style='background: #AEF1AE'>      val newField@(newFieldName, _) = addOrReplaceFields.head
</span>184 <span style=''></span><span style='background: #AEF1AE'>
</span>185 <span style=''></span><span style='background: #AEF1AE'>      if (existingFieldNames.contains(newFieldName)) {
</span>186 <span style=''></span><span style='background: #AEF1AE'>        loop(
</span>187 <span style=''></span><span style='background: #AEF1AE'>          existingFields.map {
</span>188 <span style=''></span><span style='background: #AEF1AE'>            case (fieldName, _) if fieldName == newFieldName =&gt; newField
</span>189 <span style=''></span><span style='background: #AEF1AE'>            case x =&gt; x
</span>190 <span style=''></span><span style='background: #AEF1AE'>          },
</span>191 <span style=''></span><span style='background: #AEF1AE'>          addOrReplaceFields.drop(1))
</span>192 <span style=''></span><span style='background: #AEF1AE'>      } else {
</span>193 <span style=''></span><span style='background: #AEF1AE'>        </span><span style='background: #F0ADAD'>loop(
</span>194 <span style=''></span><span style='background: #F0ADAD'>          existingFields :+ newField,
</span>195 <span style=''></span><span style='background: #F0ADAD'>          addOrReplaceFields.drop(1))</span><span style='background: #AEF1AE'>
</span>196 <span style=''></span><span style='background: #AEF1AE'>      }
</span>197 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''> else {
</span>198 <span style=''>      </span><span style='background: #AEF1AE'>existingFields</span><span style=''>
</span>199 <span style=''>    }
</span>200 <span style=''>  }
</span>201 <span style=''>
</span>202 <span style=''>  protected def withNewChildrenInternal(newChildren: IndexedSeq[Expression]): Expression = </span><span style='background: #AEF1AE'>copy(children = newChildren)</span><span style=''>
</span>203 <span style=''>}
</span></pre>
          </div>
          <div class="tab-pane" id="statementlist">
            <table cellspacing="0" cellpadding="0" class="table statementlist">
      <tr>
        <th>Line</th>
        <th>Stmt Id</th>
        <th>Pos</th>
        <th>Tree</th>
        <th>Symbol</th>
        <th>Tests</th>
        <th>Code</th>
      </tr><tr>
        <td>
          11
        </td>
        <td>
          5726
        </td>
        <td>
          387
          -
          402
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.utils.AddFields.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.apply(exps)
        </td>
      </tr><tr>
        <td>
          49
        </td>
        <td>
          5727
        </td>
        <td>
          2056
          -
          2071
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.nullable
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.struct.nullable
        </td>
      </tr><tr>
        <td>
          64
        </td>
        <td>
          5735
        </td>
        <td>
          2592
          -
          2592
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          64
        </td>
        <td>
          5728
        </td>
        <td>
          2596
          -
          2618
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.children.size.%(2).==(0)
        </td>
      </tr><tr>
        <td>
          64
        </td>
        <td>
          5736
        </td>
        <td>
          2592
          -
          2592
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          65
        </td>
        <td>
          5732
        </td>
        <td>
          2668
          -
          2718
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot; expects an odd number of arguments.&quot;).s(AddFields.this.prettyName)
        </td>
      </tr><tr>
        <td>
          65
        </td>
        <td>
          5729
        </td>
        <td>
          2670
          -
          2671
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          65
        </td>
        <td>
          5731
        </td>
        <td>
          2671
          -
          2681
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.utils.AddFields.prettyName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          AddFields.this.prettyName
        </td>
      </tr><tr>
        <td>
          65
        </td>
        <td>
          5734
        </td>
        <td>
          2628
          -
          2719
        </td>
        <td>
          Return
        </td>
        <td>
          com.sparkutils.quality.utils.AddFields.checkInputDataTypes
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          return org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply(scala.StringContext.apply(&quot;&quot;, &quot; expects an odd number of arguments.&quot;).s(AddFields.this.prettyName))
        </td>
      </tr><tr>
        <td>
          65
        </td>
        <td>
          5730
        </td>
        <td>
          2681
          -
          2718
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; expects an odd number of arguments.&quot;
        </td>
      </tr><tr>
        <td>
          65
        </td>
        <td>
          5733
        </td>
        <td>
          2635
          -
          2719
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply(scala.StringContext.apply(&quot;&quot;, &quot; expects an odd number of arguments.&quot;).s(AddFields.this.prettyName))
        </td>
      </tr><tr>
        <td>
          68
        </td>
        <td>
          5737
        </td>
        <td>
          2746
          -
          2770
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DataType.typeName
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.struct.dataType.typeName
        </td>
      </tr><tr>
        <td>
          69
        </td>
        <td>
          5738
        </td>
        <td>
          2811
          -
          2814
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.Nil
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.collection.immutable.Nil
        </td>
      </tr><tr>
        <td>
          69
        </td>
        <td>
          5739
        </td>
        <td>
          2800
          -
          2824
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DataType.typeName
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructType.apply(scala.collection.immutable.Nil).typeName
        </td>
      </tr><tr>
        <td>
          70
        </td>
        <td>
          5744
        </td>
        <td>
          2829
          -
          2829
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          70
        </td>
        <td>
          5740
        </td>
        <td>
          2833
          -
          2863
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.!=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          typeName.!=(expectedStructType)
        </td>
      </tr><tr>
        <td>
          70
        </td>
        <td>
          5745
        </td>
        <td>
          2829
          -
          2829
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          71
        </td>
        <td>
          5743
        </td>
        <td>
          2873
          -
          3006
        </td>
        <td>
          Return
        </td>
        <td>
          com.sparkutils.quality.utils.AddFields.checkInputDataTypes
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          return org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply(scala.StringContext.apply(&quot;Only &quot;, &quot; is allowed to appear at first position, got: &quot;, &quot;.&quot;).s(expectedStructType, typeName))
        </td>
      </tr><tr>
        <td>
          71
        </td>
        <td>
          5742
        </td>
        <td>
          2880
          -
          3006
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply(scala.StringContext.apply(&quot;Only &quot;, &quot; is allowed to appear at first position, got: &quot;, &quot;.&quot;).s(expectedStructType, typeName))
        </td>
      </tr><tr>
        <td>
          72
        </td>
        <td>
          5741
        </td>
        <td>
          2922
          -
          3005
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;Only &quot;, &quot; is allowed to appear at first position, got: &quot;, &quot;.&quot;).s(expectedStructType, typeName)
        </td>
      </tr><tr>
        <td>
          75
        </td>
        <td>
          5750
        </td>
        <td>
          3050
          -
          3114
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.LinearSeqOptimized.exists
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.nameExprs.exists(((e: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; e.foldable.&amp;&amp;(e.dataType.==(org.apache.spark.sql.types.StringType)).unary_!))
        </td>
      </tr><tr>
        <td>
          75
        </td>
        <td>
          5759
        </td>
        <td>
          3018
          -
          3018
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          75
        </td>
        <td>
          5747
        </td>
        <td>
          3102
          -
          3112
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StringType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StringType
        </td>
      </tr><tr>
        <td>
          75
        </td>
        <td>
          5746
        </td>
        <td>
          3041
          -
          3045
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          null
        </td>
      </tr><tr>
        <td>
          75
        </td>
        <td>
          5749
        </td>
        <td>
          3072
          -
          3113
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          e.foldable.&amp;&amp;(e.dataType.==(org.apache.spark.sql.types.StringType)).unary_!
        </td>
      </tr><tr>
        <td>
          75
        </td>
        <td>
          5758
        </td>
        <td>
          3018
          -
          3018
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          75
        </td>
        <td>
          5751
        </td>
        <td>
          3022
          -
          3114
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.||
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.nameExprs.contains[org.apache.spark.sql.catalyst.expressions.Expression](null).||(AddFields.this.nameExprs.exists(((e: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; e.foldable.&amp;&amp;(e.dataType.==(org.apache.spark.sql.types.StringType)).unary_!)))
        </td>
      </tr><tr>
        <td>
          75
        </td>
        <td>
          5748
        </td>
        <td>
          3088
          -
          3112
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          e.dataType.==(org.apache.spark.sql.types.StringType)
        </td>
      </tr><tr>
        <td>
          76
        </td>
        <td>
          5756
        </td>
        <td>
          3131
          -
          3279
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply(scala.StringContext.apply(&quot;Only non-null foldable &quot;, &quot; expressions are allowed to appear at even position.&quot;).s(org.apache.spark.sql.types.StringType.catalogString))
        </td>
      </tr><tr>
        <td>
          76
        </td>
        <td>
          5757
        </td>
        <td>
          3124
          -
          3279
        </td>
        <td>
          Return
        </td>
        <td>
          com.sparkutils.quality.utils.AddFields.checkInputDataTypes
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          return org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply(scala.StringContext.apply(&quot;Only non-null foldable &quot;, &quot; expressions are allowed to appear at even position.&quot;).s(org.apache.spark.sql.types.StringType.catalogString))
        </td>
      </tr><tr>
        <td>
          77
        </td>
        <td>
          5753
        </td>
        <td>
          3225
          -
          3278
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; expressions are allowed to appear at even position.&quot;
        </td>
      </tr><tr>
        <td>
          77
        </td>
        <td>
          5755
        </td>
        <td>
          3173
          -
          3278
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;Only non-null foldable &quot;, &quot; expressions are allowed to appear at even position.&quot;).s(org.apache.spark.sql.types.StringType.catalogString)
        </td>
      </tr><tr>
        <td>
          77
        </td>
        <td>
          5752
        </td>
        <td>
          3175
          -
          3199
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;Only non-null foldable &quot;
        </td>
      </tr><tr>
        <td>
          77
        </td>
        <td>
          5754
        </td>
        <td>
          3200
          -
          3224
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DataType.catalogString
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.StringType.catalogString
        </td>
      </tr><tr>
        <td>
          80
        </td>
        <td>
          5765
        </td>
        <td>
          3291
          -
          3291
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          80
        </td>
        <td>
          5764
        </td>
        <td>
          3291
          -
          3291
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          80
        </td>
        <td>
          5760
        </td>
        <td>
          3295
          -
          3318
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.LinearSeqOptimized.contains
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.valExprs.contains[org.apache.spark.sql.catalyst.expressions.Expression](null)
        </td>
      </tr><tr>
        <td>
          81
        </td>
        <td>
          5762
        </td>
        <td>
          3335
          -
          3467
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply(scala.StringContext.apply(&quot;Only non-null expressions are allowed to appear at odd positions after first position.&quot;).s())
        </td>
      </tr><tr>
        <td>
          81
        </td>
        <td>
          5763
        </td>
        <td>
          3328
          -
          3467
        </td>
        <td>
          Return
        </td>
        <td>
          com.sparkutils.quality.utils.AddFields.checkInputDataTypes
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          return org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply(scala.StringContext.apply(&quot;Only non-null expressions are allowed to appear at odd positions after first position.&quot;).s())
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          5761
        </td>
        <td>
          3377
          -
          3466
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;Only non-null expressions are allowed to appear at odd positions after first position.&quot;).s()
        </td>
      </tr><tr>
        <td>
          85
        </td>
        <td>
          5766
        </td>
        <td>
          3479
          -
          3511
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckSuccess
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckSuccess
        </td>
      </tr><tr>
        <td>
          89
        </td>
        <td>
          5767
        </td>
        <td>
          3588
          -
          3606
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.eval
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.struct.eval(input)
        </td>
      </tr><tr>
        <td>
          90
        </td>
        <td>
          5768
        </td>
        <td>
          3615
          -
          3634
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          structValue.==(null)
        </td>
      </tr><tr>
        <td>
          91
        </td>
        <td>
          5770
        </td>
        <td>
          3644
          -
          3648
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          null
        </td>
      </tr><tr>
        <td>
          91
        </td>
        <td>
          5769
        </td>
        <td>
          3644
          -
          3648
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          null
        </td>
      </tr><tr>
        <td>
          92
        </td>
        <td>
          5784
        </td>
        <td>
          3660
          -
          4066
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  val existingValues: Seq[(AddFields.this.FieldName, Any)] = scala.Predef.refArrayOps[String](AddFields.this.ogStructType.fieldNames).zip[String, Any, Seq[(AddFields.this.FieldName, Any)]](structValue.asInstanceOf[org.apache.spark.sql.catalyst.InternalRow].toSeq(AddFields.this.ogStructType))(scala.this.Array.fallbackCanBuildFrom[(String, Any)](Predef.this.DummyImplicit.dummyImplicit));
  val addOrReplaceValues: Seq[(AddFields.this.FieldName, Any)] = AddFields.this.pairs.map[(String, Any), Seq[(AddFields.this.FieldName, Any)]](((x0$1: (String, org.apache.spark.sql.catalyst.expressions.Expression)) =&gt; x0$1 match {
    case (_1: String, _2: org.apache.spark.sql.catalyst.expressions.Expression)(String, org.apache.spark.sql.catalyst.expressions.Expression)((fieldName @ _), (expression @ _)) =&gt; scala.Tuple2.apply[String, Any](fieldName, expression.eval(input))
  }))(immutable.this.List.canBuildFrom[(String, Any)]);
  val newValues: Seq[Any] = AddFields.this.loop[Any](existingValues, addOrReplaceValues).map[Any, Seq[Any]](((x$4: (String, Any)) =&gt; x$4._2))(collection.this.Seq.canBuildFrom[Any]);
  org.apache.spark.sql.catalyst.InternalRow.fromSeq(newValues)
}
        </td>
      </tr><tr>
        <td>
          94
        </td>
        <td>
          5774
        </td>
        <td>
          3747
          -
          3747
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.FallbackArrayBuilding.fallbackCanBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.this.Array.fallbackCanBuildFrom[(String, Any)](Predef.this.DummyImplicit.dummyImplicit)
        </td>
      </tr><tr>
        <td>
          94
        </td>
        <td>
          5771
        </td>
        <td>
          3720
          -
          3743
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructType.fieldNames
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.ogStructType.fieldNames
        </td>
      </tr><tr>
        <td>
          94
        </td>
        <td>
          5773
        </td>
        <td>
          3747
          -
          3747
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Predef.DummyImplicit.dummyImplicit
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          Predef.this.DummyImplicit.dummyImplicit
        </td>
      </tr><tr>
        <td>
          94
        </td>
        <td>
          5772
        </td>
        <td>
          3748
          -
          3805
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.InternalRow.toSeq
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          structValue.asInstanceOf[org.apache.spark.sql.catalyst.InternalRow].toSeq(AddFields.this.ogStructType)
        </td>
      </tr><tr>
        <td>
          94
        </td>
        <td>
          5775
        </td>
        <td>
          3720
          -
          3806
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.IndexedSeqOptimized.zip
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.refArrayOps[String](AddFields.this.ogStructType.fieldNames).zip[String, Any, Seq[(AddFields.this.FieldName, Any)]](structValue.asInstanceOf[org.apache.spark.sql.catalyst.InternalRow].toSeq(AddFields.this.ogStructType))(scala.this.Array.fallbackCanBuildFrom[(String, Any)](Predef.this.DummyImplicit.dummyImplicit))
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          5777
        </td>
        <td>
          3913
          -
          3948
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[String, Any](fieldName, expression.eval(input))
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          5779
        </td>
        <td>
          3869
          -
          3950
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.immutable.List.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.pairs.map[(String, Any), Seq[(AddFields.this.FieldName, Any)]](((x0$1: (String, org.apache.spark.sql.catalyst.expressions.Expression)) =&gt; x0$1 match {
  case (_1: String, _2: org.apache.spark.sql.catalyst.expressions.Expression)(String, org.apache.spark.sql.catalyst.expressions.Expression)((fieldName @ _), (expression @ _)) =&gt; scala.Tuple2.apply[String, Any](fieldName, expression.eval(input))
}))(immutable.this.List.canBuildFrom[(String, Any)])
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          5776
        </td>
        <td>
          3925
          -
          3947
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.eval
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          expression.eval(input)
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          5778
        </td>
        <td>
          3879
          -
          3879
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.List.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          immutable.this.List.canBuildFrom[(String, Any)]
        </td>
      </tr><tr>
        <td>
          97
        </td>
        <td>
          5780
        </td>
        <td>
          4018
          -
          4022
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$4._2
        </td>
      </tr><tr>
        <td>
          97
        </td>
        <td>
          5782
        </td>
        <td>
          3973
          -
          4023
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.loop[Any](existingValues, addOrReplaceValues).map[Any, Seq[Any]](((x$4: (String, Any)) =&gt; x$4._2))(collection.this.Seq.canBuildFrom[Any])
        </td>
      </tr><tr>
        <td>
          97
        </td>
        <td>
          5781
        </td>
        <td>
          4017
          -
          4017
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[Any]
        </td>
      </tr><tr>
        <td>
          98
        </td>
        <td>
          5783
        </td>
        <td>
          4030
          -
          4060
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.InternalRow.fromSeq
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.InternalRow.fromSeq(newValues)
        </td>
      </tr><tr>
        <td>
          103
        </td>
        <td>
          5785
        </td>
        <td>
          4166
          -
          4185
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.genCode
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.struct.genCode(ctx)
        </td>
      </tr><tr>
        <td>
          104
        </td>
        <td>
          5786
        </td>
        <td>
          4232
          -
          4246
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.genCode
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$5.genCode(ctx)
        </td>
      </tr><tr>
        <td>
          104
        </td>
        <td>
          5788
        </td>
        <td>
          4219
          -
          4247
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.immutable.List.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.valExprs.map[org.apache.spark.sql.catalyst.expressions.codegen.ExprCode, List[org.apache.spark.sql.catalyst.expressions.codegen.ExprCode]](((x$5: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; x$5.genCode(ctx)))(immutable.this.List.canBuildFrom[org.apache.spark.sql.catalyst.expressions.codegen.ExprCode])
        </td>
      </tr><tr>
        <td>
          104
        </td>
        <td>
          5787
        </td>
        <td>
          4231
          -
          4231
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.List.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          immutable.this.List.canBuildFrom[org.apache.spark.sql.catalyst.expressions.codegen.ExprCode]
        </td>
      </tr><tr>
        <td>
          106
        </td>
        <td>
          5789
        </td>
        <td>
          4301
          -
          4316
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.value
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          structGen.value
        </td>
      </tr><tr>
        <td>
          110
        </td>
        <td>
          5792
        </td>
        <td>
          4464
          -
          4496
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.IndexedSeqOptimized.zipWithIndex
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](AddFields.this.ogStructType.fields).zipWithIndex[org.apache.spark.sql.types.StructField, Array[(org.apache.spark.sql.types.StructField, Int)]](scala.this.Array.canBuildFrom[(org.apache.spark.sql.types.StructField, Int)]((ClassTag.apply[(org.apache.spark.sql.types.StructField, Int)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(org.apache.spark.sql.types.StructField, Int)])))
        </td>
      </tr><tr>
        <td>
          110
        </td>
        <td>
          5801
        </td>
        <td>
          4501
          -
          4501
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Predef.DummyImplicit.dummyImplicit
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          Predef.this.DummyImplicit.dummyImplicit
        </td>
      </tr><tr>
        <td>
          110
        </td>
        <td>
          5791
        </td>
        <td>
          4484
          -
          4484
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.this.Array.canBuildFrom[(org.apache.spark.sql.types.StructField, Int)]((ClassTag.apply[(org.apache.spark.sql.types.StructField, Int)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(org.apache.spark.sql.types.StructField, Int)]))
        </td>
      </tr><tr>
        <td>
          110
        </td>
        <td>
          5803
        </td>
        <td>
          4464
          -
          4759
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.refArrayOps[(org.apache.spark.sql.types.StructField, Int)](scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](AddFields.this.ogStructType.fields).zipWithIndex[org.apache.spark.sql.types.StructField, Array[(org.apache.spark.sql.types.StructField, Int)]](scala.this.Array.canBuildFrom[(org.apache.spark.sql.types.StructField, Int)]((ClassTag.apply[(org.apache.spark.sql.types.StructField, Int)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(org.apache.spark.sql.types.StructField, Int)])))).map[(String, (String, String)), Seq[(AddFields.this.FieldName, (NullCheck, NonNullValue))]](((x0$1: (org.apache.spark.sql.types.StructField, Int)) =&gt; x0$1 match {
  case (_1: org.apache.spark.sql.types.StructField, _2: Int)(org.apache.spark.sql.types.StructField, Int)((structField @ _), (i @ _)) =&gt; {
    val nullCheck: String = scala.StringContext.apply(&quot;&quot;, &quot;.isNullAt(&quot;, &quot;)&quot;).s(structVar, i);
    val nonNullValue: String = org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.getValue(codegen.this.ExprValue.exprValueToString(structVar), structField.dataType, i.toString());
    scala.Tuple2.apply[String, (String, String)](structField.name, scala.Tuple2.apply[String, String](nullCheck, nonNullValue))
  }
}))(scala.this.Array.fallbackCanBuildFrom[(String, (String, String))](Predef.this.DummyImplicit.dummyImplicit))
        </td>
      </tr><tr>
        <td>
          110
        </td>
        <td>
          5802
        </td>
        <td>
          4501
          -
          4501
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.FallbackArrayBuilding.fallbackCanBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.this.Array.fallbackCanBuildFrom[(String, (String, String))](Predef.this.DummyImplicit.dummyImplicit)
        </td>
      </tr><tr>
        <td>
          110
        </td>
        <td>
          5790
        </td>
        <td>
          4464
          -
          4483
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructType.fields
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.ogStructType.fields
        </td>
      </tr><tr>
        <td>
          112
        </td>
        <td>
          5793
        </td>
        <td>
          4566
          -
          4592
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;&quot;, &quot;.isNullAt(&quot;, &quot;)&quot;).s(structVar, i)
        </td>
      </tr><tr>
        <td>
          113
        </td>
        <td>
          5795
        </td>
        <td>
          4658
          -
          4678
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          structField.dataType
        </td>
      </tr><tr>
        <td>
          113
        </td>
        <td>
          5797
        </td>
        <td>
          4624
          -
          4691
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.getValue
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.getValue(codegen.this.ExprValue.exprValueToString(structVar), structField.dataType, i.toString())
        </td>
      </tr><tr>
        <td>
          113
        </td>
        <td>
          5794
        </td>
        <td>
          4647
          -
          4656
        </td>
        <td>
          ApplyImplicitView
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprValue.exprValueToString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          codegen.this.ExprValue.exprValueToString(structVar)
        </td>
      </tr><tr>
        <td>
          113
        </td>
        <td>
          5796
        </td>
        <td>
          4680
          -
          4690
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.toString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.toString()
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          5798
        </td>
        <td>
          4705
          -
          4721
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.name
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          structField.name
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          5800
        </td>
        <td>
          4704
          -
          4749
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[String, (String, String)](structField.name, scala.Tuple2.apply[String, String](nullCheck, nonNullValue))
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          5799
        </td>
        <td>
          4723
          -
          4748
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[String, String](nullCheck, nonNullValue)
        </td>
      </tr><tr>
        <td>
          117
        </td>
        <td>
          5810
        </td>
        <td>
          4848
          -
          5108
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.immutable.List.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.fieldNames.zip[String, org.apache.spark.sql.catalyst.expressions.codegen.ExprCode, List[(String, org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)]](addOrReplaceFieldsGens)(immutable.this.List.canBuildFrom[(String, org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)]).map[(String, (String, String)), Seq[(AddFields.this.FieldName, (NullCheck, NonNullValue))]](((x0$2: (String, org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)) =&gt; x0$2 match {
  case (_1: String, _2: org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)(String, org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)((fieldName @ _), (fieldExprCode @ _)) =&gt; {
    val nullCheck: String = fieldExprCode.isNull.code;
    val nonNullValue: String = fieldExprCode.value.code;
    scala.Tuple2.apply[String, (String, String)](fieldName, scala.Tuple2.apply[String, String](nullCheck, nonNullValue))
  }
}))(immutable.this.List.canBuildFrom[(String, (String, String))])
        </td>
      </tr><tr>
        <td>
          117
        </td>
        <td>
          5804
        </td>
        <td>
          4862
          -
          4862
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.List.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          immutable.this.List.canBuildFrom[(String, org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)]
        </td>
      </tr><tr>
        <td>
          117
        </td>
        <td>
          5809
        </td>
        <td>
          4891
          -
          4891
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.List.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          immutable.this.List.canBuildFrom[(String, (String, String))]
        </td>
      </tr><tr>
        <td>
          119
        </td>
        <td>
          5805
        </td>
        <td>
          4966
          -
          4991
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.JavaCode.code
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          fieldExprCode.isNull.code
        </td>
      </tr><tr>
        <td>
          120
        </td>
        <td>
          5806
        </td>
        <td>
          5023
          -
          5047
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.JavaCode.code
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          fieldExprCode.value.code
        </td>
      </tr><tr>
        <td>
          121
        </td>
        <td>
          5807
        </td>
        <td>
          5072
          -
          5097
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[String, String](nullCheck, nonNullValue)
        </td>
      </tr><tr>
        <td>
          121
        </td>
        <td>
          5808
        </td>
        <td>
          5060
          -
          5098
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[String, (String, String)](fieldName, scala.Tuple2.apply[String, String](nullCheck, nonNullValue))
        </td>
      </tr><tr>
        <td>
          123
        </td>
        <td>
          5811
        </td>
        <td>
          5135
          -
          5183
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.utils.AddFields.loop
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.loop[(NullCheck, NonNullValue)](existingFieldsCode, addOrReplaceFieldsCode)
        </td>
      </tr><tr>
        <td>
          124
        </td>
        <td>
          5812
        </td>
        <td>
          5205
          -
          5240
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Class.getName
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          classOf[org.apache.spark.sql.catalyst.expressions.GenericInternalRow].getName()
        </td>
      </tr><tr>
        <td>
          125
        </td>
        <td>
          5813
        </td>
        <td>
          5266
          -
          5292
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.freshName
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ctx.freshName(&quot;rowValues&quot;)
        </td>
      </tr><tr>
        <td>
          134
        </td>
        <td>
          5814
        </td>
        <td>
          5326
          -
          5626
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.mkString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          newFieldsCode.zipWithIndex[(String, (NullCheck, NonNullValue)), Seq[((String, (NullCheck, NonNullValue)), Int)]](collection.this.Seq.canBuildFrom[((String, (NullCheck, NonNullValue)), Int)]).map[String, Seq[String]](((x0$3: ((String, (NullCheck, NonNullValue)), Int)) =&gt; x0$3 match {
  case (_1: (String, (NullCheck, NonNullValue)), _2: Int)((String, (NullCheck, NonNullValue)), Int)((_1: String, _2: (NullCheck, NonNullValue))(String, (NullCheck, NonNullValue))(_, (_1: NullCheck, _2: NonNullValue)(NullCheck, NonNullValue)((nullCheck @ _), (nonNullValue @ _))), (i @ _)) =&gt; scala.Predef.augmentString(scala.StringContext.apply(&quot;\n             |if (&quot;, &quot;) {\n             | &quot;, &quot;[&quot;, &quot;] = null;\n             |} else {\n             | &quot;, &quot;[&quot;, &quot;] = &quot;, &quot;;\n             |}&quot;).s(nullCheck, rowValuesVar, i, rowValuesVar, i, nonNullValue)).stripMargin
}))(collection.this.Seq.canBuildFrom[String]).mkString(&quot;\n|&quot;)
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          5815
        </td>
        <td>
          5638
          -
          5659
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;\n         |Object[] &quot;
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          5826
        </td>
        <td>
          5634
          -
          5890
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;\n         |Object[] &quot;, &quot; = new Object[&quot;, &quot;];\n         |\n         |&quot;, &quot;\n         |&quot;, &quot;\n         |\n         |&quot;, &quot; = new &quot;, &quot;(&quot;, &quot;);\n          &quot;).s(rowValuesVar, AddFields.this.dataType.length, addOrReplaceFieldsGens.map[org.apache.spark.sql.catalyst.expressions.codegen.Block, List[org.apache.spark.sql.catalyst.expressions.codegen.Block]](((x$6: org.apache.spark.sql.catalyst.expressions.codegen.ExprCode) =&gt; x$6.code))(immutable.this.List.canBuildFrom[org.apache.spark.sql.catalyst.expressions.codegen.Block]).mkString(&quot;\n&quot;), populateRowValuesVar, ev.value, rowClass, rowValuesVar)
        </td>
      </tr><tr>
        <td>
          137
        </td>
        <td>
          5816
        </td>
        <td>
          5671
          -
          5686
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot; = new Object[&quot;
        </td>
      </tr><tr>
        <td>
          137
        </td>
        <td>
          5823
        </td>
        <td>
          5687
          -
          5702
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructType.length
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.dataType.length
        </td>
      </tr><tr>
        <td>
          137
        </td>
        <td>
          5817
        </td>
        <td>
          5703
          -
          5728
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;];\n         |\n         |&quot;
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          5824
        </td>
        <td>
          5729
          -
          5778
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.mkString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          addOrReplaceFieldsGens.map[org.apache.spark.sql.catalyst.expressions.codegen.Block, List[org.apache.spark.sql.catalyst.expressions.codegen.Block]](((x$6: org.apache.spark.sql.catalyst.expressions.codegen.ExprCode) =&gt; x$6.code))(immutable.this.List.canBuildFrom[org.apache.spark.sql.catalyst.expressions.codegen.Block]).mkString(&quot;\n&quot;)
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          5818
        </td>
        <td>
          5779
          -
          5791
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;\n         |&quot;
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          5819
        </td>
        <td>
          5811
          -
          5834
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;\n         |\n         |&quot;
        </td>
      </tr><tr>
        <td>
          142
        </td>
        <td>
          5825
        </td>
        <td>
          5835
          -
          5843
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.value
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.value
        </td>
      </tr><tr>
        <td>
          142
        </td>
        <td>
          5822
        </td>
        <td>
          5874
          -
          5890
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;);\n          &quot;
        </td>
      </tr><tr>
        <td>
          142
        </td>
        <td>
          5821
        </td>
        <td>
          5860
          -
          5862
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;(&quot;
        </td>
      </tr><tr>
        <td>
          142
        </td>
        <td>
          5820
        </td>
        <td>
          5844
          -
          5852
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot; = new &quot;
        </td>
      </tr><tr>
        <td>
          143
        </td>
        <td>
          5827
        </td>
        <td>
          5634
          -
          5902
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.StringLike.stripMargin
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.augmentString(scala.StringContext.apply(&quot;\n         |Object[] &quot;, &quot; = new Object[&quot;, &quot;];\n         |\n         |&quot;, &quot;\n         |&quot;, &quot;\n         |\n         |&quot;, &quot; = new &quot;, &quot;(&quot;, &quot;);\n          &quot;).s(rowValuesVar, AddFields.this.dataType.length, addOrReplaceFieldsGens.map[org.apache.spark.sql.catalyst.expressions.codegen.Block, List[org.apache.spark.sql.catalyst.expressions.codegen.Block]](((x$6: org.apache.spark.sql.catalyst.expressions.codegen.ExprCode) =&gt; x$6.code))(immutable.this.List.canBuildFrom[org.apache.spark.sql.catalyst.expressions.codegen.Block]).mkString(&quot;\n&quot;), populateRowValuesVar, ev.value, rowClass, rowValuesVar)).stripMargin
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          5828
        </td>
        <td>
          5918
          -
          5926
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.utils.AddFields.nullable
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.nullable
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          5851
        </td>
        <td>
          5928
          -
          6413
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  val nullSafeEval: String = scala.Predef.any2stringadd[org.apache.spark.sql.catalyst.expressions.codegen.Block](structGen.code).+(ctx.nullSafeExec(AddFields.this.struct.nullable, codegen.this.ExprValue.exprValueToString(structGen.isNull))(scala.Predef.augmentString(scala.StringContext.apply(&quot;\n             |&quot;, &quot; = false; // resultCode could change nullability.\n             |&quot;, &quot;\n             |&quot;).s(ev.isNull, resultCode)).stripMargin));
  ev.copy(org.apache.spark.sql.catalyst.expressions.codegen.Block.BlockHelper(scala.StringContext.apply(&quot;\n          boolean &quot;, &quot; = true;\n          &quot;, &quot; &quot;, &quot; = &quot;, &quot;;\n          &quot;, &quot;\n          &quot;)).code(ev.isNull, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.javaType(AddFields.this.dataType), ev.value, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue(AddFields.this.dataType, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue$default$2), nullSafeEval), ev.copy$default$2, ev.copy$default$3)
}
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          5831
        </td>
        <td>
          6014
          -
          6030
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.isNull
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          structGen.isNull
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          5840
        </td>
        <td>
          5963
          -
          6190
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Predef.any2stringadd.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.any2stringadd[org.apache.spark.sql.catalyst.expressions.codegen.Block](structGen.code).+(ctx.nullSafeExec(AddFields.this.struct.nullable, codegen.this.ExprValue.exprValueToString(structGen.isNull))(scala.Predef.augmentString(scala.StringContext.apply(&quot;\n             |&quot;, &quot; = false; // resultCode could change nullability.\n             |&quot;, &quot;\n             |&quot;).s(ev.isNull, resultCode)).stripMargin))
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          5839
        </td>
        <td>
          5980
          -
          6190
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.nullSafeExec
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ctx.nullSafeExec(AddFields.this.struct.nullable, codegen.this.ExprValue.exprValueToString(structGen.isNull))(scala.Predef.augmentString(scala.StringContext.apply(&quot;\n             |&quot;, &quot; = false; // resultCode could change nullability.\n             |&quot;, &quot;\n             |&quot;).s(ev.isNull, resultCode)).stripMargin)
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          5830
        </td>
        <td>
          5997
          -
          6012
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.nullable
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.struct.nullable
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          5829
        </td>
        <td>
          5963
          -
          5977
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.code
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          structGen.code
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          5832
        </td>
        <td>
          6014
          -
          6030
        </td>
        <td>
          ApplyImplicitView
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprValue.exprValueToString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          codegen.this.ExprValue.exprValueToString(structGen.isNull)
        </td>
      </tr><tr>
        <td>
          149
        </td>
        <td>
          5837
        </td>
        <td>
          6044
          -
          6168
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;\n             |&quot;, &quot; = false; // resultCode could change nullability.\n             |&quot;, &quot;\n             |&quot;).s(ev.isNull, resultCode)
        </td>
      </tr><tr>
        <td>
          149
        </td>
        <td>
          5833
        </td>
        <td>
          6048
          -
          6064
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;\n             |&quot;
        </td>
      </tr><tr>
        <td>
          150
        </td>
        <td>
          5834
        </td>
        <td>
          6075
          -
          6140
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot; = false; // resultCode could change nullability.\n             |&quot;
        </td>
      </tr><tr>
        <td>
          150
        </td>
        <td>
          5836
        </td>
        <td>
          6065
          -
          6074
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.isNull
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.isNull
        </td>
      </tr><tr>
        <td>
          151
        </td>
        <td>
          5835
        </td>
        <td>
          6150
          -
          6168
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;\n             |&quot;
        </td>
      </tr><tr>
        <td>
          152
        </td>
        <td>
          5838
        </td>
        <td>
          6044
          -
          6180
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.StringLike.stripMargin
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.augmentString(scala.StringContext.apply(&quot;\n             |&quot;, &quot; = false; // resultCode could change nullability.\n             |&quot;, &quot;\n             |&quot;).s(ev.isNull, resultCode)).stripMargin
        </td>
      </tr><tr>
        <td>
          155
        </td>
        <td>
          5849
        </td>
        <td>
          6201
          -
          6201
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.copy$default$3
        </td>
      </tr><tr>
        <td>
          155
        </td>
        <td>
          5848
        </td>
        <td>
          6201
          -
          6201
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.copy$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.copy$default$2
        </td>
      </tr><tr>
        <td>
          155
        </td>
        <td>
          5850
        </td>
        <td>
          6198
          -
          6407
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.copy(org.apache.spark.sql.catalyst.expressions.codegen.Block.BlockHelper(scala.StringContext.apply(&quot;\n          boolean &quot;, &quot; = true;\n          &quot;, &quot; &quot;, &quot; = &quot;, &quot;;\n          &quot;, &quot;\n          &quot;)).code(ev.isNull, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.javaType(AddFields.this.dataType), ev.value, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue(AddFields.this.dataType, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue$default$2), nullSafeEval), ev.copy$default$2, ev.copy$default$3)
        </td>
      </tr><tr>
        <td>
          156
        </td>
        <td>
          5847
        </td>
        <td>
          6221
          -
          6406
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.Block.BlockHelper.code
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.codegen.Block.BlockHelper(scala.StringContext.apply(&quot;\n          boolean &quot;, &quot; = true;\n          &quot;, &quot; &quot;, &quot; = &quot;, &quot;;\n          &quot;, &quot;\n          &quot;)).code(ev.isNull, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.javaType(AddFields.this.dataType), ev.value, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue(AddFields.this.dataType, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue$default$2), nullSafeEval)
        </td>
      </tr><tr>
        <td>
          156
        </td>
        <td>
          5841
        </td>
        <td>
          6221
          -
          6406
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;\n          boolean &quot;, &quot; = true;\n          &quot;, &quot; &quot;, &quot; = &quot;, &quot;;\n          &quot;, &quot;\n          &quot;)
        </td>
      </tr><tr>
        <td>
          157
        </td>
        <td>
          5842
        </td>
        <td>
          6249
          -
          6258
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.isNull
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.isNull
        </td>
      </tr><tr>
        <td>
          158
        </td>
        <td>
          5843
        </td>
        <td>
          6280
          -
          6312
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.javaType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.javaType(AddFields.this.dataType)
        </td>
      </tr><tr>
        <td>
          158
        </td>
        <td>
          5846
        </td>
        <td>
          6330
          -
          6366
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue(AddFields.this.dataType, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue$default$2)
        </td>
      </tr><tr>
        <td>
          158
        </td>
        <td>
          5845
        </td>
        <td>
          6344
          -
          6344
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue$default$2
        </td>
      </tr><tr>
        <td>
          158
        </td>
        <td>
          5844
        </td>
        <td>
          6316
          -
          6324
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.value
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ev.value
        </td>
      </tr><tr>
        <td>
          162
        </td>
        <td>
          5861
        </td>
        <td>
          6427
          -
          6646
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.copy
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ev.copy(org.apache.spark.sql.catalyst.expressions.codegen.Block.BlockHelper(scala.StringContext.apply(&quot;\n          &quot;, &quot;\n          &quot;, &quot; &quot;, &quot; = &quot;, &quot;;\n          &quot;, &quot;\n          &quot;)).code(structGen.code, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.javaType(AddFields.this.dataType), ev.value, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue(AddFields.this.dataType, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue$default$2), resultCode), org.apache.spark.sql.catalyst.expressions.codegen.FalseLiteral, ev.copy$default$3)
        </td>
      </tr><tr>
        <td>
          162
        </td>
        <td>
          5860
        </td>
        <td>
          6430
          -
          6430
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ev.copy$default$3
        </td>
      </tr><tr>
        <td>
          162
        </td>
        <td>
          5862
        </td>
        <td>
          6427
          -
          6646
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.copy
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ev.copy(org.apache.spark.sql.catalyst.expressions.codegen.Block.BlockHelper(scala.StringContext.apply(&quot;\n          &quot;, &quot;\n          &quot;, &quot; &quot;, &quot; = &quot;, &quot;;\n          &quot;, &quot;\n          &quot;)).code(structGen.code, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.javaType(AddFields.this.dataType), ev.value, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue(AddFields.this.dataType, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue$default$2), resultCode), org.apache.spark.sql.catalyst.expressions.codegen.FalseLiteral, ev.copy$default$3)
        </td>
      </tr><tr>
        <td>
          163
        </td>
        <td>
          5852
        </td>
        <td>
          6450
          -
          6622
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;\n          &quot;, &quot;\n          &quot;, &quot; &quot;, &quot; = &quot;, &quot;;\n          &quot;, &quot;\n          &quot;)
        </td>
      </tr><tr>
        <td>
          163
        </td>
        <td>
          5858
        </td>
        <td>
          6450
          -
          6622
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.Block.BlockHelper.code
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.codegen.Block.BlockHelper(scala.StringContext.apply(&quot;\n          &quot;, &quot;\n          &quot;, &quot; &quot;, &quot; = &quot;, &quot;;\n          &quot;, &quot;\n          &quot;)).code(structGen.code, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.javaType(AddFields.this.dataType), ev.value, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue(AddFields.this.dataType, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue$default$2), resultCode)
        </td>
      </tr><tr>
        <td>
          164
        </td>
        <td>
          5853
        </td>
        <td>
          6470
          -
          6484
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.code
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          structGen.code
        </td>
      </tr><tr>
        <td>
          165
        </td>
        <td>
          5855
        </td>
        <td>
          6534
          -
          6542
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.ExprCode.value
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ev.value
        </td>
      </tr><tr>
        <td>
          165
        </td>
        <td>
          5857
        </td>
        <td>
          6548
          -
          6584
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue(AddFields.this.dataType, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue$default$2)
        </td>
      </tr><tr>
        <td>
          165
        </td>
        <td>
          5854
        </td>
        <td>
          6498
          -
          6530
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.javaType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.javaType(AddFields.this.dataType)
        </td>
      </tr><tr>
        <td>
          165
        </td>
        <td>
          5856
        </td>
        <td>
          6562
          -
          6562
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.defaultValue$default$2
        </td>
      </tr><tr>
        <td>
          167
        </td>
        <td>
          5859
        </td>
        <td>
          6633
          -
          6645
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.FalseLiteral
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.codegen.FalseLiteral
        </td>
      </tr><tr>
        <td>
          171
        </td>
        <td>
          5863
        </td>
        <td>
          6694
          -
          6706
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;add_fields&quot;
        </td>
      </tr><tr>
        <td>
          181
        </td>
        <td>
          5864
        </td>
        <td>
          7020
          -
          7047
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.TraversableOnce.nonEmpty
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          addOrReplaceFields.nonEmpty
        </td>
      </tr><tr>
        <td>
          181
        </td>
        <td>
          5882
        </td>
        <td>
          7049
          -
          7537
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  val existingFieldNames: Seq[String] = existingFields.map[String, Seq[String]](((x$7: (String, V)) =&gt; x$7._1))(collection.this.Seq.canBuildFrom[String]);
  &lt;synthetic&gt; &lt;artifact&gt; private[this] val x$8: ((String, V), String) = (addOrReplaceFields.head: (String, V) @unchecked) match {
    case (newField @ (_1: String, _2: V)(String, V)((newFieldName @ _), _)) =&gt; scala.Tuple2.apply[(String, V), String](newField, newFieldName)
  };
  val newField: (String, V) = x$8._1;
  val newFieldName: String = x$8._2;
  if (existingFieldNames.contains[String](newFieldName))
    AddFields.this.loop[V](existingFields.map[(String, V), Seq[(String, V)]](((x0$1: (String, V)) =&gt; x0$1 match {
      case (_1: String, _2: V)(String, V)((fieldName @ _), _) if fieldName.==(newFieldName) =&gt; newField
      case (x @ _) =&gt; x
    }))(collection.this.Seq.canBuildFrom[(String, V)]), addOrReplaceFields.drop(1))
  else
    AddFields.this.loop[V](existingFields.:+[(String, V), Seq[(String, V)]](newField)(collection.this.Seq.canBuildFrom[(String, V)]), addOrReplaceFields.drop(1))
}
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          5867
        </td>
        <td>
          7082
          -
          7106
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          existingFields.map[String, Seq[String]](((x$7: (String, V)) =&gt; x$7._1))(collection.this.Seq.canBuildFrom[String])
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          5866
        </td>
        <td>
          7100
          -
          7100
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[String]
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          5865
        </td>
        <td>
          7101
          -
          7105
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$7._1
        </td>
      </tr><tr>
        <td>
          183
        </td>
        <td>
          5869
        </td>
        <td>
          7127
          -
          7127
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$8._2
        </td>
      </tr><tr>
        <td>
          183
        </td>
        <td>
          5868
        </td>
        <td>
          7117
          -
          7117
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$8._1
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          5870
        </td>
        <td>
          7181
          -
          7222
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.contains
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          existingFieldNames.contains[String](newFieldName)
        </td>
      </tr><tr>
        <td>
          186
        </td>
        <td>
          5875
        </td>
        <td>
          7234
          -
          7418
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.utils.AddFields.loop
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.loop[V](existingFields.map[(String, V), Seq[(String, V)]](((x0$1: (String, V)) =&gt; x0$1 match {
  case (_1: String, _2: V)(String, V)((fieldName @ _), _) if fieldName.==(newFieldName) =&gt; newField
  case (x @ _) =&gt; x
}))(collection.this.Seq.canBuildFrom[(String, V)]), addOrReplaceFields.drop(1))
        </td>
      </tr><tr>
        <td>
          186
        </td>
        <td>
          5876
        </td>
        <td>
          7234
          -
          7418
        </td>
        <td>
          Block
        </td>
        <td>
          com.sparkutils.quality.utils.AddFields.loop
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.loop[V](existingFields.map[(String, V), Seq[(String, V)]](((x0$1: (String, V)) =&gt; x0$1 match {
  case (_1: String, _2: V)(String, V)((fieldName @ _), _) if fieldName.==(newFieldName) =&gt; newField
  case (x @ _) =&gt; x
}))(collection.this.Seq.canBuildFrom[(String, V)]), addOrReplaceFields.drop(1))
        </td>
      </tr><tr>
        <td>
          187
        </td>
        <td>
          5873
        </td>
        <td>
          7250
          -
          7379
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          existingFields.map[(String, V), Seq[(String, V)]](((x0$1: (String, V)) =&gt; x0$1 match {
  case (_1: String, _2: V)(String, V)((fieldName @ _), _) if fieldName.==(newFieldName) =&gt; newField
  case (x @ _) =&gt; x
}))(collection.this.Seq.canBuildFrom[(String, V)])
        </td>
      </tr><tr>
        <td>
          187
        </td>
        <td>
          5872
        </td>
        <td>
          7269
          -
          7269
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[(String, V)]
        </td>
      </tr><tr>
        <td>
          188
        </td>
        <td>
          5871
        </td>
        <td>
          7306
          -
          7331
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.==
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          fieldName.==(newFieldName)
        </td>
      </tr><tr>
        <td>
          191
        </td>
        <td>
          5874
        </td>
        <td>
          7391
          -
          7417
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.IterableLike.drop
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          addOrReplaceFields.drop(1)
        </td>
      </tr><tr>
        <td>
          193
        </td>
        <td>
          5881
        </td>
        <td>
          7442
          -
          7523
        </td>
        <td>
          Block
        </td>
        <td>
          com.sparkutils.quality.utils.AddFields.loop
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          AddFields.this.loop[V](existingFields.:+[(String, V), Seq[(String, V)]](newField)(collection.this.Seq.canBuildFrom[(String, V)]), addOrReplaceFields.drop(1))
        </td>
      </tr><tr>
        <td>
          193
        </td>
        <td>
          5880
        </td>
        <td>
          7442
          -
          7523
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.utils.AddFields.loop
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          AddFields.this.loop[V](existingFields.:+[(String, V), Seq[(String, V)]](newField)(collection.this.Seq.canBuildFrom[(String, V)]), addOrReplaceFields.drop(1))
        </td>
      </tr><tr>
        <td>
          194
        </td>
        <td>
          5878
        </td>
        <td>
          7458
          -
          7484
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.SeqLike.:+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          existingFields.:+[(String, V), Seq[(String, V)]](newField)(collection.this.Seq.canBuildFrom[(String, V)])
        </td>
      </tr><tr>
        <td>
          194
        </td>
        <td>
          5877
        </td>
        <td>
          7473
          -
          7473
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          collection.this.Seq.canBuildFrom[(String, V)]
        </td>
      </tr><tr>
        <td>
          195
        </td>
        <td>
          5879
        </td>
        <td>
          7496
          -
          7522
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.IterableLike.drop
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          addOrReplaceFields.drop(1)
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          5883
        </td>
        <td>
          7551
          -
          7565
        </td>
        <td>
          Ident
        </td>
        <td>
          com.sparkutils.quality.utils.AddFields.existingFields
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          existingFields
        </td>
      </tr><tr>
        <td>
          202
        </td>
        <td>
          5884
        </td>
        <td>
          7668
          -
          7696
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.utils.AddFields.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          AddFields.this.copy(newChildren)
        </td>
      </tr>
    </table>
          </div>
        </div>
      </body>
    </html>