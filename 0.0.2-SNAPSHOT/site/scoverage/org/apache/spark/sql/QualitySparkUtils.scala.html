<html>
      <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <title id="title">
          org/apache/spark/sql/QualitySparkUtils.scala.html
        </title>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/css/theme.default.min.css" type="text/css"/><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/js/jquery.tablesorter.min.js"></script><link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css" type="text/css"/><script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script><script type="text/javascript">
        $(document).ready(function() {$(".tablesorter").tablesorter();});
      </script>
        <style>
          table.codegrid { font-family: monospace; font-size: 12px; width: auto!important; }table.statementlist { width: auto!important; font-size: 13px; } table.codegrid td { padding: 0!important; border: 0!important } table td.linenumber { width: 40px!important; } 
        </style>
      </head>
      <body style="font-family: monospace;">
        <ul class="nav nav-tabs">
          <li>
            <a href="#codegrid" data-toggle="tab">Codegrid</a>
          </li>
          <li>
            <a href="#statementlist" data-toggle="tab">Statement List</a>
          </li>
        </ul>
        <div class="tab-content">
          <div class="tab-pane active" id="codegrid">
            <pre style='font-size: 12pt; font-family: courier, monospace;'>1 <span style=''>package org.apache.spark.sql
</span>2 <span style=''>
</span>3 <span style=''>import com.sparkutils.quality.impl.{RuleEngineRunner, RuleFolderRunner, RuleRunner, ShowParams}
</span>4 <span style=''>import com.sparkutils.quality.{RuleSuite, debugTime}
</span>5 <span style=''>import com.sparkutils.quality.utils.PassThrough
</span>6 <span style=''>import org.apache.spark.sql.catalyst.{CatalystTypeConverters, FunctionIdentifier}
</span>7 <span style=''>import org.apache.spark.sql.catalyst.analysis.{Analyzer, DeduplicateRelations, FunctionRegistry, ResolveCatalogs, ResolveExpressionsWithNamePlaceholders, ResolveHigherOrderFunctions, ResolveInlineTables, ResolveLambdaVariables, ResolvePartitionSpec, ResolveTimeZone, ResolveUnion, ResolveWithCTE, SessionWindowing, TimeWindowing, TypeCoercion, UnresolvedFunction}
</span>8 <span style=''>import org.apache.spark.sql.catalyst.catalog.SessionCatalog
</span>9 <span style=''>import org.apache.spark.sql.catalyst.expressions.{Add, Alias, Attribute, BindReferences, Cast, EqualNullSafe, Expression, ExpressionInfo, ExpressionSet, LambdaFunction, Literal}
</span>10 <span style=''>import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, UnaryNode}
</span>11 <span style=''>import org.apache.spark.sql.catalyst.rules.Rule
</span>12 <span style=''>import org.apache.spark.sql.catalyst.trees.TreeNode
</span>13 <span style=''>import org.apache.spark.sql.execution.SparkSqlParser
</span>14 <span style=''>import org.apache.spark.sql.internal.SQLConf
</span>15 <span style=''>import org.apache.spark.sql.qualityFunctions.{Digest, InterpretedHashLongsFunction}
</span>16 <span style=''>import org.apache.spark.sql.types._
</span>17 <span style=''>import org.apache.spark.unsafe.types.CalendarInterval
</span>18 <span style=''>import org.apache.spark.util.Utils
</span>19 <span style=''>
</span>20 <span style=''>/**
</span>21 <span style=''> * Set of utilities to reach in to private functions
</span>22 <span style=''> */
</span>23 <span style=''>object QualitySparkUtils {
</span>24 <span style=''>
</span>25 <span style=''>  implicit class UnresolvedFunctionOps(unresolvedFunction: UnresolvedFunction) {
</span>26 <span style=''>
</span>27 <span style=''>    def theArguments: Seq[Expression] =
</span>28 <span style=''>      </span><span style='background: #AEF1AE'>unresolvedFunction.arguments</span><span style=''>
</span>29 <span style=''>
</span>30 <span style=''>    def withArguments(children: Seq[Expression]): UnresolvedFunction =
</span>31 <span style=''>      </span><span style='background: #AEF1AE'>unresolvedFunction.copy(arguments = children)</span><span style=''>
</span>32 <span style=''>  }
</span>33 <span style=''>
</span>34 <span style=''>  def isPrimitive(dataType: DataType) = </span><span style='background: #AEF1AE'>CatalystTypeConverters.isPrimitive(dataType)</span><span style=''>
</span>35 <span style=''>
</span>36 <span style=''>  /**
</span>37 <span style=''>   * Where resolveWith is not possible (e.g. 10.x DBRs) it is disabled here.
</span>38 <span style=''>   * This is, in the 10.x DBR case, due to the class files for UnaryNode (FakePlan) being radically different and causing an IncompatibleClassChangeError: Implementing class
</span>39 <span style=''>   * @param orig
</span>40 <span style=''>   * @return
</span>41 <span style=''>   */
</span>42 <span style=''>  def resolveWithOverride(orig: Option[DataFrame]): Option[DataFrame] =
</span>43 <span style=''>    orig
</span>44 <span style=''>
</span>45 <span style=''>  /**
</span>46 <span style=''>   * Dbr 11.2 broke the contract for add and cast
</span>47 <span style=''>   * @param left
</span>48 <span style=''>   * @param right
</span>49 <span style=''>   * @return
</span>50 <span style=''>   */
</span>51 <span style=''>  def add(left: Expression, right: Expression, dataType: DataType): Expression =
</span>52 <span style=''>    </span><span style='background: #AEF1AE'>Add(left, right)</span><span style=''>
</span>53 <span style=''>
</span>54 <span style=''>  /**
</span>55 <span style=''>   * Dbr 11.2 broke the contract for add and cast
</span>56 <span style=''>   * @param child
</span>57 <span style=''>   * @param dataType
</span>58 <span style=''>   * @return
</span>59 <span style=''>   */
</span>60 <span style=''>  def cast(child: Expression, dataType: DataType): Expression =
</span>61 <span style=''>    </span><span style='background: #AEF1AE'>Cast(child, dataType)</span><span style=''>
</span>62 <span style=''>
</span>63 <span style=''>  /**
</span>64 <span style=''>   * Arguments for everything above 2.4
</span>65 <span style=''>   */
</span>66 <span style=''>  def arguments(unresolvedFunction: UnresolvedFunction): Seq[Expression] =
</span>67 <span style=''>    </span><span style='background: #AEF1AE'>unresolvedFunction.arguments</span><span style=''>
</span>68 <span style=''>
</span>69 <span style=''>  /**
</span>70 <span style=''>   * Provides Spark 3 specific version of hashing CalendarInterval
</span>71 <span style=''>   *
</span>72 <span style=''>   * @param c
</span>73 <span style=''>   * @param hashlongs
</span>74 <span style=''>   * @param digest
</span>75 <span style=''>   * @return
</span>76 <span style=''>   */
</span>77 <span style=''>  def hashCalendarInterval(c: CalendarInterval, hashlongs: InterpretedHashLongsFunction, digest: Digest): Digest = {
</span>78 <span style=''>    import hashlongs._
</span>79 <span style=''>    </span><span style='background: #F0ADAD'>hashInt(c.months, hashInt(
</span>80 <span style=''></span><span style='background: #F0ADAD'>      c.days
</span>81 <span style=''></span><span style='background: #F0ADAD'>      , hashLong(c.microseconds, digest)))</span><span style=''>
</span>82 <span style=''>  }
</span>83 <span style=''>
</span>84 <span style=''>  /**
</span>85 <span style=''>   * Resolves expressions against a dataframe, this allows them to be swapped out after name checking - spark cannot then
</span>86 <span style=''>   * simply optimise the tree so certain things like constant folding etc. won't show up.
</span>87 <span style=''>   *
</span>88 <span style=''>   * @param dataFrame resolve's must be against a given dataframe to keep names matching
</span>89 <span style=''>   * @param expr      the expression to resolve
</span>90 <span style=''>   */
</span>91 <span style=''>  def resolveExpression(dataFrame: DataFrame, expr: Expression): Expression = {
</span>92 <span style=''>
</span>93 <span style=''>    val sparkSession = </span><span style='background: #AEF1AE'>SparkSession.getActiveSession.get</span><span style=''>
</span>94 <span style=''>
</span>95 <span style=''>    val plan = </span><span style='background: #AEF1AE'>dataFrame.select(&quot;*&quot;).logicalPlan</span><span style=''> // select * needed for toDF's etc. from dataset to force evaluation of the attributes
</span>96 <span style=''>    val res = </span><span style='background: #AEF1AE'>debugTime(&quot;tryResolveReferences&quot;) {
</span>97 <span style=''></span><span style='background: #AEF1AE'>      tryResolveReferences(sparkSession)(expr, plan)
</span>98 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>99 <span style=''>
</span>100 <span style=''>    val fres = </span><span style='background: #AEF1AE'>debugTime(&quot;bindReferences&quot;) {
</span>101 <span style=''></span><span style='background: #AEF1AE'>      BindReferences.bindReference(res, plan.allAttributes)
</span>102 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>103 <span style=''>
</span>104 <span style=''>    fres
</span>105 <span style=''>  }
</span>106 <span style=''>
</span>107 <span style=''>  def execute(logicalPlan: LogicalPlan, batch: Batch) = {
</span>108 <span style=''>    var iteration = </span><span style='background: #AEF1AE'>1</span><span style=''>
</span>109 <span style=''>    var curPlan = logicalPlan
</span>110 <span style=''>    var lastPlan = logicalPlan
</span>111 <span style=''>
</span>112 <span style=''>    var start = </span><span style='background: #AEF1AE'>System.currentTimeMillis</span><span style=''>
</span>113 <span style=''>
</span>114 <span style=''>
</span>115 <span style=''>    var continue = </span><span style='background: #AEF1AE'>true</span><span style=''>
</span>116 <span style=''>    val analyzer = SparkSession.getActiveSession.get.sessionState.analyzer
</span>117 <span style=''>
</span>118 <span style=''>    // Run until fix point (or the max number of iterations as specified in the strategy.
</span>119 <span style=''>    while (continue) </span><span style='background: #AEF1AE'>{
</span>120 <span style=''></span><span style='background: #AEF1AE'>      curPlan = batch.rules.foldLeft(curPlan) {
</span>121 <span style=''></span><span style='background: #AEF1AE'>        case (plan, rule) =&gt;
</span>122 <span style=''></span><span style='background: #AEF1AE'>          val startTime = System.nanoTime()
</span>123 <span style=''></span><span style='background: #AEF1AE'>          val result = rule(plan)
</span>124 <span style=''></span><span style='background: #AEF1AE'>
</span>125 <span style=''></span><span style='background: #AEF1AE'>          result
</span>126 <span style=''></span><span style='background: #AEF1AE'>      }
</span>127 <span style=''></span><span style='background: #AEF1AE'>      iteration += 1
</span>128 <span style=''></span><span style='background: #AEF1AE'>      if (iteration &gt; batch.strategy.maxIterations) </span><span style='background: #F0ADAD'>{
</span>129 <span style=''></span><span style='background: #F0ADAD'>        // Only log if this is a rule that is supposed to run more than once.
</span>130 <span style=''></span><span style='background: #F0ADAD'>        if (iteration != 2) {
</span>131 <span style=''></span><span style='background: #F0ADAD'>          val endingMsg = if (batch.strategy.maxIterationsSetting == null) {
</span>132 <span style=''></span><span style='background: #F0ADAD'>            &quot;.&quot;
</span>133 <span style=''></span><span style='background: #F0ADAD'>          } else {
</span>134 <span style=''></span><span style='background: #F0ADAD'>            s&quot;, please set '${batch.strategy.maxIterationsSetting}' to a larger value.&quot;
</span>135 <span style=''></span><span style='background: #F0ADAD'>          }
</span>136 <span style=''></span><span style='background: #F0ADAD'>          val message = s&quot;Max iterations (${iteration - 1}) reached for batch ${batch.name}&quot; +
</span>137 <span style=''></span><span style='background: #F0ADAD'>            s&quot;$endingMsg&quot;
</span>138 <span style=''></span><span style='background: #F0ADAD'>          if (Utils.isTesting || batch.strategy.errorOnExceed) {
</span>139 <span style=''></span><span style='background: #F0ADAD'>            throw new Exception(message)
</span>140 <span style=''></span><span style='background: #F0ADAD'>          } else {
</span>141 <span style=''></span><span style='background: #F0ADAD'>          }
</span>142 <span style=''></span><span style='background: #F0ADAD'>        }
</span>143 <span style=''></span><span style='background: #F0ADAD'>        continue = false
</span>144 <span style=''></span><span style='background: #F0ADAD'>      }</span><span style='background: #AEF1AE'>
</span>145 <span style=''></span><span style='background: #AEF1AE'>
</span>146 <span style=''></span><span style='background: #AEF1AE'>      if (curPlan.fastEquals(lastPlan)) {
</span>147 <span style=''></span><span style='background: #AEF1AE'>        continue = false
</span>148 <span style=''></span><span style='background: #AEF1AE'>      }
</span>149 <span style=''></span><span style='background: #AEF1AE'>      lastPlan = curPlan
</span>150 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>151 <span style=''>    var stop = </span><span style='background: #AEF1AE'>System.currentTimeMillis</span><span style=''>
</span>152 <span style=''>    //println(s&quot;did $iteration iterations to execute the plan in ${stop-start}ms&quot;)
</span>153 <span style=''>    curPlan
</span>154 <span style=''>  }
</span>155 <span style=''>
</span>156 <span style=''>  case class Strategy(
</span>157 <span style=''>                       maxIterations: Int, errorOnExceed: Boolean = false, maxIterationsSetting: String = null
</span>158 <span style=''>                     )
</span>159 <span style=''>
</span>160 <span style=''>  case class Batch(name: String, strategy: Strategy, rules: Rule[LogicalPlan]*)
</span>161 <span style=''>
</span>162 <span style=''>
</span>163 <span style=''>  def resolution(analyzer: Analyzer, sparkSession: SparkSession, plan: LogicalPlan) = {
</span>164 <span style=''>    val conf = </span><span style='background: #AEF1AE'>sparkSession.sqlContext.conf</span><span style=''>
</span>165 <span style=''>    val fixedPoint = </span><span style='background: #AEF1AE'>new Strategy(
</span>166 <span style=''></span><span style='background: #AEF1AE'>      conf.analyzerMaxIterations,
</span>167 <span style=''></span><span style='background: #AEF1AE'>      errorOnExceed = true,
</span>168 <span style=''></span><span style='background: #AEF1AE'>      maxIterationsSetting = SQLConf.ANALYZER_MAX_ITERATIONS.key)</span><span style=''>
</span>169 <span style=''>
</span>170 <span style=''>    import analyzer._
</span>171 <span style=''>
</span>172 <span style=''>    </span><span style='background: #AEF1AE'>Batch(&quot;Resolution&quot;, fixedPoint,
</span>173 <span style=''></span><span style='background: #AEF1AE'>      ResolveNamespace(catalogManager) ::
</span>174 <span style=''></span><span style='background: #AEF1AE'>        new ResolveCatalogs(catalogManager) ::
</span>175 <span style=''></span><span style='background: #AEF1AE'>        ResolveUserSpecifiedColumns ::
</span>176 <span style=''></span><span style='background: #AEF1AE'>        ResolveInsertInto ::
</span>177 <span style=''></span><span style='background: #AEF1AE'>        ResolveRelations ::
</span>178 <span style=''></span><span style='background: #AEF1AE'>        ResolveTables ::
</span>179 <span style=''></span><span style='background: #AEF1AE'>        ResolvePartitionSpec ::
</span>180 <span style=''></span><span style='background: #AEF1AE'>        ResolveAlterTableCommands ::
</span>181 <span style=''></span><span style='background: #AEF1AE'>        AddMetadataColumns ::
</span>182 <span style=''></span><span style='background: #AEF1AE'>        DeduplicateRelations ::
</span>183 <span style=''></span><span style='background: #AEF1AE'>        ResolveReferences ::
</span>184 <span style=''></span><span style='background: #AEF1AE'>        ResolveExpressionsWithNamePlaceholders ::
</span>185 <span style=''></span><span style='background: #AEF1AE'>        ResolveDeserializer ::
</span>186 <span style=''></span><span style='background: #AEF1AE'>        ResolveNewInstance ::
</span>187 <span style=''></span><span style='background: #AEF1AE'>        ResolveUpCast ::
</span>188 <span style=''></span><span style='background: #AEF1AE'>        ResolveGroupingAnalytics ::
</span>189 <span style=''></span><span style='background: #AEF1AE'>        ResolvePivot ::
</span>190 <span style=''></span><span style='background: #AEF1AE'>        ResolveOrdinalInOrderByAndGroupBy ::
</span>191 <span style=''></span><span style='background: #AEF1AE'>        ResolveAggAliasInGroupBy ::
</span>192 <span style=''></span><span style='background: #AEF1AE'>        ResolveMissingReferences ::
</span>193 <span style=''></span><span style='background: #AEF1AE'>        ExtractGenerator ::
</span>194 <span style=''></span><span style='background: #AEF1AE'>        ResolveGenerate ::
</span>195 <span style=''></span><span style='background: #AEF1AE'>        ResolveFunctions ::
</span>196 <span style=''></span><span style='background: #AEF1AE'>        ResolveAliases ::
</span>197 <span style=''></span><span style='background: #AEF1AE'>        ResolveSubquery ::
</span>198 <span style=''></span><span style='background: #AEF1AE'>        ResolveSubqueryColumnAliases ::
</span>199 <span style=''></span><span style='background: #AEF1AE'>        ResolveWindowOrder ::
</span>200 <span style=''></span><span style='background: #AEF1AE'>        ResolveWindowFrame ::
</span>201 <span style=''></span><span style='background: #AEF1AE'>        ResolveNaturalAndUsingJoin ::
</span>202 <span style=''></span><span style='background: #AEF1AE'>        ResolveOutputRelation ::
</span>203 <span style=''></span><span style='background: #AEF1AE'>        ExtractWindowExpressions ::
</span>204 <span style=''></span><span style='background: #AEF1AE'>        GlobalAggregates ::
</span>205 <span style=''></span><span style='background: #AEF1AE'>        ResolveAggregateFunctions ::
</span>206 <span style=''></span><span style='background: #AEF1AE'>        TimeWindowing ::
</span>207 <span style=''></span><span style='background: #AEF1AE'>        SessionWindowing ::
</span>208 <span style=''></span><span style='background: #AEF1AE'>        ResolveInlineTables ::
</span>209 <span style=''></span><span style='background: #AEF1AE'>        ResolveHigherOrderFunctions(catalogManager) ::
</span>210 <span style=''></span><span style='background: #AEF1AE'>        ResolveLambdaVariables ::
</span>211 <span style=''></span><span style='background: #AEF1AE'>        ResolveTimeZone ::
</span>212 <span style=''></span><span style='background: #AEF1AE'>        ResolveRandomSeed ::
</span>213 <span style=''></span><span style='background: #AEF1AE'>        ResolveBinaryArithmetic ::
</span>214 <span style=''></span><span style='background: #AEF1AE'>        ResolveUnion ::
</span>215 <span style=''></span><span style='background: #AEF1AE'>        TypeCoercion.typeCoercionRules ++
</span>216 <span style=''></span><span style='background: #AEF1AE'>          Seq(ResolveWithCTE): _*)</span><span style=''>
</span>217 <span style=''>  }
</span>218 <span style=''>
</span>219 <span style=''>  // below based on approach from delta / discussed with Alex to use a Project, LeafNode should be fine
</span>220 <span style=''>  protected def tryResolveReferences(
</span>221 <span style=''>                                      sparkSession: SparkSession)(
</span>222 <span style=''>                                      expr: Expression,
</span>223 <span style=''>                                      child: LogicalPlan): Expression = {
</span>224 <span style=''>    val analyzer = sparkSession.sessionState.analyzer
</span>225 <span style=''>
</span>226 <span style=''>    def forExpr(expr: Expression) = {
</span>227 <span style=''>      val newPlan = </span><span style='background: #AEF1AE'>FakePlan(expr, child)</span><span style=''>
</span>228 <span style=''>      //analyzer.execute(newPlan)
</span>229 <span style=''>      </span><span style='background: #AEF1AE'>execute(newPlan, resolution(analyzer, sparkSession, newPlan))</span><span style=''>
</span>230 <span style=''>      match {
</span>231 <span style=''>        case FakePlan(resolvedExpr, _) =&gt;
</span>232 <span style=''>          // Return even if it did not successfully resolve
</span>233 <span style=''>          resolvedExpr
</span>234 <span style=''>        case _ =&gt;
</span>235 <span style=''>          // This is unexpected
</span>236 <span style=''>          </span><span style='background: #F0ADAD'>throw new Exception(
</span>237 <span style=''></span><span style='background: #F0ADAD'>            s&quot;Could not resolve expression $expr with child $child}&quot;)</span><span style=''>
</span>238 <span style=''>      }
</span>239 <span style=''>    }
</span>240 <span style=''>    // special case as it's faster to do individual items it seems, 36816ms vs 48974ms
</span>241 <span style=''>    expr match {
</span>242 <span style=''>      case r @ RuleEngineRunner(ruleSuite, PassThrough( expressions ), realType, compileEvals, debugMode, func, group, forceRunnerEval, expressionOffsets, forceTriggerEval) =&gt;
</span>243 <span style=''>        val nexprs = </span><span style='background: #AEF1AE'>expressions.map(forExpr)</span><span style=''>
</span>244 <span style=''>        </span><span style='background: #AEF1AE'>RuleEngineRunner(ruleSuite, PassThrough( nexprs ), realType, compileEvals, debugMode, func, group, forceRunnerEval, expressionOffsets, forceTriggerEval)</span><span style=''>
</span>245 <span style=''>      case r @ RuleFolderRunner(ruleSuite, left, PassThrough( expressions ), resultDataType, compileEvals, debugMode, variablesPerFunc,
</span>246 <span style=''>        variableFuncGroup, forceRunnerEval, expressionOffsets, dataRef, forceTriggerEval) =&gt;
</span>247 <span style=''>        val nexprs = </span><span style='background: #F0ADAD'>expressions.map(forExpr)</span><span style=''>
</span>248 <span style=''>        </span><span style='background: #F0ADAD'>RuleFolderRunner(ruleSuite, left, PassThrough( nexprs ), resultDataType, compileEvals, debugMode, variablesPerFunc,
</span>249 <span style=''></span><span style='background: #F0ADAD'>          variableFuncGroup, forceRunnerEval, expressionOffsets, dataRef, forceTriggerEval)</span><span style=''>
</span>250 <span style=''>      case r @ RuleRunner(ruleSuite, PassThrough( expressions ), compileEvals, func, group, forceRunnerEval) =&gt;
</span>251 <span style=''>        val nexprs = </span><span style='background: #AEF1AE'>expressions.map(forExpr)</span><span style=''>
</span>252 <span style=''>        </span><span style='background: #AEF1AE'>RuleRunner(ruleSuite, PassThrough( nexprs ), compileEvals, func, group, forceRunnerEval)</span><span style=''>
</span>253 <span style=''>      case _ =&gt; </span><span style='background: #F0ADAD'>forExpr(expr)</span><span style=''>
</span>254 <span style=''>    }
</span>255 <span style=''>  }
</span>256 <span style=''>
</span>257 <span style=''>  case class FakePlan(expr: Expression, child: LogicalPlan)
</span>258 <span style=''>    extends UnaryNode {
</span>259 <span style=''>
</span>260 <span style=''>    override def output: Seq[Attribute] = </span><span style='background: #F0ADAD'>child.allAttributes.attrs</span><span style=''>
</span>261 <span style=''>
</span>262 <span style=''>    override def maxRows: Option[Long] = </span><span style='background: #F0ADAD'>Some(1)</span><span style=''>
</span>263 <span style=''>
</span>264 <span style=''>    protected def mygetAllValidConstraints(projectList: Seq[Expression]): Set[Expression] = {
</span>265 <span style=''>      var allConstraints = </span><span style='background: #F0ADAD'>Set.empty[Expression]</span><span style=''>
</span>266 <span style=''>      </span><span style='background: #F0ADAD'>projectList.foreach {
</span>267 <span style=''></span><span style='background: #F0ADAD'>        case a@Alias(l: Literal, _) =&gt;
</span>268 <span style=''></span><span style='background: #F0ADAD'>          allConstraints += EqualNullSafe(a.toAttribute, l)
</span>269 <span style=''></span><span style='background: #F0ADAD'>        case a@Alias(e, _) =&gt;
</span>270 <span style=''></span><span style='background: #F0ADAD'>          // For every alias in `projectList`, replace the reference in constraints by its attribute.
</span>271 <span style=''></span><span style='background: #F0ADAD'>          allConstraints ++= allConstraints.map(_ transform {
</span>272 <span style=''></span><span style='background: #F0ADAD'>            case expr: Expression if expr.semanticEquals(e) =&gt;
</span>273 <span style=''></span><span style='background: #F0ADAD'>              a.toAttribute
</span>274 <span style=''></span><span style='background: #F0ADAD'>          })
</span>275 <span style=''></span><span style='background: #F0ADAD'>          allConstraints += EqualNullSafe(e, a.toAttribute)
</span>276 <span style=''></span><span style='background: #F0ADAD'>        case _ =&gt; // Don't change.
</span>277 <span style=''></span><span style='background: #F0ADAD'>      }</span><span style=''>
</span>278 <span style=''>
</span>279 <span style=''>      allConstraints
</span>280 <span style=''>    }
</span>281 <span style=''>
</span>282 <span style=''>    override lazy val validConstraints: ExpressionSet = ExpressionSet(mygetAllValidConstraints(Seq(expr)))
</span>283 <span style=''>
</span>284 <span style=''>    protected def withNewChildInternal(newChild: LogicalPlan): LogicalPlan = </span><span style='background: #F0ADAD'>copy(child = newChild)</span><span style=''>
</span>285 <span style=''>  }
</span>286 <span style=''>
</span>287 <span style=''>  /**
</span>288 <span style=''>   * Creates a new parser, introduced in 0.4 - 3.2.0 due to SparkSqlParser having no params
</span>289 <span style=''>   *
</span>290 <span style=''>   * @return
</span>291 <span style=''>   */
</span>292 <span style=''>  def newParser() = {
</span>293 <span style=''>    </span><span style='background: #AEF1AE'>new SparkSqlParser()</span><span style=''>
</span>294 <span style=''>  }
</span>295 <span style=''>
</span>296 <span style=''>  /**
</span>297 <span style=''>   * Registers functions with spark, Introduced in 0.4 - 3.2.0 support due to extra source parameter - &quot;built-in&quot; is used as no other option is remotely close
</span>298 <span style=''>   *
</span>299 <span style=''>   * @param funcReg
</span>300 <span style=''>   * @param name
</span>301 <span style=''>   * @param builder
</span>302 <span style=''>   */
</span>303 <span style=''>  def registerFunction(funcReg: FunctionRegistry)(name: String, builder: Seq[Expression] =&gt; Expression) =
</span>304 <span style=''>    </span><span style='background: #AEF1AE'>funcReg.createOrReplaceTempFunction(name, builder, &quot;built-in&quot;)</span><span style=''>
</span>305 <span style=''>
</span>306 <span style=''>  def toString(dataFrame: DataFrame, showParams: ShowParams = ShowParams()) =
</span>307 <span style=''>    </span><span style='background: #AEF1AE'>dataFrame.showString(showParams.numRows, showParams.truncate, showParams.vertical)</span><span style=''>
</span>308 <span style=''>
</span>309 <span style=''>  /**
</span>310 <span style=''>   * Used by the SparkSessionExtensions mechanism
</span>311 <span style=''>   * @param extensions
</span>312 <span style=''>   * @param name
</span>313 <span style=''>   * @param builder
</span>314 <span style=''>   */
</span>315 <span style=''>  def registerFunctionViaExtension(extensions: SparkSessionExtensions)(name: String, builder: Seq[Expression] =&gt; Expression) =
</span>316 <span style=''>    </span><span style='background: #AEF1AE'>extensions.injectFunction( (FunctionIdentifier(name), new ExpressionInfo(name, name) , builder) )</span><span style=''>
</span>317 <span style=''>
</span>318 <span style=''>}
</span></pre>
          </div>
          <div class="tab-pane" id="statementlist">
            <table cellspacing="0" cellpadding="0" class="table statementlist">
      <tr>
        <th>Line</th>
        <th>Stmt Id</th>
        <th>Pos</th>
        <th>Tree</th>
        <th>Symbol</th>
        <th>Tests</th>
        <th>Code</th>
      </tr><tr>
        <td>
          28
        </td>
        <td>
          254
        </td>
        <td>
          1612
          -
          1640
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedFunction.arguments
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          UnresolvedFunctionOps.this.unresolvedFunction.arguments
        </td>
      </tr><tr>
        <td>
          31
        </td>
        <td>
          256
        </td>
        <td>
          1738
          -
          1738
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedFunction.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          UnresolvedFunctionOps.this.unresolvedFunction.copy$default$3
        </td>
      </tr><tr>
        <td>
          31
        </td>
        <td>
          259
        </td>
        <td>
          1719
          -
          1764
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedFunction.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          UnresolvedFunctionOps.this.unresolvedFunction.copy(x$2, x$1, x$3, x$4, x$5)
        </td>
      </tr><tr>
        <td>
          31
        </td>
        <td>
          258
        </td>
        <td>
          1738
          -
          1738
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedFunction.copy$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          UnresolvedFunctionOps.this.unresolvedFunction.copy$default$5
        </td>
      </tr><tr>
        <td>
          31
        </td>
        <td>
          255
        </td>
        <td>
          1738
          -
          1738
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedFunction.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          UnresolvedFunctionOps.this.unresolvedFunction.copy$default$1
        </td>
      </tr><tr>
        <td>
          31
        </td>
        <td>
          257
        </td>
        <td>
          1738
          -
          1738
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedFunction.copy$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          UnresolvedFunctionOps.this.unresolvedFunction.copy$default$4
        </td>
      </tr><tr>
        <td>
          34
        </td>
        <td>
          260
        </td>
        <td>
          1810
          -
          1854
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.CatalystTypeConverters.isPrimitive
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.CatalystTypeConverters.isPrimitive(dataType)
        </td>
      </tr><tr>
        <td>
          52
        </td>
        <td>
          262
        </td>
        <td>
          2426
          -
          2442
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Add.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Add.apply(left, right, org.apache.spark.sql.catalyst.expressions.Add.apply$default$3)
        </td>
      </tr><tr>
        <td>
          52
        </td>
        <td>
          261
        </td>
        <td>
          2426
          -
          2426
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Add.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Add.apply$default$3
        </td>
      </tr><tr>
        <td>
          61
        </td>
        <td>
          265
        </td>
        <td>
          2626
          -
          2647
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Cast.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Cast.apply(child, dataType, org.apache.spark.sql.catalyst.expressions.Cast.apply$default$3, org.apache.spark.sql.catalyst.expressions.Cast.apply$default$4)
        </td>
      </tr><tr>
        <td>
          61
        </td>
        <td>
          264
        </td>
        <td>
          2626
          -
          2626
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Cast.apply$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Cast.apply$default$4
        </td>
      </tr><tr>
        <td>
          61
        </td>
        <td>
          263
        </td>
        <td>
          2626
          -
          2626
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Cast.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Cast.apply$default$3
        </td>
      </tr><tr>
        <td>
          67
        </td>
        <td>
          266
        </td>
        <td>
          2780
          -
          2808
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedFunction.arguments
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          unresolvedFunction.arguments
        </td>
      </tr><tr>
        <td>
          79
        </td>
        <td>
          271
        </td>
        <td>
          3124
          -
          3187
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          hashlongs.hashInt(c.days, hashlongs.hashLong(c.microseconds, digest))
        </td>
      </tr><tr>
        <td>
          79
        </td>
        <td>
          267
        </td>
        <td>
          3114
          -
          3122
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.unsafe.types.CalendarInterval.months
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          c.months
        </td>
      </tr><tr>
        <td>
          79
        </td>
        <td>
          272
        </td>
        <td>
          3106
          -
          3188
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          hashlongs.hashInt(c.months, hashlongs.hashInt(c.days, hashlongs.hashLong(c.microseconds, digest)))
        </td>
      </tr><tr>
        <td>
          80
        </td>
        <td>
          268
        </td>
        <td>
          3139
          -
          3145
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.unsafe.types.CalendarInterval.days
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          c.days
        </td>
      </tr><tr>
        <td>
          81
        </td>
        <td>
          270
        </td>
        <td>
          3154
          -
          3186
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          hashlongs.hashLong(c.microseconds, digest)
        </td>
      </tr><tr>
        <td>
          81
        </td>
        <td>
          269
        </td>
        <td>
          3163
          -
          3177
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.unsafe.types.CalendarInterval.microseconds
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          c.microseconds
        </td>
      </tr><tr>
        <td>
          93
        </td>
        <td>
          273
        </td>
        <td>
          3664
          -
          3697
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.get
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SparkSession.getActiveSession.get
        </td>
      </tr><tr>
        <td>
          95
        </td>
        <td>
          274
        </td>
        <td>
          3731
          -
          3734
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;*&quot;
        </td>
      </tr><tr>
        <td>
          95
        </td>
        <td>
          275
        </td>
        <td>
          3714
          -
          3747
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.Dataset.logicalPlan
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          dataFrame.select(&quot;*&quot;).logicalPlan
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          277
        </td>
        <td>
          3848
          -
          3848
        </td>
        <td>
          TypeApply
        </td>
        <td>
          com.sparkutils.quality.debugTime$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.`package`.debugTime$default$2[Nothing]
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          276
        </td>
        <td>
          3858
          -
          3880
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;tryResolveReferences&quot;
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          279
        </td>
        <td>
          3848
          -
          3942
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.debugTime
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.`package`.debugTime[org.apache.spark.sql.catalyst.expressions.Expression](&quot;tryResolveReferences&quot;, com.sparkutils.quality.`package`.debugTime$default$2[Nothing])(QualitySparkUtils.this.tryResolveReferences(sparkSession)(expr, plan))
        </td>
      </tr><tr>
        <td>
          97
        </td>
        <td>
          278
        </td>
        <td>
          3890
          -
          3936
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.tryResolveReferences
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          QualitySparkUtils.this.tryResolveReferences(sparkSession)(expr, plan)
        </td>
      </tr><tr>
        <td>
          100
        </td>
        <td>
          280
        </td>
        <td>
          3969
          -
          3985
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;bindReferences&quot;
        </td>
      </tr><tr>
        <td>
          100
        </td>
        <td>
          281
        </td>
        <td>
          3959
          -
          3959
        </td>
        <td>
          TypeApply
        </td>
        <td>
          com.sparkutils.quality.debugTime$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.`package`.debugTime$default$2[Nothing]
        </td>
      </tr><tr>
        <td>
          100
        </td>
        <td>
          284
        </td>
        <td>
          3959
          -
          4054
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.debugTime
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.`package`.debugTime[org.apache.spark.sql.catalyst.expressions.Expression](&quot;bindReferences&quot;, com.sparkutils.quality.`package`.debugTime$default$2[Nothing])(org.apache.spark.sql.catalyst.expressions.BindReferences.bindReference[org.apache.spark.sql.catalyst.expressions.Expression](res, plan.allAttributes, org.apache.spark.sql.catalyst.expressions.BindReferences.bindReference$default$3[Nothing]))
        </td>
      </tr><tr>
        <td>
          101
        </td>
        <td>
          283
        </td>
        <td>
          3995
          -
          4048
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.BindReferences.bindReference
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.BindReferences.bindReference[org.apache.spark.sql.catalyst.expressions.Expression](res, plan.allAttributes, org.apache.spark.sql.catalyst.expressions.BindReferences.bindReference$default$3[Nothing])
        </td>
      </tr><tr>
        <td>
          101
        </td>
        <td>
          282
        </td>
        <td>
          4010
          -
          4010
        </td>
        <td>
          TypeApply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.BindReferences.bindReference$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.BindReferences.bindReference$default$3[Nothing]
        </td>
      </tr><tr>
        <td>
          108
        </td>
        <td>
          285
        </td>
        <td>
          4148
          -
          4149
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          1
        </td>
      </tr><tr>
        <td>
          112
        </td>
        <td>
          286
        </td>
        <td>
          4228
          -
          4252
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.System.currentTimeMillis
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          java.lang.System.currentTimeMillis()
        </td>
      </tr><tr>
        <td>
          115
        </td>
        <td>
          287
        </td>
        <td>
          4274
          -
          4278
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          119
        </td>
        <td>
          331
        </td>
        <td>
          4449
          -
          4449
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          119
        </td>
        <td>
          328
        </td>
        <td>
          4466
          -
          4466
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.while$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          while$1()
        </td>
      </tr><tr>
        <td>
          119
        </td>
        <td>
          330
        </td>
        <td>
          4449
          -
          4449
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          119
        </td>
        <td>
          329
        </td>
        <td>
          4466
          -
          5451
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  {
    curPlan = batch.rules.foldLeft[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan](curPlan)(((x0$1: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan, x1$1: org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]) =&gt; scala.Tuple2.apply[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan, org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x0$1, x1$1) match {
      case (_1: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan, _2: org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan])(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan, org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan])((plan @ _), (rule @ _)) =&gt; {
        val startTime: Long = java.lang.System.nanoTime();
        val result: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan = rule.apply(plan);
        result
      }
    }));
    iteration = iteration.+(1);
    if (iteration.&gt;(batch.strategy.maxIterations))
      {
        if (iteration.!=(2))
          {
            val endingMsg: String = if (batch.strategy.maxIterationsSetting.==(null))
              &quot;.&quot;
            else
              scala.StringContext.apply(&quot;, please set \'&quot;, &quot;\' to a larger value.&quot;).s(batch.strategy.maxIterationsSetting);
            val message: String = scala.StringContext.apply(&quot;Max iterations (&quot;, &quot;) reached for batch &quot;, &quot;&quot;).s(iteration.-(1), batch.name).+(scala.StringContext.apply(&quot;&quot;, &quot;&quot;).s(endingMsg));
            if (org.apache.spark.util.Utils.isTesting.||(batch.strategy.errorOnExceed))
              throw new scala.`package`.Exception(message)
            else
              ()
          }
        else
          ();
        continue = false
      }
    else
      ();
    if (curPlan.fastEquals(lastPlan))
      continue = false
    else
      ();
    lastPlan = curPlan
  };
  while$1()
}
        </td>
      </tr><tr>
        <td>
          120
        </td>
        <td>
          290
        </td>
        <td>
          4484
          -
          4648
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.foldLeft
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          batch.rules.foldLeft[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan](curPlan)(((x0$1: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan, x1$1: org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]) =&gt; scala.Tuple2.apply[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan, org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x0$1, x1$1) match {
  case (_1: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan, _2: org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan])(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan, org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan])((plan @ _), (rule @ _)) =&gt; {
    val startTime: Long = java.lang.System.nanoTime();
    val result: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan = rule.apply(plan);
    result
  }
}))
        </td>
      </tr><tr>
        <td>
          122
        </td>
        <td>
          288
        </td>
        <td>
          4571
          -
          4588
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.System.nanoTime
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          java.lang.System.nanoTime()
        </td>
      </tr><tr>
        <td>
          123
        </td>
        <td>
          289
        </td>
        <td>
          4612
          -
          4622
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.rules.Rule.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          rule.apply(plan)
        </td>
      </tr><tr>
        <td>
          127
        </td>
        <td>
          291
        </td>
        <td>
          4655
          -
          4669
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          iteration.+(1)
        </td>
      </tr><tr>
        <td>
          128
        </td>
        <td>
          322
        </td>
        <td>
          4676
          -
          4676
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          128
        </td>
        <td>
          292
        </td>
        <td>
          4692
          -
          4720
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.Strategy.maxIterations
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          batch.strategy.maxIterations
        </td>
      </tr><tr>
        <td>
          128
        </td>
        <td>
          321
        </td>
        <td>
          4676
          -
          4676
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          128
        </td>
        <td>
          320
        </td>
        <td>
          4722
          -
          5344
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          {
  if (iteration.!=(2))
    {
      val endingMsg: String = if (batch.strategy.maxIterationsSetting.==(null))
        &quot;.&quot;
      else
        scala.StringContext.apply(&quot;, please set \'&quot;, &quot;\' to a larger value.&quot;).s(batch.strategy.maxIterationsSetting);
      val message: String = scala.StringContext.apply(&quot;Max iterations (&quot;, &quot;) reached for batch &quot;, &quot;&quot;).s(iteration.-(1), batch.name).+(scala.StringContext.apply(&quot;&quot;, &quot;&quot;).s(endingMsg));
      if (org.apache.spark.util.Utils.isTesting.||(batch.strategy.errorOnExceed))
        throw new scala.`package`.Exception(message)
      else
        ()
    }
  else
    ();
  continue = false
}
        </td>
      </tr><tr>
        <td>
          128
        </td>
        <td>
          293
        </td>
        <td>
          4680
          -
          4720
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          iteration.&gt;(batch.strategy.maxIterations)
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          316
        </td>
        <td>
          4830
          -
          5311
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          {
  val endingMsg: String = if (batch.strategy.maxIterationsSetting.==(null))
    &quot;.&quot;
  else
    scala.StringContext.apply(&quot;, please set \'&quot;, &quot;\' to a larger value.&quot;).s(batch.strategy.maxIterationsSetting);
  val message: String = scala.StringContext.apply(&quot;Max iterations (&quot;, &quot;) reached for batch &quot;, &quot;&quot;).s(iteration.-(1), batch.name).+(scala.StringContext.apply(&quot;&quot;, &quot;&quot;).s(endingMsg));
  if (org.apache.spark.util.Utils.isTesting.||(batch.strategy.errorOnExceed))
    throw new scala.`package`.Exception(message)
  else
    ()
}
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          318
        </td>
        <td>
          4810
          -
          4810
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          294
        </td>
        <td>
          4814
          -
          4828
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.!=
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          iteration.!=(2)
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          317
        </td>
        <td>
          4810
          -
          4810
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          131
        </td>
        <td>
          295
        </td>
        <td>
          4862
          -
          4905
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.==
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          batch.strategy.maxIterationsSetting.==(null)
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          297
        </td>
        <td>
          4921
          -
          4924
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.&quot;
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          296
        </td>
        <td>
          4921
          -
          4924
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.&quot;
        </td>
      </tr><tr>
        <td>
          134
        </td>
        <td>
          298
        </td>
        <td>
          4958
          -
          4973
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;, please set \'&quot;
        </td>
      </tr><tr>
        <td>
          134
        </td>
        <td>
          301
        </td>
        <td>
          4956
          -
          5031
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;, please set \'&quot;, &quot;\' to a larger value.&quot;).s(batch.strategy.maxIterationsSetting)
        </td>
      </tr><tr>
        <td>
          134
        </td>
        <td>
          300
        </td>
        <td>
          4974
          -
          5009
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.Strategy.maxIterationsSetting
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          batch.strategy.maxIterationsSetting
        </td>
      </tr><tr>
        <td>
          134
        </td>
        <td>
          299
        </td>
        <td>
          5010
          -
          5031
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\' to a larger value.&quot;
        </td>
      </tr><tr>
        <td>
          134
        </td>
        <td>
          302
        </td>
        <td>
          4956
          -
          5031
        </td>
        <td>
          Block
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;, please set \'&quot;, &quot;\' to a larger value.&quot;).s(batch.strategy.maxIterationsSetting)
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          304
        </td>
        <td>
          5102
          -
          5123
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;) reached for batch &quot;
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          307
        </td>
        <td>
          5124
          -
          5134
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.Batch.name
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          batch.name
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          309
        </td>
        <td>
          5068
          -
          5164
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;Max iterations (&quot;, &quot;) reached for batch &quot;, &quot;&quot;).s(iteration.-(1), batch.name).+(scala.StringContext.apply(&quot;&quot;, &quot;&quot;).s(endingMsg))
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          303
        </td>
        <td>
          5070
          -
          5087
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;Max iterations (&quot;
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          306
        </td>
        <td>
          5088
          -
          5101
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.-
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          iteration.-(1)
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          305
        </td>
        <td>
          5135
          -
          5136
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          137
        </td>
        <td>
          308
        </td>
        <td>
          5151
          -
          5164
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot;&quot;).s(endingMsg)
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          310
        </td>
        <td>
          5198
          -
          5226
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.Strategy.errorOnExceed
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          batch.strategy.errorOnExceed
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          311
        </td>
        <td>
          5179
          -
          5226
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.||
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.util.Utils.isTesting.||(batch.strategy.errorOnExceed)
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          313
        </td>
        <td>
          5242
          -
          5270
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          throw new scala.`package`.Exception(message)
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          312
        </td>
        <td>
          5242
          -
          5270
        </td>
        <td>
          Throw
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          throw new scala.`package`.Exception(message)
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          315
        </td>
        <td>
          5288
          -
          5301
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          314
        </td>
        <td>
          5288
          -
          5301
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          143
        </td>
        <td>
          319
        </td>
        <td>
          5331
          -
          5336
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          false
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          327
        </td>
        <td>
          5352
          -
          5352
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          323
        </td>
        <td>
          5356
          -
          5384
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.fastEquals
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          curPlan.fastEquals(lastPlan)
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          326
        </td>
        <td>
          5352
          -
          5352
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          147
        </td>
        <td>
          325
        </td>
        <td>
          5396
          -
          5412
        </td>
        <td>
          Assign
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          continue = false
        </td>
      </tr><tr>
        <td>
          147
        </td>
        <td>
          324
        </td>
        <td>
          5407
          -
          5412
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          false
        </td>
      </tr><tr>
        <td>
          151
        </td>
        <td>
          332
        </td>
        <td>
          5467
          -
          5491
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.System.currentTimeMillis
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          java.lang.System.currentTimeMillis()
        </td>
      </tr><tr>
        <td>
          164
        </td>
        <td>
          333
        </td>
        <td>
          5935
          -
          5963
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.SQLContext.conf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          sparkSession.sqlContext.conf
        </td>
      </tr><tr>
        <td>
          165
        </td>
        <td>
          337
        </td>
        <td>
          5985
          -
          6126
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.Strategy.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new QualitySparkUtils.this.Strategy(conf.analyzerMaxIterations, true, org.apache.spark.sql.internal.SQLConf.ANALYZER_MAX_ITERATIONS.key)
        </td>
      </tr><tr>
        <td>
          166
        </td>
        <td>
          334
        </td>
        <td>
          6005
          -
          6031
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.internal.SQLConf.analyzerMaxIterations
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          conf.analyzerMaxIterations
        </td>
      </tr><tr>
        <td>
          167
        </td>
        <td>
          335
        </td>
        <td>
          6055
          -
          6059
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          168
        </td>
        <td>
          336
        </td>
        <td>
          6090
          -
          6125
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.internal.config.ConfigEntry.key
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.internal.SQLConf.ANALYZER_MAX_ITERATIONS.key
        </td>
      </tr><tr>
        <td>
          172
        </td>
        <td>
          340
        </td>
        <td>
          6155
          -
          7642
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.Batch.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          QualitySparkUtils.this.Batch.apply(&quot;Resolution&quot;, fixedPoint, ({
  &lt;synthetic&gt; &lt;artifact&gt; val x$42: analyzer.ResolveNamespace = analyzer.ResolveNamespace.apply(analyzer.catalogManager);
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$41: org.apache.spark.sql.catalyst.analysis.ResolveCatalogs = new org.apache.spark.sql.catalyst.analysis.ResolveCatalogs(analyzer.catalogManager);
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$40: analyzer.ResolveUserSpecifiedColumns.type = analyzer.ResolveUserSpecifiedColumns;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$39: analyzer.ResolveInsertInto.type = analyzer.ResolveInsertInto;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$38: analyzer.ResolveRelations.type = analyzer.ResolveRelations;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$37: analyzer.ResolveTables.type = analyzer.ResolveTables;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$36: org.apache.spark.sql.catalyst.analysis.ResolvePartitionSpec.type = org.apache.spark.sql.catalyst.analysis.ResolvePartitionSpec;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$35: analyzer.ResolveAlterTableCommands.type = analyzer.ResolveAlterTableCommands;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$34: analyzer.AddMetadataColumns.type = analyzer.AddMetadataColumns;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$33: org.apache.spark.sql.catalyst.analysis.DeduplicateRelations.type = org.apache.spark.sql.catalyst.analysis.DeduplicateRelations;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$32: analyzer.ResolveReferences.type = analyzer.ResolveReferences;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$31: org.apache.spark.sql.catalyst.analysis.ResolveExpressionsWithNamePlaceholders.type = org.apache.spark.sql.catalyst.analysis.ResolveExpressionsWithNamePlaceholders;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$30: analyzer.ResolveDeserializer.type = analyzer.ResolveDeserializer;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$29: analyzer.ResolveNewInstance.type = analyzer.ResolveNewInstance;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$28: analyzer.ResolveUpCast.type = analyzer.ResolveUpCast;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$27: analyzer.ResolveGroupingAnalytics.type = analyzer.ResolveGroupingAnalytics;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$26: analyzer.ResolvePivot.type = analyzer.ResolvePivot;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$25: analyzer.ResolveOrdinalInOrderByAndGroupBy.type = analyzer.ResolveOrdinalInOrderByAndGroupBy;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$24: analyzer.ResolveAggAliasInGroupBy.type = analyzer.ResolveAggAliasInGroupBy;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$23: analyzer.ResolveMissingReferences.type = analyzer.ResolveMissingReferences;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$22: analyzer.ExtractGenerator.type = analyzer.ExtractGenerator;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$21: analyzer.ResolveGenerate.type = analyzer.ResolveGenerate;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$20: analyzer.ResolveFunctions.type = analyzer.ResolveFunctions;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$19: analyzer.ResolveAliases.type = analyzer.ResolveAliases;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$18: analyzer.ResolveSubquery.type = analyzer.ResolveSubquery;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$17: analyzer.ResolveSubqueryColumnAliases.type = analyzer.ResolveSubqueryColumnAliases;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$16: analyzer.ResolveWindowOrder.type = analyzer.ResolveWindowOrder;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$15: analyzer.ResolveWindowFrame.type = analyzer.ResolveWindowFrame;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$14: analyzer.ResolveNaturalAndUsingJoin.type = analyzer.ResolveNaturalAndUsingJoin;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$13: analyzer.ResolveOutputRelation.type = analyzer.ResolveOutputRelation;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$12: analyzer.ExtractWindowExpressions.type = analyzer.ExtractWindowExpressions;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$11: analyzer.GlobalAggregates.type = analyzer.GlobalAggregates;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$10: analyzer.ResolveAggregateFunctions.type = analyzer.ResolveAggregateFunctions;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$9: org.apache.spark.sql.catalyst.analysis.TimeWindowing.type = org.apache.spark.sql.catalyst.analysis.TimeWindowing;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$8: org.apache.spark.sql.catalyst.analysis.SessionWindowing.type = org.apache.spark.sql.catalyst.analysis.SessionWindowing;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$7: org.apache.spark.sql.catalyst.analysis.ResolveInlineTables.type = org.apache.spark.sql.catalyst.analysis.ResolveInlineTables;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$6: org.apache.spark.sql.catalyst.analysis.ResolveHigherOrderFunctions = org.apache.spark.sql.catalyst.analysis.ResolveHigherOrderFunctions.apply(analyzer.catalogManager);
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$5: org.apache.spark.sql.catalyst.analysis.ResolveLambdaVariables.type = org.apache.spark.sql.catalyst.analysis.ResolveLambdaVariables;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$4: org.apache.spark.sql.catalyst.analysis.ResolveTimeZone.type = org.apache.spark.sql.catalyst.analysis.ResolveTimeZone;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$3: analyzer.ResolveRandomSeed.type = analyzer.ResolveRandomSeed;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$2: analyzer.ResolveBinaryArithmetic.type = analyzer.ResolveBinaryArithmetic;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$1: org.apache.spark.sql.catalyst.analysis.ResolveUnion.type = org.apache.spark.sql.catalyst.analysis.ResolveUnion;
  org.apache.spark.sql.catalyst.analysis.TypeCoercion.typeCoercionRules.++[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan], List[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]]](scala.collection.Seq.apply[org.apache.spark.sql.catalyst.analysis.ResolveWithCTE.type](org.apache.spark.sql.catalyst.analysis.ResolveWithCTE))(immutable.this.List.canBuildFrom[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]]).::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$1)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$2)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$3)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$4)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$5)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$6)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$7)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$8)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$9)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$10)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$11)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$12)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$13)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$14)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$15)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$16)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$17)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$18)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$19)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$20)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$21)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$22)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$23)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$24)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$25)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$26)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$27)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$28)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$29)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$30)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$31)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$32)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$33)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$34)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$35)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$36)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$37)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$38)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$39)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$40)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$41)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$42)
}: _*))
        </td>
      </tr><tr>
        <td>
          172
        </td>
        <td>
          338
        </td>
        <td>
          6161
          -
          6173
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;Resolution&quot;
        </td>
      </tr><tr>
        <td>
          173
        </td>
        <td>
          339
        </td>
        <td>
          6193
          -
          7637
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.immutable.List.::
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  &lt;synthetic&gt; &lt;artifact&gt; val x$41: org.apache.spark.sql.catalyst.analysis.ResolveCatalogs = new org.apache.spark.sql.catalyst.analysis.ResolveCatalogs(analyzer.catalogManager);
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$40: analyzer.ResolveUserSpecifiedColumns.type = analyzer.ResolveUserSpecifiedColumns;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$39: analyzer.ResolveInsertInto.type = analyzer.ResolveInsertInto;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$38: analyzer.ResolveRelations.type = analyzer.ResolveRelations;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$37: analyzer.ResolveTables.type = analyzer.ResolveTables;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$36: org.apache.spark.sql.catalyst.analysis.ResolvePartitionSpec.type = org.apache.spark.sql.catalyst.analysis.ResolvePartitionSpec;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$35: analyzer.ResolveAlterTableCommands.type = analyzer.ResolveAlterTableCommands;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$34: analyzer.AddMetadataColumns.type = analyzer.AddMetadataColumns;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$33: org.apache.spark.sql.catalyst.analysis.DeduplicateRelations.type = org.apache.spark.sql.catalyst.analysis.DeduplicateRelations;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$32: analyzer.ResolveReferences.type = analyzer.ResolveReferences;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$31: org.apache.spark.sql.catalyst.analysis.ResolveExpressionsWithNamePlaceholders.type = org.apache.spark.sql.catalyst.analysis.ResolveExpressionsWithNamePlaceholders;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$30: analyzer.ResolveDeserializer.type = analyzer.ResolveDeserializer;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$29: analyzer.ResolveNewInstance.type = analyzer.ResolveNewInstance;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$28: analyzer.ResolveUpCast.type = analyzer.ResolveUpCast;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$27: analyzer.ResolveGroupingAnalytics.type = analyzer.ResolveGroupingAnalytics;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$26: analyzer.ResolvePivot.type = analyzer.ResolvePivot;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$25: analyzer.ResolveOrdinalInOrderByAndGroupBy.type = analyzer.ResolveOrdinalInOrderByAndGroupBy;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$24: analyzer.ResolveAggAliasInGroupBy.type = analyzer.ResolveAggAliasInGroupBy;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$23: analyzer.ResolveMissingReferences.type = analyzer.ResolveMissingReferences;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$22: analyzer.ExtractGenerator.type = analyzer.ExtractGenerator;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$21: analyzer.ResolveGenerate.type = analyzer.ResolveGenerate;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$20: analyzer.ResolveFunctions.type = analyzer.ResolveFunctions;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$19: analyzer.ResolveAliases.type = analyzer.ResolveAliases;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$18: analyzer.ResolveSubquery.type = analyzer.ResolveSubquery;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$17: analyzer.ResolveSubqueryColumnAliases.type = analyzer.ResolveSubqueryColumnAliases;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$16: analyzer.ResolveWindowOrder.type = analyzer.ResolveWindowOrder;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$15: analyzer.ResolveWindowFrame.type = analyzer.ResolveWindowFrame;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$14: analyzer.ResolveNaturalAndUsingJoin.type = analyzer.ResolveNaturalAndUsingJoin;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$13: analyzer.ResolveOutputRelation.type = analyzer.ResolveOutputRelation;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$12: analyzer.ExtractWindowExpressions.type = analyzer.ExtractWindowExpressions;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$11: analyzer.GlobalAggregates.type = analyzer.GlobalAggregates;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$10: analyzer.ResolveAggregateFunctions.type = analyzer.ResolveAggregateFunctions;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$9: org.apache.spark.sql.catalyst.analysis.TimeWindowing.type = org.apache.spark.sql.catalyst.analysis.TimeWindowing;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$8: org.apache.spark.sql.catalyst.analysis.SessionWindowing.type = org.apache.spark.sql.catalyst.analysis.SessionWindowing;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$7: org.apache.spark.sql.catalyst.analysis.ResolveInlineTables.type = org.apache.spark.sql.catalyst.analysis.ResolveInlineTables;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$6: org.apache.spark.sql.catalyst.analysis.ResolveHigherOrderFunctions = org.apache.spark.sql.catalyst.analysis.ResolveHigherOrderFunctions.apply(analyzer.catalogManager);
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$5: org.apache.spark.sql.catalyst.analysis.ResolveLambdaVariables.type = org.apache.spark.sql.catalyst.analysis.ResolveLambdaVariables;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$4: org.apache.spark.sql.catalyst.analysis.ResolveTimeZone.type = org.apache.spark.sql.catalyst.analysis.ResolveTimeZone;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$3: analyzer.ResolveRandomSeed.type = analyzer.ResolveRandomSeed;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$2: analyzer.ResolveBinaryArithmetic.type = analyzer.ResolveBinaryArithmetic;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$1: org.apache.spark.sql.catalyst.analysis.ResolveUnion.type = org.apache.spark.sql.catalyst.analysis.ResolveUnion;
  org.apache.spark.sql.catalyst.analysis.TypeCoercion.typeCoercionRules.++[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan], List[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]]](scala.collection.Seq.apply[org.apache.spark.sql.catalyst.analysis.ResolveWithCTE.type](org.apache.spark.sql.catalyst.analysis.ResolveWithCTE))(immutable.this.List.canBuildFrom[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]]).::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$1)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$2)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$3)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$4)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$5)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$6)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$7)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$8)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$9)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$10)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$11)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$12)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$13)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$14)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$15)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$16)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$17)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$18)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$19)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$20)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$21)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$22)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$23)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$24)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$25)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$26)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$27)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$28)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$29)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$30)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$31)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$32)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$33)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$34)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$35)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$36)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$37)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$38)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$39)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$40)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$41)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$42)
        </td>
      </tr><tr>
        <td>
          227
        </td>
        <td>
          341
        </td>
        <td>
          8100
          -
          8121
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.FakePlan.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          QualitySparkUtils.this.FakePlan.apply(expr, child)
        </td>
      </tr><tr>
        <td>
          229
        </td>
        <td>
          343
        </td>
        <td>
          8162
          -
          8223
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.execute
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          QualitySparkUtils.this.execute(newPlan, QualitySparkUtils.this.resolution(analyzer, sparkSession, newPlan))
        </td>
      </tr><tr>
        <td>
          229
        </td>
        <td>
          342
        </td>
        <td>
          8179
          -
          8222
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.resolution
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          QualitySparkUtils.this.resolution(analyzer, sparkSession, newPlan)
        </td>
      </tr><tr>
        <td>
          236
        </td>
        <td>
          344
        </td>
        <td>
          8423
          -
          8513
        </td>
        <td>
          Throw
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          throw new scala.`package`.Exception(scala.StringContext.apply(&quot;Could not resolve expression &quot;, &quot; with child &quot;, &quot;}&quot;).s(expr, child))
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          346
        </td>
        <td>
          8844
          -
          8844
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          345
        </td>
        <td>
          8845
          -
          8852
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.forExpr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          forExpr(expr)
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          347
        </td>
        <td>
          8829
          -
          8853
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          expressions.map[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]]({
  ((expr: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; forExpr(expr))
})(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression])
        </td>
      </tr><tr>
        <td>
          244
        </td>
        <td>
          349
        </td>
        <td>
          8862
          -
          9014
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleEngineRunner.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.RuleEngineRunner.apply(ruleSuite, com.sparkutils.quality.utils.PassThrough.apply(nexprs), realType, compileEvals, debugMode, func, group, forceRunnerEval, expressionOffsets, forceTriggerEval)
        </td>
      </tr><tr>
        <td>
          244
        </td>
        <td>
          348
        </td>
        <td>
          8890
          -
          8911
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.utils.PassThrough.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.utils.PassThrough.apply(nexprs)
        </td>
      </tr><tr>
        <td>
          247
        </td>
        <td>
          352
        </td>
        <td>
          9265
          -
          9289
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          expressions.map[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]]({
  ((expr: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; forExpr(expr))
})(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression])
        </td>
      </tr><tr>
        <td>
          247
        </td>
        <td>
          351
        </td>
        <td>
          9280
          -
          9280
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]
        </td>
      </tr><tr>
        <td>
          247
        </td>
        <td>
          350
        </td>
        <td>
          9281
          -
          9288
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.forExpr
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          forExpr(expr)
        </td>
      </tr><tr>
        <td>
          248
        </td>
        <td>
          354
        </td>
        <td>
          9298
          -
          9505
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleFolderRunner.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.sparkutils.quality.impl.RuleFolderRunner.apply(ruleSuite, left, com.sparkutils.quality.utils.PassThrough.apply(nexprs), resultDataType, compileEvals, debugMode, variablesPerFunc, variableFuncGroup, forceRunnerEval, expressionOffsets, dataRef, forceTriggerEval)
        </td>
      </tr><tr>
        <td>
          248
        </td>
        <td>
          353
        </td>
        <td>
          9332
          -
          9353
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.utils.PassThrough.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.sparkutils.quality.utils.PassThrough.apply(nexprs)
        </td>
      </tr><tr>
        <td>
          251
        </td>
        <td>
          355
        </td>
        <td>
          9655
          -
          9662
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.forExpr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          forExpr(expr)
        </td>
      </tr><tr>
        <td>
          251
        </td>
        <td>
          357
        </td>
        <td>
          9639
          -
          9663
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          expressions.map[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]]({
  ((expr: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; forExpr(expr))
})(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression])
        </td>
      </tr><tr>
        <td>
          251
        </td>
        <td>
          356
        </td>
        <td>
          9654
          -
          9654
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]
        </td>
      </tr><tr>
        <td>
          252
        </td>
        <td>
          358
        </td>
        <td>
          9694
          -
          9715
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.utils.PassThrough.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.utils.PassThrough.apply(nexprs)
        </td>
      </tr><tr>
        <td>
          252
        </td>
        <td>
          359
        </td>
        <td>
          9672
          -
          9760
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunner.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.RuleRunner.apply(ruleSuite, com.sparkutils.quality.utils.PassThrough.apply(nexprs), compileEvals, func, group, forceRunnerEval)
        </td>
      </tr><tr>
        <td>
          253
        </td>
        <td>
          360
        </td>
        <td>
          9777
          -
          9790
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.forExpr
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          forExpr(expr)
        </td>
      </tr><tr>
        <td>
          260
        </td>
        <td>
          361
        </td>
        <td>
          9929
          -
          9954
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.AttributeSeq.attrs
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          FakePlan.this.child.allAttributes.attrs
        </td>
      </tr><tr>
        <td>
          262
        </td>
        <td>
          362
        </td>
        <td>
          9997
          -
          10004
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Some.apply[Long](1L)
        </td>
      </tr><tr>
        <td>
          265
        </td>
        <td>
          363
        </td>
        <td>
          10127
          -
          10148
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.generic.ImmutableSetFactory.empty
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.Set.empty[org.apache.spark.sql.catalyst.expressions.Expression]
        </td>
      </tr><tr>
        <td>
          266
        </td>
        <td>
          378
        </td>
        <td>
          10155
          -
          10676
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.IterableLike.foreach
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          projectList.foreach[Unit](((x0$1: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; x0$1 match {
  case (a @ (child: org.apache.spark.sql.catalyst.expressions.Expression, name: String)(exprId: org.apache.spark.sql.catalyst.expressions.ExprId, qualifier: Seq[String], explicitMetadata: Option[org.apache.spark.sql.types.Metadata], nonInheritableMetadataKeys: Seq[String])org.apache.spark.sql.catalyst.expressions.Alias((l @ (_: org.apache.spark.sql.catalyst.expressions.Literal)), _)) =&gt; allConstraints = allConstraints.+(org.apache.spark.sql.catalyst.expressions.EqualNullSafe.apply(a.toAttribute, l))
  case (a @ (child: org.apache.spark.sql.catalyst.expressions.Expression, name: String)(exprId: org.apache.spark.sql.catalyst.expressions.ExprId, qualifier: Seq[String], explicitMetadata: Option[org.apache.spark.sql.types.Metadata], nonInheritableMetadataKeys: Seq[String])org.apache.spark.sql.catalyst.expressions.Alias((e @ _), _)) =&gt; {
    allConstraints = allConstraints.++(allConstraints.map[org.apache.spark.sql.catalyst.expressions.Expression, scala.collection.immutable.Set[org.apache.spark.sql.catalyst.expressions.Expression]](((x$43: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; x$43.transform(({
      @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
        def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
          $anonfun.super.&lt;init&gt;();
          ()
        };
        final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x1: A1, default: A1 =&gt; B1): B1 = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
          case (expr @ (_: org.apache.spark.sql.catalyst.expressions.Expression)) if expr.semanticEquals(e) =&gt; a.toAttribute
          case (defaultCase$ @ _) =&gt; default.apply(x1)
        };
        final def isDefinedAt(x1: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
          case (expr @ (_: org.apache.spark.sql.catalyst.expressions.Expression)) if expr.semanticEquals(e) =&gt; true
          case (defaultCase$ @ _) =&gt; false
        }
      };
      new $anonfun()
    }: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))))(immutable.this.Set.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]));
    allConstraints = allConstraints.+(org.apache.spark.sql.catalyst.expressions.EqualNullSafe.apply(e, a.toAttribute))
  }
  case _ =&gt; ()
}))
        </td>
      </tr><tr>
        <td>
          268
        </td>
        <td>
          364
        </td>
        <td>
          10258
          -
          10271
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Alias.toAttribute
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          a.toAttribute
        </td>
      </tr><tr>
        <td>
          268
        </td>
        <td>
          366
        </td>
        <td>
          10226
          -
          10275
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SetLike.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          allConstraints.+(org.apache.spark.sql.catalyst.expressions.EqualNullSafe.apply(a.toAttribute, l))
        </td>
      </tr><tr>
        <td>
          268
        </td>
        <td>
          365
        </td>
        <td>
          10244
          -
          10275
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.EqualNullSafe.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.EqualNullSafe.apply(a.toAttribute, l)
        </td>
      </tr><tr>
        <td>
          271
        </td>
        <td>
          373
        </td>
        <td>
          10418
          -
          10573
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SetLike.++
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          allConstraints.++(allConstraints.map[org.apache.spark.sql.catalyst.expressions.Expression, scala.collection.immutable.Set[org.apache.spark.sql.catalyst.expressions.Expression]](((x$43: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; x$43.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x1: A1, default: A1 =&gt; B1): B1 = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (expr @ (_: org.apache.spark.sql.catalyst.expressions.Expression)) if expr.semanticEquals(e) =&gt; a.toAttribute
      case (defaultCase$ @ _) =&gt; default.apply(x1)
    };
    final def isDefinedAt(x1: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (expr @ (_: org.apache.spark.sql.catalyst.expressions.Expression)) if expr.semanticEquals(e) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))))(immutable.this.Set.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          271
        </td>
        <td>
          370
        </td>
        <td>
          10456
          -
          10572
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.transform
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$43.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x1: A1, default: A1 =&gt; B1): B1 = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (expr @ (_: org.apache.spark.sql.catalyst.expressions.Expression)) if expr.semanticEquals(e) =&gt; a.toAttribute
      case (defaultCase$ @ _) =&gt; default.apply(x1)
    };
    final def isDefinedAt(x1: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (expr @ (_: org.apache.spark.sql.catalyst.expressions.Expression)) if expr.semanticEquals(e) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          271
        </td>
        <td>
          369
        </td>
        <td>
          10468
          -
          10468
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.FakePlan.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          271
        </td>
        <td>
          372
        </td>
        <td>
          10437
          -
          10573
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.SetLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          allConstraints.map[org.apache.spark.sql.catalyst.expressions.Expression, scala.collection.immutable.Set[org.apache.spark.sql.catalyst.expressions.Expression]](((x$43: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; x$43.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x1: A1, default: A1 =&gt; B1): B1 = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (expr @ (_: org.apache.spark.sql.catalyst.expressions.Expression)) if expr.semanticEquals(e) =&gt; a.toAttribute
      case (defaultCase$ @ _) =&gt; default.apply(x1)
    };
    final def isDefinedAt(x1: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (expr @ (_: org.apache.spark.sql.catalyst.expressions.Expression)) if expr.semanticEquals(e) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))))(immutable.this.Set.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression])
        </td>
      </tr><tr>
        <td>
          271
        </td>
        <td>
          371
        </td>
        <td>
          10455
          -
          10455
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.Set.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          immutable.this.Set.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]
        </td>
      </tr><tr>
        <td>
          272
        </td>
        <td>
          367
        </td>
        <td>
          10507
          -
          10529
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.semanticEquals
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          expr.semanticEquals(e)
        </td>
      </tr><tr>
        <td>
          273
        </td>
        <td>
          368
        </td>
        <td>
          10547
          -
          10560
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Alias.toAttribute
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          a.toAttribute
        </td>
      </tr><tr>
        <td>
          275
        </td>
        <td>
          376
        </td>
        <td>
          10584
          -
          10633
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SetLike.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          allConstraints.+(org.apache.spark.sql.catalyst.expressions.EqualNullSafe.apply(e, a.toAttribute))
        </td>
      </tr><tr>
        <td>
          275
        </td>
        <td>
          375
        </td>
        <td>
          10602
          -
          10633
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.EqualNullSafe.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.EqualNullSafe.apply(e, a.toAttribute)
        </td>
      </tr><tr>
        <td>
          275
        </td>
        <td>
          374
        </td>
        <td>
          10619
          -
          10632
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Alias.toAttribute
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          a.toAttribute
        </td>
      </tr><tr>
        <td>
          276
        </td>
        <td>
          377
        </td>
        <td>
          10649
          -
          10651
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          284
        </td>
        <td>
          379
        </td>
        <td>
          10891
          -
          10891
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.FakePlan.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          FakePlan.this.copy$default$1
        </td>
      </tr><tr>
        <td>
          284
        </td>
        <td>
          380
        </td>
        <td>
          10891
          -
          10913
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.FakePlan.copy
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          FakePlan.this.copy(x$2, x$1)
        </td>
      </tr><tr>
        <td>
          293
        </td>
        <td>
          381
        </td>
        <td>
          11067
          -
          11087
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.execution.SparkSqlParser.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.sql.execution.SparkSqlParser()
        </td>
      </tr><tr>
        <td>
          304
        </td>
        <td>
          382
        </td>
        <td>
          11436
          -
          11498
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.FunctionRegistryBase.createOrReplaceTempFunction
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          funcReg.createOrReplaceTempFunction(name, builder, &quot;built-in&quot;)
        </td>
      </tr><tr>
        <td>
          307
        </td>
        <td>
          385
        </td>
        <td>
          11644
          -
          11663
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.ShowParams.vertical
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          showParams.vertical
        </td>
      </tr><tr>
        <td>
          307
        </td>
        <td>
          384
        </td>
        <td>
          11623
          -
          11642
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.ShowParams.truncate
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          showParams.truncate
        </td>
      </tr><tr>
        <td>
          307
        </td>
        <td>
          383
        </td>
        <td>
          11603
          -
          11621
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.ShowParams.numRows
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          showParams.numRows
        </td>
      </tr><tr>
        <td>
          307
        </td>
        <td>
          386
        </td>
        <td>
          11582
          -
          11664
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.Dataset.showString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          dataFrame.showString(showParams.numRows, showParams.truncate, showParams.vertical)
        </td>
      </tr><tr>
        <td>
          316
        </td>
        <td>
          388
        </td>
        <td>
          11973
          -
          12003
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.ExpressionInfo.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.sql.catalyst.expressions.ExpressionInfo(name, name)
        </td>
      </tr><tr>
        <td>
          316
        </td>
        <td>
          387
        </td>
        <td>
          11947
          -
          11971
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.FunctionIdentifier.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.FunctionIdentifier.apply(name)
        </td>
      </tr><tr>
        <td>
          316
        </td>
        <td>
          390
        </td>
        <td>
          11919
          -
          12016
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.SparkSessionExtensions.injectFunction
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          extensions.injectFunction(scala.Tuple3.apply[org.apache.spark.sql.catalyst.FunctionIdentifier, org.apache.spark.sql.catalyst.expressions.ExpressionInfo, Seq[org.apache.spark.sql.catalyst.expressions.Expression] =&gt; org.apache.spark.sql.catalyst.expressions.Expression](org.apache.spark.sql.catalyst.FunctionIdentifier.apply(name), new org.apache.spark.sql.catalyst.expressions.ExpressionInfo(name, name), builder))
        </td>
      </tr><tr>
        <td>
          316
        </td>
        <td>
          389
        </td>
        <td>
          11946
          -
          12014
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple3.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple3.apply[org.apache.spark.sql.catalyst.FunctionIdentifier, org.apache.spark.sql.catalyst.expressions.ExpressionInfo, Seq[org.apache.spark.sql.catalyst.expressions.Expression] =&gt; org.apache.spark.sql.catalyst.expressions.Expression](org.apache.spark.sql.catalyst.FunctionIdentifier.apply(name), new org.apache.spark.sql.catalyst.expressions.ExpressionInfo(name, name), builder)
        </td>
      </tr>
    </table>
          </div>
        </div>
      </body>
    </html>