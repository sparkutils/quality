<html>
      <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <title id="title">
          org/apache/spark/sql/QualitySparkUtils.scala.html
        </title>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/css/theme.default.min.css" type="text/css"/><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/js/jquery.tablesorter.min.js"></script><link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css" type="text/css"/><script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script><script type="text/javascript">
        $(document).ready(function() {$(".tablesorter").tablesorter();});
      </script>
        <style>
          table.codegrid { font-family: monospace; font-size: 12px; width: auto!important; }table.statementlist { width: auto!important; font-size: 13px; } table.codegrid td { padding: 0!important; border: 0!important } table td.linenumber { width: 40px!important; } 
        </style>
      </head>
      <body style="font-family: monospace;">
        <ul class="nav nav-tabs">
          <li>
            <a href="#codegrid" data-toggle="tab">Codegrid</a>
          </li>
          <li>
            <a href="#statementlist" data-toggle="tab">Statement List</a>
          </li>
        </ul>
        <div class="tab-content">
          <div class="tab-pane active" id="codegrid">
            <pre style='font-size: 12pt; font-family: courier, monospace;'>1 <span style=''>package org.apache.spark.sql
</span>2 <span style=''>
</span>3 <span style=''>import java.util.Locale
</span>4 <span style=''>
</span>5 <span style=''>import com.sparkutils.quality.impl.{RuleEngineRunner, RuleFolderRunner, RuleRunner, ShowParams}
</span>6 <span style=''>import com.sparkutils.quality.{RuleSuite, debugTime}
</span>7 <span style=''>import com.sparkutils.quality.utils.PassThrough
</span>8 <span style=''>import org.apache.spark.sql.catalyst.{CatalystTypeConverters, FunctionIdentifier}
</span>9 <span style=''>import org.apache.spark.sql.catalyst.analysis.{Analyzer, DeduplicateRelations, FunctionRegistry, ResolveCatalogs, ResolveExpressionsWithNamePlaceholders, ResolveHigherOrderFunctions, ResolveInlineTables, ResolveLambdaVariables, ResolvePartitionSpec, ResolveTimeZone, ResolveUnion, ResolveWithCTE, SessionWindowing, TimeWindowing, TypeCheckResult, TypeCoercion, UnresolvedFunction}
</span>10 <span style=''>import org.apache.spark.sql.catalyst.catalog.SessionCatalog
</span>11 <span style=''>import org.apache.spark.sql.catalyst.expressions.{Add, Alias, Attribute, BindReferences, Cast, EqualNullSafe, Expression, ExpressionInfo, ExpressionSet, GetArrayStructFields, GetStructField, LambdaFunction, Literal, PrettyAttribute}
</span>12 <span style=''>import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, UnaryNode}
</span>13 <span style=''>import org.apache.spark.sql.catalyst.rules.Rule
</span>14 <span style=''>import org.apache.spark.sql.catalyst.trees.TreeNode
</span>15 <span style=''>import org.apache.spark.sql.execution.SparkSqlParser
</span>16 <span style=''>import org.apache.spark.sql.internal.SQLConf
</span>17 <span style=''>import org.apache.spark.sql.qualityFunctions.{Digest, InterpretedHashLongsFunction}
</span>18 <span style=''>import org.apache.spark.sql.types._
</span>19 <span style=''>import org.apache.spark.unsafe.types.{CalendarInterval, UTF8String}
</span>20 <span style=''>import org.apache.spark.util.Utils
</span>21 <span style=''>
</span>22 <span style=''>/**
</span>23 <span style=''> * Set of utilities to reach in to private functions
</span>24 <span style=''> */
</span>25 <span style=''>object QualitySparkUtils {
</span>26 <span style=''>
</span>27 <span style=''>  implicit class UnresolvedFunctionOps(unresolvedFunction: UnresolvedFunction) {
</span>28 <span style=''>
</span>29 <span style=''>    def theArguments: Seq[Expression] =
</span>30 <span style=''>      </span><span style='background: #AEF1AE'>unresolvedFunction.arguments</span><span style=''>
</span>31 <span style=''>
</span>32 <span style=''>    def withArguments(children: Seq[Expression]): UnresolvedFunction =
</span>33 <span style=''>      </span><span style='background: #AEF1AE'>unresolvedFunction.copy(arguments = children)</span><span style=''>
</span>34 <span style=''>  }
</span>35 <span style=''>
</span>36 <span style=''>  def isPrimitive(dataType: DataType) = </span><span style='background: #AEF1AE'>CatalystTypeConverters.isPrimitive(dataType)</span><span style=''>
</span>37 <span style=''>
</span>38 <span style=''>  /**
</span>39 <span style=''>   * Where resolveWith is not possible (e.g. 10.x DBRs) it is disabled here.
</span>40 <span style=''>   * This is, in the 10.x DBR case, due to the class files for UnaryNode (FakePlan) being radically different and causing an IncompatibleClassChangeError: Implementing class
</span>41 <span style=''>   * @param orig
</span>42 <span style=''>   * @return
</span>43 <span style=''>   */
</span>44 <span style=''>  def resolveWithOverride(orig: Option[DataFrame]): Option[DataFrame] =
</span>45 <span style=''>    orig
</span>46 <span style=''>
</span>47 <span style=''>  /**
</span>48 <span style=''>   * Dbr 11.2 broke the contract for add and cast
</span>49 <span style=''>   * @param left
</span>50 <span style=''>   * @param right
</span>51 <span style=''>   * @return
</span>52 <span style=''>   */
</span>53 <span style=''>  def add(left: Expression, right: Expression, dataType: DataType): Expression =
</span>54 <span style=''>    </span><span style='background: #AEF1AE'>Add(left, right)</span><span style=''>
</span>55 <span style=''>
</span>56 <span style=''>  /**
</span>57 <span style=''>   * Dbr 11.2 broke the contract for add and cast
</span>58 <span style=''>   * @param child
</span>59 <span style=''>   * @param dataType
</span>60 <span style=''>   * @return
</span>61 <span style=''>   */
</span>62 <span style=''>  def cast(child: Expression, dataType: DataType): Expression =
</span>63 <span style=''>    </span><span style='background: #AEF1AE'>Cast(child, dataType)</span><span style=''>
</span>64 <span style=''>
</span>65 <span style=''>  /**
</span>66 <span style=''>   * Arguments for everything above 2.4
</span>67 <span style=''>   */
</span>68 <span style=''>  def arguments(unresolvedFunction: UnresolvedFunction): Seq[Expression] =
</span>69 <span style=''>    </span><span style='background: #AEF1AE'>unresolvedFunction.arguments</span><span style=''>
</span>70 <span style=''>
</span>71 <span style=''>  /**
</span>72 <span style=''>   * Provides Spark 3 specific version of hashing CalendarInterval
</span>73 <span style=''>   *
</span>74 <span style=''>   * @param c
</span>75 <span style=''>   * @param hashlongs
</span>76 <span style=''>   * @param digest
</span>77 <span style=''>   * @return
</span>78 <span style=''>   */
</span>79 <span style=''>  def hashCalendarInterval(c: CalendarInterval, hashlongs: InterpretedHashLongsFunction, digest: Digest): Digest = {
</span>80 <span style=''>    import hashlongs._
</span>81 <span style=''>    </span><span style='background: #F0ADAD'>hashInt(c.months, hashInt(
</span>82 <span style=''></span><span style='background: #F0ADAD'>      c.days
</span>83 <span style=''></span><span style='background: #F0ADAD'>      , hashLong(c.microseconds, digest)))</span><span style=''>
</span>84 <span style=''>  }
</span>85 <span style=''>
</span>86 <span style=''>  /**
</span>87 <span style=''>   * Resolves expressions against a dataframe, this allows them to be swapped out after name checking - spark cannot then
</span>88 <span style=''>   * simply optimise the tree so certain things like constant folding etc. won't show up.
</span>89 <span style=''>   *
</span>90 <span style=''>   * @param dataFrame resolve's must be against a given dataframe to keep names matching
</span>91 <span style=''>   * @param expr      the expression to resolve
</span>92 <span style=''>   */
</span>93 <span style=''>  def resolveExpression(dataFrame: DataFrame, expr: Expression): Expression = {
</span>94 <span style=''>
</span>95 <span style=''>    val sparkSession = </span><span style='background: #AEF1AE'>SparkSession.getActiveSession.get</span><span style=''>
</span>96 <span style=''>
</span>97 <span style=''>    val plan = </span><span style='background: #AEF1AE'>dataFrame.select(&quot;*&quot;).logicalPlan</span><span style=''> // select * needed for toDF's etc. from dataset to force evaluation of the attributes
</span>98 <span style=''>    val res = </span><span style='background: #AEF1AE'>debugTime(&quot;tryResolveReferences&quot;) {
</span>99 <span style=''></span><span style='background: #AEF1AE'>      tryResolveReferences(sparkSession)(expr, plan)
</span>100 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>101 <span style=''>
</span>102 <span style=''>    val fres = </span><span style='background: #AEF1AE'>debugTime(&quot;bindReferences&quot;) {
</span>103 <span style=''></span><span style='background: #AEF1AE'>      BindReferences.bindReference(res, plan.allAttributes)
</span>104 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>105 <span style=''>
</span>106 <span style=''>    fres
</span>107 <span style=''>  }
</span>108 <span style=''>
</span>109 <span style=''>  def execute(logicalPlan: LogicalPlan, batch: Batch) = {
</span>110 <span style=''>    var iteration = </span><span style='background: #AEF1AE'>1</span><span style=''>
</span>111 <span style=''>    var curPlan = logicalPlan
</span>112 <span style=''>    var lastPlan = logicalPlan
</span>113 <span style=''>
</span>114 <span style=''>    var start = </span><span style='background: #AEF1AE'>System.currentTimeMillis</span><span style=''>
</span>115 <span style=''>
</span>116 <span style=''>
</span>117 <span style=''>    var continue = </span><span style='background: #AEF1AE'>true</span><span style=''>
</span>118 <span style=''>    val analyzer = SparkSession.getActiveSession.get.sessionState.analyzer
</span>119 <span style=''>
</span>120 <span style=''>    // Run until fix point (or the max number of iterations as specified in the strategy.
</span>121 <span style=''>    while (continue) </span><span style='background: #AEF1AE'>{
</span>122 <span style=''></span><span style='background: #AEF1AE'>      curPlan = batch.rules.foldLeft(curPlan) {
</span>123 <span style=''></span><span style='background: #AEF1AE'>        case (plan, rule) =&gt;
</span>124 <span style=''></span><span style='background: #AEF1AE'>          val startTime = System.nanoTime()
</span>125 <span style=''></span><span style='background: #AEF1AE'>          val result = rule(plan)
</span>126 <span style=''></span><span style='background: #AEF1AE'>
</span>127 <span style=''></span><span style='background: #AEF1AE'>          result
</span>128 <span style=''></span><span style='background: #AEF1AE'>      }
</span>129 <span style=''></span><span style='background: #AEF1AE'>      iteration += 1
</span>130 <span style=''></span><span style='background: #AEF1AE'>      if (iteration &gt; batch.strategy.maxIterations) </span><span style='background: #F0ADAD'>{
</span>131 <span style=''></span><span style='background: #F0ADAD'>        // Only log if this is a rule that is supposed to run more than once.
</span>132 <span style=''></span><span style='background: #F0ADAD'>        if (iteration != 2) {
</span>133 <span style=''></span><span style='background: #F0ADAD'>          val endingMsg = if (batch.strategy.maxIterationsSetting == null) {
</span>134 <span style=''></span><span style='background: #F0ADAD'>            &quot;.&quot;
</span>135 <span style=''></span><span style='background: #F0ADAD'>          } else {
</span>136 <span style=''></span><span style='background: #F0ADAD'>            s&quot;, please set '${batch.strategy.maxIterationsSetting}' to a larger value.&quot;
</span>137 <span style=''></span><span style='background: #F0ADAD'>          }
</span>138 <span style=''></span><span style='background: #F0ADAD'>          val message = s&quot;Max iterations (${iteration - 1}) reached for batch ${batch.name}&quot; +
</span>139 <span style=''></span><span style='background: #F0ADAD'>            s&quot;$endingMsg&quot;
</span>140 <span style=''></span><span style='background: #F0ADAD'>          if (Utils.isTesting || batch.strategy.errorOnExceed) {
</span>141 <span style=''></span><span style='background: #F0ADAD'>            throw new Exception(message)
</span>142 <span style=''></span><span style='background: #F0ADAD'>          } else {
</span>143 <span style=''></span><span style='background: #F0ADAD'>          }
</span>144 <span style=''></span><span style='background: #F0ADAD'>        }
</span>145 <span style=''></span><span style='background: #F0ADAD'>        continue = false
</span>146 <span style=''></span><span style='background: #F0ADAD'>      }</span><span style='background: #AEF1AE'>
</span>147 <span style=''></span><span style='background: #AEF1AE'>
</span>148 <span style=''></span><span style='background: #AEF1AE'>      if (curPlan.fastEquals(lastPlan)) {
</span>149 <span style=''></span><span style='background: #AEF1AE'>        continue = false
</span>150 <span style=''></span><span style='background: #AEF1AE'>      }
</span>151 <span style=''></span><span style='background: #AEF1AE'>      lastPlan = curPlan
</span>152 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>153 <span style=''>    var stop = </span><span style='background: #AEF1AE'>System.currentTimeMillis</span><span style=''>
</span>154 <span style=''>    //println(s&quot;did $iteration iterations to execute the plan in ${stop-start}ms&quot;)
</span>155 <span style=''>    curPlan
</span>156 <span style=''>  }
</span>157 <span style=''>
</span>158 <span style=''>  case class Strategy(
</span>159 <span style=''>                       maxIterations: Int, errorOnExceed: Boolean = false, maxIterationsSetting: String = null
</span>160 <span style=''>                     )
</span>161 <span style=''>
</span>162 <span style=''>  case class Batch(name: String, strategy: Strategy, rules: Rule[LogicalPlan]*)
</span>163 <span style=''>
</span>164 <span style=''>
</span>165 <span style=''>  def resolution(analyzer: Analyzer, sparkSession: SparkSession, plan: LogicalPlan) = {
</span>166 <span style=''>    val conf = </span><span style='background: #AEF1AE'>sparkSession.sqlContext.conf</span><span style=''>
</span>167 <span style=''>    val fixedPoint = </span><span style='background: #AEF1AE'>new Strategy(
</span>168 <span style=''></span><span style='background: #AEF1AE'>      conf.analyzerMaxIterations,
</span>169 <span style=''></span><span style='background: #AEF1AE'>      errorOnExceed = true,
</span>170 <span style=''></span><span style='background: #AEF1AE'>      maxIterationsSetting = SQLConf.ANALYZER_MAX_ITERATIONS.key)</span><span style=''>
</span>171 <span style=''>
</span>172 <span style=''>    import analyzer._
</span>173 <span style=''>
</span>174 <span style=''>    </span><span style='background: #AEF1AE'>Batch(&quot;Resolution&quot;, fixedPoint,
</span>175 <span style=''></span><span style='background: #AEF1AE'>      ResolveNamespace(catalogManager) ::
</span>176 <span style=''></span><span style='background: #AEF1AE'>        new ResolveCatalogs(catalogManager) ::
</span>177 <span style=''></span><span style='background: #AEF1AE'>        ResolveUserSpecifiedColumns ::
</span>178 <span style=''></span><span style='background: #AEF1AE'>        ResolveInsertInto ::
</span>179 <span style=''></span><span style='background: #AEF1AE'>        ResolveRelations ::
</span>180 <span style=''></span><span style='background: #AEF1AE'>        ResolveTables ::
</span>181 <span style=''></span><span style='background: #AEF1AE'>        ResolvePartitionSpec ::
</span>182 <span style=''></span><span style='background: #AEF1AE'>        ResolveAlterTableCommands ::
</span>183 <span style=''></span><span style='background: #AEF1AE'>        AddMetadataColumns ::
</span>184 <span style=''></span><span style='background: #AEF1AE'>        DeduplicateRelations ::
</span>185 <span style=''></span><span style='background: #AEF1AE'>        ResolveReferences ::
</span>186 <span style=''></span><span style='background: #AEF1AE'>        ResolveExpressionsWithNamePlaceholders ::
</span>187 <span style=''></span><span style='background: #AEF1AE'>        ResolveDeserializer ::
</span>188 <span style=''></span><span style='background: #AEF1AE'>        ResolveNewInstance ::
</span>189 <span style=''></span><span style='background: #AEF1AE'>        ResolveUpCast ::
</span>190 <span style=''></span><span style='background: #AEF1AE'>        ResolveGroupingAnalytics ::
</span>191 <span style=''></span><span style='background: #AEF1AE'>        ResolvePivot ::
</span>192 <span style=''></span><span style='background: #AEF1AE'>        ResolveOrdinalInOrderByAndGroupBy ::
</span>193 <span style=''></span><span style='background: #AEF1AE'>        ResolveAggAliasInGroupBy ::
</span>194 <span style=''></span><span style='background: #AEF1AE'>        ResolveMissingReferences ::
</span>195 <span style=''></span><span style='background: #AEF1AE'>        ExtractGenerator ::
</span>196 <span style=''></span><span style='background: #AEF1AE'>        ResolveGenerate ::
</span>197 <span style=''></span><span style='background: #AEF1AE'>        ResolveFunctions ::
</span>198 <span style=''></span><span style='background: #AEF1AE'>        ResolveAliases ::
</span>199 <span style=''></span><span style='background: #AEF1AE'>        ResolveSubquery ::
</span>200 <span style=''></span><span style='background: #AEF1AE'>        ResolveSubqueryColumnAliases ::
</span>201 <span style=''></span><span style='background: #AEF1AE'>        ResolveWindowOrder ::
</span>202 <span style=''></span><span style='background: #AEF1AE'>        ResolveWindowFrame ::
</span>203 <span style=''></span><span style='background: #AEF1AE'>        ResolveNaturalAndUsingJoin ::
</span>204 <span style=''></span><span style='background: #AEF1AE'>        ResolveOutputRelation ::
</span>205 <span style=''></span><span style='background: #AEF1AE'>        ExtractWindowExpressions ::
</span>206 <span style=''></span><span style='background: #AEF1AE'>        GlobalAggregates ::
</span>207 <span style=''></span><span style='background: #AEF1AE'>        ResolveAggregateFunctions ::
</span>208 <span style=''></span><span style='background: #AEF1AE'>        TimeWindowing ::
</span>209 <span style=''></span><span style='background: #AEF1AE'>        SessionWindowing ::
</span>210 <span style=''></span><span style='background: #AEF1AE'>        ResolveInlineTables ::
</span>211 <span style=''></span><span style='background: #AEF1AE'>        ResolveHigherOrderFunctions(catalogManager) ::
</span>212 <span style=''></span><span style='background: #AEF1AE'>        ResolveLambdaVariables ::
</span>213 <span style=''></span><span style='background: #AEF1AE'>        ResolveTimeZone ::
</span>214 <span style=''></span><span style='background: #AEF1AE'>        ResolveRandomSeed ::
</span>215 <span style=''></span><span style='background: #AEF1AE'>        ResolveBinaryArithmetic ::
</span>216 <span style=''></span><span style='background: #AEF1AE'>        ResolveUnion ::
</span>217 <span style=''></span><span style='background: #AEF1AE'>        TypeCoercion.typeCoercionRules ++
</span>218 <span style=''></span><span style='background: #AEF1AE'>          Seq(ResolveWithCTE): _*)</span><span style=''>
</span>219 <span style=''>  }
</span>220 <span style=''>
</span>221 <span style=''>  // below based on approach from delta / discussed with Alex to use a Project, LeafNode should be fine
</span>222 <span style=''>  protected def tryResolveReferences(
</span>223 <span style=''>                                      sparkSession: SparkSession)(
</span>224 <span style=''>                                      expr: Expression,
</span>225 <span style=''>                                      child: LogicalPlan): Expression = {
</span>226 <span style=''>    val analyzer = sparkSession.sessionState.analyzer
</span>227 <span style=''>
</span>228 <span style=''>    def forExpr(expr: Expression) = {
</span>229 <span style=''>      val newPlan = </span><span style='background: #AEF1AE'>FakePlan(expr, child)</span><span style=''>
</span>230 <span style=''>      //analyzer.execute(newPlan)
</span>231 <span style=''>      </span><span style='background: #AEF1AE'>execute(newPlan, resolution(analyzer, sparkSession, newPlan))</span><span style=''>
</span>232 <span style=''>      match {
</span>233 <span style=''>        case FakePlan(resolvedExpr, _) =&gt;
</span>234 <span style=''>          // Return even if it did not successfully resolve
</span>235 <span style=''>          resolvedExpr
</span>236 <span style=''>        case _ =&gt;
</span>237 <span style=''>          // This is unexpected
</span>238 <span style=''>          </span><span style='background: #F0ADAD'>throw new Exception(
</span>239 <span style=''></span><span style='background: #F0ADAD'>            s&quot;Could not resolve expression $expr with child $child}&quot;)</span><span style=''>
</span>240 <span style=''>      }
</span>241 <span style=''>    }
</span>242 <span style=''>    // special case as it's faster to do individual items it seems, 36816ms vs 48974ms
</span>243 <span style=''>    expr match {
</span>244 <span style=''>      case r @ RuleEngineRunner(ruleSuite, PassThrough( expressions ), realType, compileEvals, debugMode, func, group, forceRunnerEval, expressionOffsets, forceTriggerEval) =&gt;
</span>245 <span style=''>        val nexprs = </span><span style='background: #AEF1AE'>expressions.map(forExpr)</span><span style=''>
</span>246 <span style=''>        </span><span style='background: #AEF1AE'>RuleEngineRunner(ruleSuite, PassThrough( nexprs ), realType, compileEvals, debugMode, func, group, forceRunnerEval, expressionOffsets, forceTriggerEval)</span><span style=''>
</span>247 <span style=''>      case r @ RuleFolderRunner(ruleSuite, left, PassThrough( expressions ), resultDataType, compileEvals, debugMode, variablesPerFunc,
</span>248 <span style=''>        variableFuncGroup, forceRunnerEval, expressionOffsets, dataRef, forceTriggerEval) =&gt;
</span>249 <span style=''>        val nexprs = </span><span style='background: #F0ADAD'>expressions.map(forExpr)</span><span style=''>
</span>250 <span style=''>        </span><span style='background: #F0ADAD'>RuleFolderRunner(ruleSuite, left, PassThrough( nexprs ), resultDataType, compileEvals, debugMode, variablesPerFunc,
</span>251 <span style=''></span><span style='background: #F0ADAD'>          variableFuncGroup, forceRunnerEval, expressionOffsets, dataRef, forceTriggerEval)</span><span style=''>
</span>252 <span style=''>      case r @ RuleRunner(ruleSuite, PassThrough( expressions ), compileEvals, func, group, forceRunnerEval) =&gt;
</span>253 <span style=''>        val nexprs = </span><span style='background: #AEF1AE'>expressions.map(forExpr)</span><span style=''>
</span>254 <span style=''>        </span><span style='background: #AEF1AE'>RuleRunner(ruleSuite, PassThrough( nexprs ), compileEvals, func, group, forceRunnerEval)</span><span style=''>
</span>255 <span style=''>      case _ =&gt; </span><span style='background: #F0ADAD'>forExpr(expr)</span><span style=''>
</span>256 <span style=''>    }
</span>257 <span style=''>  }
</span>258 <span style=''>
</span>259 <span style=''>  case class FakePlan(expr: Expression, child: LogicalPlan)
</span>260 <span style=''>    extends UnaryNode {
</span>261 <span style=''>
</span>262 <span style=''>    override def output: Seq[Attribute] = </span><span style='background: #F0ADAD'>child.allAttributes.attrs</span><span style=''>
</span>263 <span style=''>
</span>264 <span style=''>    override def maxRows: Option[Long] = </span><span style='background: #F0ADAD'>Some(1)</span><span style=''>
</span>265 <span style=''>
</span>266 <span style=''>    protected def mygetAllValidConstraints(projectList: Seq[Expression]): Set[Expression] = {
</span>267 <span style=''>      var allConstraints = </span><span style='background: #F0ADAD'>Set.empty[Expression]</span><span style=''>
</span>268 <span style=''>      </span><span style='background: #F0ADAD'>projectList.foreach {
</span>269 <span style=''></span><span style='background: #F0ADAD'>        case a@Alias(l: Literal, _) =&gt;
</span>270 <span style=''></span><span style='background: #F0ADAD'>          allConstraints += EqualNullSafe(a.toAttribute, l)
</span>271 <span style=''></span><span style='background: #F0ADAD'>        case a@Alias(e, _) =&gt;
</span>272 <span style=''></span><span style='background: #F0ADAD'>          // For every alias in `projectList`, replace the reference in constraints by its attribute.
</span>273 <span style=''></span><span style='background: #F0ADAD'>          allConstraints ++= allConstraints.map(_ transform {
</span>274 <span style=''></span><span style='background: #F0ADAD'>            case expr: Expression if expr.semanticEquals(e) =&gt;
</span>275 <span style=''></span><span style='background: #F0ADAD'>              a.toAttribute
</span>276 <span style=''></span><span style='background: #F0ADAD'>          })
</span>277 <span style=''></span><span style='background: #F0ADAD'>          allConstraints += EqualNullSafe(e, a.toAttribute)
</span>278 <span style=''></span><span style='background: #F0ADAD'>        case _ =&gt; // Don't change.
</span>279 <span style=''></span><span style='background: #F0ADAD'>      }</span><span style=''>
</span>280 <span style=''>
</span>281 <span style=''>      allConstraints
</span>282 <span style=''>    }
</span>283 <span style=''>
</span>284 <span style=''>    override lazy val validConstraints: ExpressionSet = ExpressionSet(mygetAllValidConstraints(Seq(expr)))
</span>285 <span style=''>
</span>286 <span style=''>    protected def withNewChildInternal(newChild: LogicalPlan): LogicalPlan = </span><span style='background: #F0ADAD'>copy(child = newChild)</span><span style=''>
</span>287 <span style=''>  }
</span>288 <span style=''>
</span>289 <span style=''>  /**
</span>290 <span style=''>   * Creates a new parser, introduced in 0.4 - 3.2.0 due to SparkSqlParser having no params
</span>291 <span style=''>   *
</span>292 <span style=''>   * @return
</span>293 <span style=''>   */
</span>294 <span style=''>  def newParser() = {
</span>295 <span style=''>    </span><span style='background: #AEF1AE'>new SparkSqlParser()</span><span style=''>
</span>296 <span style=''>  }
</span>297 <span style=''>
</span>298 <span style=''>  /**
</span>299 <span style=''>   * Registers functions with spark, Introduced in 0.4 - 3.2.0 support due to extra source parameter - &quot;built-in&quot; is used as no other option is remotely close
</span>300 <span style=''>   *
</span>301 <span style=''>   * @param funcReg
</span>302 <span style=''>   * @param name
</span>303 <span style=''>   * @param builder
</span>304 <span style=''>   */
</span>305 <span style=''>  def registerFunction(funcReg: FunctionRegistry)(name: String, builder: Seq[Expression] =&gt; Expression) =
</span>306 <span style=''>    </span><span style='background: #AEF1AE'>funcReg.createOrReplaceTempFunction(name, builder, &quot;built-in&quot;)</span><span style=''>
</span>307 <span style=''>
</span>308 <span style=''>  def toString(dataFrame: DataFrame, showParams: ShowParams = ShowParams()) =
</span>309 <span style=''>    </span><span style='background: #AEF1AE'>dataFrame.showString(showParams.numRows, showParams.truncate, showParams.vertical)</span><span style=''>
</span>310 <span style=''>
</span>311 <span style=''>  /**
</span>312 <span style=''>   * Used by the SparkSessionExtensions mechanism
</span>313 <span style=''>   * @param extensions
</span>314 <span style=''>   * @param name
</span>315 <span style=''>   * @param builder
</span>316 <span style=''>   */
</span>317 <span style=''>  def registerFunctionViaExtension(extensions: SparkSessionExtensions)(name: String, builder: Seq[Expression] =&gt; Expression) =
</span>318 <span style=''>    </span><span style='background: #AEF1AE'>extensions.injectFunction( (FunctionIdentifier(name), new ExpressionInfo(name, name) , builder) )</span><span style=''>
</span>319 <span style=''>
</span>320 <span style=''>  /**
</span>321 <span style=''>   * Type signature changed for 3.4 to more detailed setup, 12.2 already uses it
</span>322 <span style=''>   * @param errorSubClass
</span>323 <span style=''>   * @param messageParameters
</span>324 <span style=''>   * @return
</span>325 <span style=''>   */
</span>326 <span style=''>  def mismatch(errorSubClass: String, messageParameters: Map[String, String]): TypeCheckResult =
</span>327 <span style=''>    </span><span style='background: #AEF1AE'>TypeCheckResult.TypeCheckFailure(s&quot;$errorSubClass extra info - $messageParameters&quot;)</span><span style=''>
</span>328 <span style=''>
</span>329 <span style=''>  def toSQLType(t: AbstractDataType): String = t match {
</span>330 <span style=''>    case TypeCollection(types) =&gt; </span><span style='background: #F0ADAD'>types.map(toSQLType).mkString(&quot;(&quot;, &quot; or &quot;, &quot;)&quot;)</span><span style=''>
</span>331 <span style=''>    case dt: DataType =&gt; </span><span style='background: #AEF1AE'>quoteByDefault(dt.sql)</span><span style=''>
</span>332 <span style=''>    case at =&gt; </span><span style='background: #F0ADAD'>quoteByDefault(at.simpleString.toUpperCase(Locale.ROOT))</span><span style=''>
</span>333 <span style=''>  }
</span>334 <span style=''>  def toSQLExpr(e: Expression): String = {
</span>335 <span style=''>    </span><span style='background: #AEF1AE'>quoteByDefault(toPrettySQL(e))</span><span style=''>
</span>336 <span style=''>  }
</span>337 <span style=''>
</span>338 <span style=''>  def usePrettyExpression(e: Expression): Expression = </span><span style='background: #AEF1AE'>e transform {
</span>339 <span style=''></span><span style='background: #AEF1AE'>    case a: Attribute =&gt; </span><span style='background: #F0ADAD'>new PrettyAttribute(a)</span><span style='background: #AEF1AE'>
</span>340 <span style=''></span><span style='background: #AEF1AE'>    case Literal(s: UTF8String, StringType) =&gt; PrettyAttribute(s.toString, StringType)
</span>341 <span style=''></span><span style='background: #AEF1AE'>    case Literal(v, t: NumericType) if v != null =&gt; PrettyAttribute(v.toString, t)
</span>342 <span style=''></span><span style='background: #AEF1AE'>    case Literal(null, dataType) =&gt; </span><span style='background: #F0ADAD'>PrettyAttribute(&quot;NULL&quot;, dataType)</span><span style='background: #AEF1AE'>
</span>343 <span style=''></span><span style='background: #AEF1AE'>    case e: GetStructField =&gt;
</span>344 <span style=''></span><span style='background: #AEF1AE'>      val name = </span><span style='background: #F0ADAD'>e.name.getOrElse(e.childSchema(e.ordinal).name)</span><span style='background: #AEF1AE'>
</span>345 <span style=''></span><span style='background: #AEF1AE'>      </span><span style='background: #F0ADAD'>PrettyAttribute(usePrettyExpression(e.child).sql + &quot;.&quot; + name, e.dataType)</span><span style='background: #AEF1AE'>
</span>346 <span style=''></span><span style='background: #AEF1AE'>    case e: GetArrayStructFields =&gt;
</span>347 <span style=''></span><span style='background: #AEF1AE'>      </span><span style='background: #F0ADAD'>PrettyAttribute(usePrettyExpression(e.child) + &quot;.&quot; + e.field.name, e.dataType)</span><span style='background: #AEF1AE'>
</span>348 <span style=''></span><span style='background: #AEF1AE'>    case c: Cast =&gt;
</span>349 <span style=''></span><span style='background: #AEF1AE'>      </span><span style='background: #F0ADAD'>PrettyAttribute(usePrettyExpression(c.child).sql, c.dataType)</span><span style='background: #AEF1AE'>
</span>350 <span style=''></span><span style='background: #AEF1AE'>  }</span><span style=''>
</span>351 <span style=''>
</span>352 <span style=''>  def toPrettySQL(e: Expression): String = </span><span style='background: #AEF1AE'>usePrettyExpression(e).sql</span><span style=''>
</span>353 <span style=''>  // Converts an error class parameter to its SQL representation
</span>354 <span style=''>  def toSQLValue(v: Any, t: DataType): String = </span><span style='background: #AEF1AE'>Literal.create(v, t)</span><span style=''> match {
</span>355 <span style=''>    case Literal(null, _) =&gt; </span><span style='background: #F0ADAD'>&quot;NULL&quot;</span><span style=''>
</span>356 <span style=''>    case Literal(v: Float, FloatType) =&gt;
</span>357 <span style=''>      if (</span><span style='background: #F0ADAD'>v.isNaN</span><span style=''>) </span><span style='background: #F0ADAD'>&quot;NaN&quot;</span><span style=''>
</span>358 <span style=''>      else </span><span style='background: #F0ADAD'>if (v.isPosInfinity) &quot;Infinity&quot;
</span>359 <span style=''></span><span style='background: #F0ADAD'>      else if (v.isNegInfinity) &quot;-Infinity&quot;
</span>360 <span style=''></span><span style='background: #F0ADAD'>      else v.toString</span><span style=''>
</span>361 <span style=''>    case l @ Literal(v: Double, DoubleType) =&gt;
</span>362 <span style=''>      if (</span><span style='background: #F0ADAD'>v.isNaN</span><span style=''>) </span><span style='background: #F0ADAD'>&quot;NaN&quot;</span><span style=''>
</span>363 <span style=''>      else </span><span style='background: #F0ADAD'>if (v.isPosInfinity) &quot;Infinity&quot;
</span>364 <span style=''></span><span style='background: #F0ADAD'>      else if (v.isNegInfinity) &quot;-Infinity&quot;
</span>365 <span style=''></span><span style='background: #F0ADAD'>      else l.sql</span><span style=''>
</span>366 <span style=''>    case l =&gt; </span><span style='background: #AEF1AE'>l.sql</span><span style=''>
</span>367 <span style=''>  }
</span>368 <span style=''>
</span>369 <span style=''>  private def quoteByDefault(elem: String): String = {
</span>370 <span style=''>    </span><span style='background: #AEF1AE'>&quot;\&quot;&quot; + elem + &quot;\&quot;&quot;</span><span style=''>
</span>371 <span style=''>  }
</span>372 <span style=''>
</span>373 <span style=''>}
</span></pre>
          </div>
          <div class="tab-pane" id="statementlist">
            <table cellspacing="0" cellpadding="0" class="table statementlist">
      <tr>
        <th>Line</th>
        <th>Stmt Id</th>
        <th>Pos</th>
        <th>Tree</th>
        <th>Symbol</th>
        <th>Tests</th>
        <th>Code</th>
      </tr><tr>
        <td>
          30
        </td>
        <td>
          257
        </td>
        <td>
          1723
          -
          1751
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedFunction.arguments
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          UnresolvedFunctionOps.this.unresolvedFunction.arguments
        </td>
      </tr><tr>
        <td>
          33
        </td>
        <td>
          262
        </td>
        <td>
          1830
          -
          1875
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedFunction.copy
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          UnresolvedFunctionOps.this.unresolvedFunction.copy(x$2, x$1, x$3, x$4, x$5)
        </td>
      </tr><tr>
        <td>
          33
        </td>
        <td>
          259
        </td>
        <td>
          1849
          -
          1849
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedFunction.copy$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          UnresolvedFunctionOps.this.unresolvedFunction.copy$default$3
        </td>
      </tr><tr>
        <td>
          33
        </td>
        <td>
          258
        </td>
        <td>
          1849
          -
          1849
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedFunction.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          UnresolvedFunctionOps.this.unresolvedFunction.copy$default$1
        </td>
      </tr><tr>
        <td>
          33
        </td>
        <td>
          261
        </td>
        <td>
          1849
          -
          1849
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedFunction.copy$default$5
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          UnresolvedFunctionOps.this.unresolvedFunction.copy$default$5
        </td>
      </tr><tr>
        <td>
          33
        </td>
        <td>
          260
        </td>
        <td>
          1849
          -
          1849
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedFunction.copy$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          UnresolvedFunctionOps.this.unresolvedFunction.copy$default$4
        </td>
      </tr><tr>
        <td>
          36
        </td>
        <td>
          263
        </td>
        <td>
          1921
          -
          1965
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.CatalystTypeConverters.isPrimitive
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.CatalystTypeConverters.isPrimitive(dataType)
        </td>
      </tr><tr>
        <td>
          54
        </td>
        <td>
          265
        </td>
        <td>
          2537
          -
          2553
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Add.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Add.apply(left, right, org.apache.spark.sql.catalyst.expressions.Add.apply$default$3)
        </td>
      </tr><tr>
        <td>
          54
        </td>
        <td>
          264
        </td>
        <td>
          2537
          -
          2537
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Add.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Add.apply$default$3
        </td>
      </tr><tr>
        <td>
          63
        </td>
        <td>
          268
        </td>
        <td>
          2737
          -
          2758
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Cast.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Cast.apply(child, dataType, org.apache.spark.sql.catalyst.expressions.Cast.apply$default$3, org.apache.spark.sql.catalyst.expressions.Cast.apply$default$4)
        </td>
      </tr><tr>
        <td>
          63
        </td>
        <td>
          267
        </td>
        <td>
          2737
          -
          2737
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Cast.apply$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Cast.apply$default$4
        </td>
      </tr><tr>
        <td>
          63
        </td>
        <td>
          266
        </td>
        <td>
          2737
          -
          2737
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Cast.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Cast.apply$default$3
        </td>
      </tr><tr>
        <td>
          69
        </td>
        <td>
          269
        </td>
        <td>
          2891
          -
          2919
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.UnresolvedFunction.arguments
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          unresolvedFunction.arguments
        </td>
      </tr><tr>
        <td>
          81
        </td>
        <td>
          274
        </td>
        <td>
          3235
          -
          3298
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          hashlongs.hashInt(c.days, hashlongs.hashLong(c.microseconds, digest))
        </td>
      </tr><tr>
        <td>
          81
        </td>
        <td>
          270
        </td>
        <td>
          3225
          -
          3233
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.unsafe.types.CalendarInterval.months
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          c.months
        </td>
      </tr><tr>
        <td>
          81
        </td>
        <td>
          275
        </td>
        <td>
          3217
          -
          3299
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          hashlongs.hashInt(c.months, hashlongs.hashInt(c.days, hashlongs.hashLong(c.microseconds, digest)))
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          271
        </td>
        <td>
          3250
          -
          3256
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.unsafe.types.CalendarInterval.days
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          c.days
        </td>
      </tr><tr>
        <td>
          83
        </td>
        <td>
          273
        </td>
        <td>
          3265
          -
          3297
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          hashlongs.hashLong(c.microseconds, digest)
        </td>
      </tr><tr>
        <td>
          83
        </td>
        <td>
          272
        </td>
        <td>
          3274
          -
          3288
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.unsafe.types.CalendarInterval.microseconds
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          c.microseconds
        </td>
      </tr><tr>
        <td>
          95
        </td>
        <td>
          276
        </td>
        <td>
          3775
          -
          3808
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.get
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          SparkSession.getActiveSession.get
        </td>
      </tr><tr>
        <td>
          97
        </td>
        <td>
          277
        </td>
        <td>
          3842
          -
          3845
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;*&quot;
        </td>
      </tr><tr>
        <td>
          97
        </td>
        <td>
          278
        </td>
        <td>
          3825
          -
          3858
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.Dataset.logicalPlan
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          dataFrame.select(&quot;*&quot;).logicalPlan
        </td>
      </tr><tr>
        <td>
          98
        </td>
        <td>
          280
        </td>
        <td>
          3959
          -
          3959
        </td>
        <td>
          TypeApply
        </td>
        <td>
          com.sparkutils.quality.debugTime$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.`package`.debugTime$default$2[Nothing]
        </td>
      </tr><tr>
        <td>
          98
        </td>
        <td>
          279
        </td>
        <td>
          3969
          -
          3991
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;tryResolveReferences&quot;
        </td>
      </tr><tr>
        <td>
          98
        </td>
        <td>
          282
        </td>
        <td>
          3959
          -
          4053
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.debugTime
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.`package`.debugTime[org.apache.spark.sql.catalyst.expressions.Expression](&quot;tryResolveReferences&quot;, com.sparkutils.quality.`package`.debugTime$default$2[Nothing])(QualitySparkUtils.this.tryResolveReferences(sparkSession)(expr, plan))
        </td>
      </tr><tr>
        <td>
          99
        </td>
        <td>
          281
        </td>
        <td>
          4001
          -
          4047
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.tryResolveReferences
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          QualitySparkUtils.this.tryResolveReferences(sparkSession)(expr, plan)
        </td>
      </tr><tr>
        <td>
          102
        </td>
        <td>
          283
        </td>
        <td>
          4080
          -
          4096
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;bindReferences&quot;
        </td>
      </tr><tr>
        <td>
          102
        </td>
        <td>
          284
        </td>
        <td>
          4070
          -
          4070
        </td>
        <td>
          TypeApply
        </td>
        <td>
          com.sparkutils.quality.debugTime$default$2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.`package`.debugTime$default$2[Nothing]
        </td>
      </tr><tr>
        <td>
          102
        </td>
        <td>
          287
        </td>
        <td>
          4070
          -
          4165
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.debugTime
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.`package`.debugTime[org.apache.spark.sql.catalyst.expressions.Expression](&quot;bindReferences&quot;, com.sparkutils.quality.`package`.debugTime$default$2[Nothing])(org.apache.spark.sql.catalyst.expressions.BindReferences.bindReference[org.apache.spark.sql.catalyst.expressions.Expression](res, plan.allAttributes, org.apache.spark.sql.catalyst.expressions.BindReferences.bindReference$default$3[Nothing]))
        </td>
      </tr><tr>
        <td>
          103
        </td>
        <td>
          286
        </td>
        <td>
          4106
          -
          4159
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.BindReferences.bindReference
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.BindReferences.bindReference[org.apache.spark.sql.catalyst.expressions.Expression](res, plan.allAttributes, org.apache.spark.sql.catalyst.expressions.BindReferences.bindReference$default$3[Nothing])
        </td>
      </tr><tr>
        <td>
          103
        </td>
        <td>
          285
        </td>
        <td>
          4121
          -
          4121
        </td>
        <td>
          TypeApply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.BindReferences.bindReference$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.BindReferences.bindReference$default$3[Nothing]
        </td>
      </tr><tr>
        <td>
          110
        </td>
        <td>
          288
        </td>
        <td>
          4259
          -
          4260
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          1
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          289
        </td>
        <td>
          4339
          -
          4363
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.System.currentTimeMillis
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          java.lang.System.currentTimeMillis()
        </td>
      </tr><tr>
        <td>
          117
        </td>
        <td>
          290
        </td>
        <td>
          4385
          -
          4389
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          121
        </td>
        <td>
          331
        </td>
        <td>
          4577
          -
          4577
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.while$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          while$1()
        </td>
      </tr><tr>
        <td>
          121
        </td>
        <td>
          334
        </td>
        <td>
          4560
          -
          4560
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          121
        </td>
        <td>
          333
        </td>
        <td>
          4560
          -
          4560
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          121
        </td>
        <td>
          332
        </td>
        <td>
          4577
          -
          5562
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  {
    curPlan = batch.rules.foldLeft[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan](curPlan)(((x0$1: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan, x1$1: org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]) =&gt; scala.Tuple2.apply[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan, org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x0$1, x1$1) match {
      case (_1: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan, _2: org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan])(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan, org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan])((plan @ _), (rule @ _)) =&gt; {
        val startTime: Long = java.lang.System.nanoTime();
        val result: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan = rule.apply(plan);
        result
      }
    }));
    iteration = iteration.+(1);
    if (iteration.&gt;(batch.strategy.maxIterations))
      {
        if (iteration.!=(2))
          {
            val endingMsg: String = if (batch.strategy.maxIterationsSetting.==(null))
              &quot;.&quot;
            else
              scala.StringContext.apply(&quot;, please set \'&quot;, &quot;\' to a larger value.&quot;).s(batch.strategy.maxIterationsSetting);
            val message: String = scala.StringContext.apply(&quot;Max iterations (&quot;, &quot;) reached for batch &quot;, &quot;&quot;).s(iteration.-(1), batch.name).+(scala.StringContext.apply(&quot;&quot;, &quot;&quot;).s(endingMsg));
            if (org.apache.spark.util.Utils.isTesting.||(batch.strategy.errorOnExceed))
              throw new scala.`package`.Exception(message)
            else
              ()
          }
        else
          ();
        continue = false
      }
    else
      ();
    if (curPlan.fastEquals(lastPlan))
      continue = false
    else
      ();
    lastPlan = curPlan
  };
  while$1()
}
        </td>
      </tr><tr>
        <td>
          122
        </td>
        <td>
          293
        </td>
        <td>
          4595
          -
          4759
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.foldLeft
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          batch.rules.foldLeft[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan](curPlan)(((x0$1: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan, x1$1: org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]) =&gt; scala.Tuple2.apply[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan, org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x0$1, x1$1) match {
  case (_1: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan, _2: org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan])(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan, org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan])((plan @ _), (rule @ _)) =&gt; {
    val startTime: Long = java.lang.System.nanoTime();
    val result: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan = rule.apply(plan);
    result
  }
}))
        </td>
      </tr><tr>
        <td>
          124
        </td>
        <td>
          291
        </td>
        <td>
          4682
          -
          4699
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.System.nanoTime
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          java.lang.System.nanoTime()
        </td>
      </tr><tr>
        <td>
          125
        </td>
        <td>
          292
        </td>
        <td>
          4723
          -
          4733
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.rules.Rule.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          rule.apply(plan)
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          294
        </td>
        <td>
          4766
          -
          4780
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          iteration.+(1)
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          295
        </td>
        <td>
          4803
          -
          4831
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.Strategy.maxIterations
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          batch.strategy.maxIterations
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          325
        </td>
        <td>
          4787
          -
          4787
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          324
        </td>
        <td>
          4787
          -
          4787
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          323
        </td>
        <td>
          4833
          -
          5455
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          {
  if (iteration.!=(2))
    {
      val endingMsg: String = if (batch.strategy.maxIterationsSetting.==(null))
        &quot;.&quot;
      else
        scala.StringContext.apply(&quot;, please set \'&quot;, &quot;\' to a larger value.&quot;).s(batch.strategy.maxIterationsSetting);
      val message: String = scala.StringContext.apply(&quot;Max iterations (&quot;, &quot;) reached for batch &quot;, &quot;&quot;).s(iteration.-(1), batch.name).+(scala.StringContext.apply(&quot;&quot;, &quot;&quot;).s(endingMsg));
      if (org.apache.spark.util.Utils.isTesting.||(batch.strategy.errorOnExceed))
        throw new scala.`package`.Exception(message)
      else
        ()
    }
  else
    ();
  continue = false
}
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          296
        </td>
        <td>
          4791
          -
          4831
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          iteration.&gt;(batch.strategy.maxIterations)
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          319
        </td>
        <td>
          4941
          -
          5422
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          {
  val endingMsg: String = if (batch.strategy.maxIterationsSetting.==(null))
    &quot;.&quot;
  else
    scala.StringContext.apply(&quot;, please set \'&quot;, &quot;\' to a larger value.&quot;).s(batch.strategy.maxIterationsSetting);
  val message: String = scala.StringContext.apply(&quot;Max iterations (&quot;, &quot;) reached for batch &quot;, &quot;&quot;).s(iteration.-(1), batch.name).+(scala.StringContext.apply(&quot;&quot;, &quot;&quot;).s(endingMsg));
  if (org.apache.spark.util.Utils.isTesting.||(batch.strategy.errorOnExceed))
    throw new scala.`package`.Exception(message)
  else
    ()
}
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          321
        </td>
        <td>
          4921
          -
          4921
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          297
        </td>
        <td>
          4925
          -
          4939
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.!=
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          iteration.!=(2)
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          320
        </td>
        <td>
          4921
          -
          4921
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          133
        </td>
        <td>
          298
        </td>
        <td>
          4973
          -
          5016
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.==
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          batch.strategy.maxIterationsSetting.==(null)
        </td>
      </tr><tr>
        <td>
          134
        </td>
        <td>
          300
        </td>
        <td>
          5032
          -
          5035
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.&quot;
        </td>
      </tr><tr>
        <td>
          134
        </td>
        <td>
          299
        </td>
        <td>
          5032
          -
          5035
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.&quot;
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          304
        </td>
        <td>
          5067
          -
          5142
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;, please set \'&quot;, &quot;\' to a larger value.&quot;).s(batch.strategy.maxIterationsSetting)
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          301
        </td>
        <td>
          5069
          -
          5084
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;, please set \'&quot;
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          303
        </td>
        <td>
          5085
          -
          5120
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.Strategy.maxIterationsSetting
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          batch.strategy.maxIterationsSetting
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          302
        </td>
        <td>
          5121
          -
          5142
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\' to a larger value.&quot;
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          305
        </td>
        <td>
          5067
          -
          5142
        </td>
        <td>
          Block
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;, please set \'&quot;, &quot;\' to a larger value.&quot;).s(batch.strategy.maxIterationsSetting)
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          307
        </td>
        <td>
          5213
          -
          5234
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;) reached for batch &quot;
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          310
        </td>
        <td>
          5235
          -
          5245
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.Batch.name
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          batch.name
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          309
        </td>
        <td>
          5199
          -
          5212
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.-
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          iteration.-(1)
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          312
        </td>
        <td>
          5179
          -
          5275
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;Max iterations (&quot;, &quot;) reached for batch &quot;, &quot;&quot;).s(iteration.-(1), batch.name).+(scala.StringContext.apply(&quot;&quot;, &quot;&quot;).s(endingMsg))
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          306
        </td>
        <td>
          5181
          -
          5198
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;Max iterations (&quot;
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          308
        </td>
        <td>
          5246
          -
          5247
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          311
        </td>
        <td>
          5262
          -
          5275
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot;&quot;).s(endingMsg)
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          313
        </td>
        <td>
          5309
          -
          5337
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.Strategy.errorOnExceed
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          batch.strategy.errorOnExceed
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          314
        </td>
        <td>
          5290
          -
          5337
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.||
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.util.Utils.isTesting.||(batch.strategy.errorOnExceed)
        </td>
      </tr><tr>
        <td>
          141
        </td>
        <td>
          316
        </td>
        <td>
          5353
          -
          5381
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          throw new scala.`package`.Exception(message)
        </td>
      </tr><tr>
        <td>
          141
        </td>
        <td>
          315
        </td>
        <td>
          5353
          -
          5381
        </td>
        <td>
          Throw
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          throw new scala.`package`.Exception(message)
        </td>
      </tr><tr>
        <td>
          142
        </td>
        <td>
          318
        </td>
        <td>
          5399
          -
          5412
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          142
        </td>
        <td>
          317
        </td>
        <td>
          5399
          -
          5412
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          145
        </td>
        <td>
          322
        </td>
        <td>
          5442
          -
          5447
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          false
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          330
        </td>
        <td>
          5463
          -
          5463
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          326
        </td>
        <td>
          5467
          -
          5495
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.fastEquals
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          curPlan.fastEquals(lastPlan)
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          329
        </td>
        <td>
          5463
          -
          5463
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          149
        </td>
        <td>
          328
        </td>
        <td>
          5507
          -
          5523
        </td>
        <td>
          Assign
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          continue = false
        </td>
      </tr><tr>
        <td>
          149
        </td>
        <td>
          327
        </td>
        <td>
          5518
          -
          5523
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          false
        </td>
      </tr><tr>
        <td>
          153
        </td>
        <td>
          335
        </td>
        <td>
          5578
          -
          5602
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.System.currentTimeMillis
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          java.lang.System.currentTimeMillis()
        </td>
      </tr><tr>
        <td>
          166
        </td>
        <td>
          336
        </td>
        <td>
          6046
          -
          6074
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.SQLContext.conf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          sparkSession.sqlContext.conf
        </td>
      </tr><tr>
        <td>
          167
        </td>
        <td>
          340
        </td>
        <td>
          6096
          -
          6237
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.Strategy.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new QualitySparkUtils.this.Strategy(conf.analyzerMaxIterations, true, org.apache.spark.sql.internal.SQLConf.ANALYZER_MAX_ITERATIONS.key)
        </td>
      </tr><tr>
        <td>
          168
        </td>
        <td>
          337
        </td>
        <td>
          6116
          -
          6142
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.internal.SQLConf.analyzerMaxIterations
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          conf.analyzerMaxIterations
        </td>
      </tr><tr>
        <td>
          169
        </td>
        <td>
          338
        </td>
        <td>
          6166
          -
          6170
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          170
        </td>
        <td>
          339
        </td>
        <td>
          6201
          -
          6236
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.internal.config.ConfigEntry.key
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.internal.SQLConf.ANALYZER_MAX_ITERATIONS.key
        </td>
      </tr><tr>
        <td>
          174
        </td>
        <td>
          343
        </td>
        <td>
          6266
          -
          7753
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.Batch.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          QualitySparkUtils.this.Batch.apply(&quot;Resolution&quot;, fixedPoint, ({
  &lt;synthetic&gt; &lt;artifact&gt; val x$42: analyzer.ResolveNamespace = analyzer.ResolveNamespace.apply(analyzer.catalogManager);
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$41: org.apache.spark.sql.catalyst.analysis.ResolveCatalogs = new org.apache.spark.sql.catalyst.analysis.ResolveCatalogs(analyzer.catalogManager);
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$40: analyzer.ResolveUserSpecifiedColumns.type = analyzer.ResolveUserSpecifiedColumns;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$39: analyzer.ResolveInsertInto.type = analyzer.ResolveInsertInto;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$38: analyzer.ResolveRelations.type = analyzer.ResolveRelations;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$37: analyzer.ResolveTables.type = analyzer.ResolveTables;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$36: org.apache.spark.sql.catalyst.analysis.ResolvePartitionSpec.type = org.apache.spark.sql.catalyst.analysis.ResolvePartitionSpec;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$35: analyzer.ResolveAlterTableCommands.type = analyzer.ResolveAlterTableCommands;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$34: analyzer.AddMetadataColumns.type = analyzer.AddMetadataColumns;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$33: org.apache.spark.sql.catalyst.analysis.DeduplicateRelations.type = org.apache.spark.sql.catalyst.analysis.DeduplicateRelations;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$32: analyzer.ResolveReferences.type = analyzer.ResolveReferences;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$31: org.apache.spark.sql.catalyst.analysis.ResolveExpressionsWithNamePlaceholders.type = org.apache.spark.sql.catalyst.analysis.ResolveExpressionsWithNamePlaceholders;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$30: analyzer.ResolveDeserializer.type = analyzer.ResolveDeserializer;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$29: analyzer.ResolveNewInstance.type = analyzer.ResolveNewInstance;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$28: analyzer.ResolveUpCast.type = analyzer.ResolveUpCast;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$27: analyzer.ResolveGroupingAnalytics.type = analyzer.ResolveGroupingAnalytics;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$26: analyzer.ResolvePivot.type = analyzer.ResolvePivot;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$25: analyzer.ResolveOrdinalInOrderByAndGroupBy.type = analyzer.ResolveOrdinalInOrderByAndGroupBy;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$24: analyzer.ResolveAggAliasInGroupBy.type = analyzer.ResolveAggAliasInGroupBy;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$23: analyzer.ResolveMissingReferences.type = analyzer.ResolveMissingReferences;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$22: analyzer.ExtractGenerator.type = analyzer.ExtractGenerator;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$21: analyzer.ResolveGenerate.type = analyzer.ResolveGenerate;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$20: analyzer.ResolveFunctions.type = analyzer.ResolveFunctions;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$19: analyzer.ResolveAliases.type = analyzer.ResolveAliases;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$18: analyzer.ResolveSubquery.type = analyzer.ResolveSubquery;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$17: analyzer.ResolveSubqueryColumnAliases.type = analyzer.ResolveSubqueryColumnAliases;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$16: analyzer.ResolveWindowOrder.type = analyzer.ResolveWindowOrder;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$15: analyzer.ResolveWindowFrame.type = analyzer.ResolveWindowFrame;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$14: analyzer.ResolveNaturalAndUsingJoin.type = analyzer.ResolveNaturalAndUsingJoin;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$13: analyzer.ResolveOutputRelation.type = analyzer.ResolveOutputRelation;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$12: analyzer.ExtractWindowExpressions.type = analyzer.ExtractWindowExpressions;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$11: analyzer.GlobalAggregates.type = analyzer.GlobalAggregates;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$10: analyzer.ResolveAggregateFunctions.type = analyzer.ResolveAggregateFunctions;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$9: org.apache.spark.sql.catalyst.analysis.TimeWindowing.type = org.apache.spark.sql.catalyst.analysis.TimeWindowing;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$8: org.apache.spark.sql.catalyst.analysis.SessionWindowing.type = org.apache.spark.sql.catalyst.analysis.SessionWindowing;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$7: org.apache.spark.sql.catalyst.analysis.ResolveInlineTables.type = org.apache.spark.sql.catalyst.analysis.ResolveInlineTables;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$6: org.apache.spark.sql.catalyst.analysis.ResolveHigherOrderFunctions = org.apache.spark.sql.catalyst.analysis.ResolveHigherOrderFunctions.apply(analyzer.catalogManager);
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$5: org.apache.spark.sql.catalyst.analysis.ResolveLambdaVariables.type = org.apache.spark.sql.catalyst.analysis.ResolveLambdaVariables;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$4: org.apache.spark.sql.catalyst.analysis.ResolveTimeZone.type = org.apache.spark.sql.catalyst.analysis.ResolveTimeZone;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$3: analyzer.ResolveRandomSeed.type = analyzer.ResolveRandomSeed;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$2: analyzer.ResolveBinaryArithmetic.type = analyzer.ResolveBinaryArithmetic;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$1: org.apache.spark.sql.catalyst.analysis.ResolveUnion.type = org.apache.spark.sql.catalyst.analysis.ResolveUnion;
  org.apache.spark.sql.catalyst.analysis.TypeCoercion.typeCoercionRules.++[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan], List[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]]](scala.collection.Seq.apply[org.apache.spark.sql.catalyst.analysis.ResolveWithCTE.type](org.apache.spark.sql.catalyst.analysis.ResolveWithCTE))(immutable.this.List.canBuildFrom[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]]).::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$1)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$2)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$3)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$4)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$5)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$6)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$7)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$8)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$9)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$10)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$11)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$12)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$13)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$14)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$15)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$16)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$17)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$18)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$19)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$20)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$21)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$22)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$23)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$24)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$25)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$26)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$27)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$28)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$29)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$30)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$31)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$32)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$33)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$34)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$35)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$36)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$37)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$38)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$39)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$40)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$41)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$42)
}: _*))
        </td>
      </tr><tr>
        <td>
          174
        </td>
        <td>
          341
        </td>
        <td>
          6272
          -
          6284
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;Resolution&quot;
        </td>
      </tr><tr>
        <td>
          175
        </td>
        <td>
          342
        </td>
        <td>
          6304
          -
          7748
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.immutable.List.::
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  &lt;synthetic&gt; &lt;artifact&gt; val x$41: org.apache.spark.sql.catalyst.analysis.ResolveCatalogs = new org.apache.spark.sql.catalyst.analysis.ResolveCatalogs(analyzer.catalogManager);
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$40: analyzer.ResolveUserSpecifiedColumns.type = analyzer.ResolveUserSpecifiedColumns;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$39: analyzer.ResolveInsertInto.type = analyzer.ResolveInsertInto;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$38: analyzer.ResolveRelations.type = analyzer.ResolveRelations;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$37: analyzer.ResolveTables.type = analyzer.ResolveTables;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$36: org.apache.spark.sql.catalyst.analysis.ResolvePartitionSpec.type = org.apache.spark.sql.catalyst.analysis.ResolvePartitionSpec;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$35: analyzer.ResolveAlterTableCommands.type = analyzer.ResolveAlterTableCommands;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$34: analyzer.AddMetadataColumns.type = analyzer.AddMetadataColumns;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$33: org.apache.spark.sql.catalyst.analysis.DeduplicateRelations.type = org.apache.spark.sql.catalyst.analysis.DeduplicateRelations;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$32: analyzer.ResolveReferences.type = analyzer.ResolveReferences;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$31: org.apache.spark.sql.catalyst.analysis.ResolveExpressionsWithNamePlaceholders.type = org.apache.spark.sql.catalyst.analysis.ResolveExpressionsWithNamePlaceholders;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$30: analyzer.ResolveDeserializer.type = analyzer.ResolveDeserializer;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$29: analyzer.ResolveNewInstance.type = analyzer.ResolveNewInstance;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$28: analyzer.ResolveUpCast.type = analyzer.ResolveUpCast;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$27: analyzer.ResolveGroupingAnalytics.type = analyzer.ResolveGroupingAnalytics;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$26: analyzer.ResolvePivot.type = analyzer.ResolvePivot;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$25: analyzer.ResolveOrdinalInOrderByAndGroupBy.type = analyzer.ResolveOrdinalInOrderByAndGroupBy;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$24: analyzer.ResolveAggAliasInGroupBy.type = analyzer.ResolveAggAliasInGroupBy;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$23: analyzer.ResolveMissingReferences.type = analyzer.ResolveMissingReferences;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$22: analyzer.ExtractGenerator.type = analyzer.ExtractGenerator;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$21: analyzer.ResolveGenerate.type = analyzer.ResolveGenerate;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$20: analyzer.ResolveFunctions.type = analyzer.ResolveFunctions;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$19: analyzer.ResolveAliases.type = analyzer.ResolveAliases;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$18: analyzer.ResolveSubquery.type = analyzer.ResolveSubquery;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$17: analyzer.ResolveSubqueryColumnAliases.type = analyzer.ResolveSubqueryColumnAliases;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$16: analyzer.ResolveWindowOrder.type = analyzer.ResolveWindowOrder;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$15: analyzer.ResolveWindowFrame.type = analyzer.ResolveWindowFrame;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$14: analyzer.ResolveNaturalAndUsingJoin.type = analyzer.ResolveNaturalAndUsingJoin;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$13: analyzer.ResolveOutputRelation.type = analyzer.ResolveOutputRelation;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$12: analyzer.ExtractWindowExpressions.type = analyzer.ExtractWindowExpressions;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$11: analyzer.GlobalAggregates.type = analyzer.GlobalAggregates;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$10: analyzer.ResolveAggregateFunctions.type = analyzer.ResolveAggregateFunctions;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$9: org.apache.spark.sql.catalyst.analysis.TimeWindowing.type = org.apache.spark.sql.catalyst.analysis.TimeWindowing;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$8: org.apache.spark.sql.catalyst.analysis.SessionWindowing.type = org.apache.spark.sql.catalyst.analysis.SessionWindowing;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$7: org.apache.spark.sql.catalyst.analysis.ResolveInlineTables.type = org.apache.spark.sql.catalyst.analysis.ResolveInlineTables;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$6: org.apache.spark.sql.catalyst.analysis.ResolveHigherOrderFunctions = org.apache.spark.sql.catalyst.analysis.ResolveHigherOrderFunctions.apply(analyzer.catalogManager);
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$5: org.apache.spark.sql.catalyst.analysis.ResolveLambdaVariables.type = org.apache.spark.sql.catalyst.analysis.ResolveLambdaVariables;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$4: org.apache.spark.sql.catalyst.analysis.ResolveTimeZone.type = org.apache.spark.sql.catalyst.analysis.ResolveTimeZone;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$3: analyzer.ResolveRandomSeed.type = analyzer.ResolveRandomSeed;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$2: analyzer.ResolveBinaryArithmetic.type = analyzer.ResolveBinaryArithmetic;
  {
  &lt;synthetic&gt; &lt;artifact&gt; val x$1: org.apache.spark.sql.catalyst.analysis.ResolveUnion.type = org.apache.spark.sql.catalyst.analysis.ResolveUnion;
  org.apache.spark.sql.catalyst.analysis.TypeCoercion.typeCoercionRules.++[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan], List[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]]](scala.collection.Seq.apply[org.apache.spark.sql.catalyst.analysis.ResolveWithCTE.type](org.apache.spark.sql.catalyst.analysis.ResolveWithCTE))(immutable.this.List.canBuildFrom[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]]).::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$1)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$2)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$3)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$4)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$5)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$6)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$7)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$8)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$9)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$10)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$11)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$12)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$13)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$14)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$15)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$16)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$17)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$18)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$19)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$20)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$21)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$22)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$23)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$24)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$25)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$26)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$27)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$28)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$29)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$30)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$31)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$32)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$33)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$34)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$35)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$36)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$37)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$38)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$39)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$40)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$41)
}.::[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]](x$42)
        </td>
      </tr><tr>
        <td>
          229
        </td>
        <td>
          344
        </td>
        <td>
          8211
          -
          8232
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.FakePlan.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          QualitySparkUtils.this.FakePlan.apply(expr, child)
        </td>
      </tr><tr>
        <td>
          231
        </td>
        <td>
          346
        </td>
        <td>
          8273
          -
          8334
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.execute
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          QualitySparkUtils.this.execute(newPlan, QualitySparkUtils.this.resolution(analyzer, sparkSession, newPlan))
        </td>
      </tr><tr>
        <td>
          231
        </td>
        <td>
          345
        </td>
        <td>
          8290
          -
          8333
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.resolution
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          QualitySparkUtils.this.resolution(analyzer, sparkSession, newPlan)
        </td>
      </tr><tr>
        <td>
          238
        </td>
        <td>
          347
        </td>
        <td>
          8534
          -
          8624
        </td>
        <td>
          Throw
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          throw new scala.`package`.Exception(scala.StringContext.apply(&quot;Could not resolve expression &quot;, &quot; with child &quot;, &quot;}&quot;).s(expr, child))
        </td>
      </tr><tr>
        <td>
          245
        </td>
        <td>
          349
        </td>
        <td>
          8955
          -
          8955
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]
        </td>
      </tr><tr>
        <td>
          245
        </td>
        <td>
          348
        </td>
        <td>
          8956
          -
          8963
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.forExpr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          forExpr(expr)
        </td>
      </tr><tr>
        <td>
          245
        </td>
        <td>
          350
        </td>
        <td>
          8940
          -
          8964
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          expressions.map[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]]({
  ((expr: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; forExpr(expr))
})(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression])
        </td>
      </tr><tr>
        <td>
          246
        </td>
        <td>
          352
        </td>
        <td>
          8973
          -
          9125
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleEngineRunner.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.RuleEngineRunner.apply(ruleSuite, com.sparkutils.quality.utils.PassThrough.apply(nexprs), realType, compileEvals, debugMode, func, group, forceRunnerEval, expressionOffsets, forceTriggerEval)
        </td>
      </tr><tr>
        <td>
          246
        </td>
        <td>
          351
        </td>
        <td>
          9001
          -
          9022
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.utils.PassThrough.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.utils.PassThrough.apply(nexprs)
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          355
        </td>
        <td>
          9376
          -
          9400
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          expressions.map[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]]({
  ((expr: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; forExpr(expr))
})(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression])
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          354
        </td>
        <td>
          9391
          -
          9391
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]
        </td>
      </tr><tr>
        <td>
          249
        </td>
        <td>
          353
        </td>
        <td>
          9392
          -
          9399
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.forExpr
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          forExpr(expr)
        </td>
      </tr><tr>
        <td>
          250
        </td>
        <td>
          357
        </td>
        <td>
          9409
          -
          9616
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleFolderRunner.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.sparkutils.quality.impl.RuleFolderRunner.apply(ruleSuite, left, com.sparkutils.quality.utils.PassThrough.apply(nexprs), resultDataType, compileEvals, debugMode, variablesPerFunc, variableFuncGroup, forceRunnerEval, expressionOffsets, dataRef, forceTriggerEval)
        </td>
      </tr><tr>
        <td>
          250
        </td>
        <td>
          356
        </td>
        <td>
          9443
          -
          9464
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.utils.PassThrough.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          com.sparkutils.quality.utils.PassThrough.apply(nexprs)
        </td>
      </tr><tr>
        <td>
          253
        </td>
        <td>
          358
        </td>
        <td>
          9766
          -
          9773
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.forExpr
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          forExpr(expr)
        </td>
      </tr><tr>
        <td>
          253
        </td>
        <td>
          360
        </td>
        <td>
          9750
          -
          9774
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          expressions.map[org.apache.spark.sql.catalyst.expressions.Expression, Seq[org.apache.spark.sql.catalyst.expressions.Expression]]({
  ((expr: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; forExpr(expr))
})(collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression])
        </td>
      </tr><tr>
        <td>
          253
        </td>
        <td>
          359
        </td>
        <td>
          9765
          -
          9765
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]
        </td>
      </tr><tr>
        <td>
          254
        </td>
        <td>
          361
        </td>
        <td>
          9805
          -
          9826
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.utils.PassThrough.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.utils.PassThrough.apply(nexprs)
        </td>
      </tr><tr>
        <td>
          254
        </td>
        <td>
          362
        </td>
        <td>
          9783
          -
          9871
        </td>
        <td>
          Apply
        </td>
        <td>
          com.sparkutils.quality.impl.RuleRunner.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          com.sparkutils.quality.impl.RuleRunner.apply(ruleSuite, com.sparkutils.quality.utils.PassThrough.apply(nexprs), compileEvals, func, group, forceRunnerEval)
        </td>
      </tr><tr>
        <td>
          255
        </td>
        <td>
          363
        </td>
        <td>
          9888
          -
          9901
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.forExpr
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          forExpr(expr)
        </td>
      </tr><tr>
        <td>
          262
        </td>
        <td>
          364
        </td>
        <td>
          10040
          -
          10065
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.AttributeSeq.attrs
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          FakePlan.this.child.allAttributes.attrs
        </td>
      </tr><tr>
        <td>
          264
        </td>
        <td>
          365
        </td>
        <td>
          10108
          -
          10115
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Some.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Some.apply[Long](1L)
        </td>
      </tr><tr>
        <td>
          267
        </td>
        <td>
          366
        </td>
        <td>
          10238
          -
          10259
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.generic.ImmutableSetFactory.empty
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.Set.empty[org.apache.spark.sql.catalyst.expressions.Expression]
        </td>
      </tr><tr>
        <td>
          268
        </td>
        <td>
          381
        </td>
        <td>
          10266
          -
          10787
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.IterableLike.foreach
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          projectList.foreach[Unit](((x0$1: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; x0$1 match {
  case (a @ (child: org.apache.spark.sql.catalyst.expressions.Expression, name: String)(exprId: org.apache.spark.sql.catalyst.expressions.ExprId, qualifier: Seq[String], explicitMetadata: Option[org.apache.spark.sql.types.Metadata], nonInheritableMetadataKeys: Seq[String])org.apache.spark.sql.catalyst.expressions.Alias((l @ (_: org.apache.spark.sql.catalyst.expressions.Literal)), _)) =&gt; allConstraints = allConstraints.+(org.apache.spark.sql.catalyst.expressions.EqualNullSafe.apply(a.toAttribute, l))
  case (a @ (child: org.apache.spark.sql.catalyst.expressions.Expression, name: String)(exprId: org.apache.spark.sql.catalyst.expressions.ExprId, qualifier: Seq[String], explicitMetadata: Option[org.apache.spark.sql.types.Metadata], nonInheritableMetadataKeys: Seq[String])org.apache.spark.sql.catalyst.expressions.Alias((e @ _), _)) =&gt; {
    allConstraints = allConstraints.++(allConstraints.map[org.apache.spark.sql.catalyst.expressions.Expression, scala.collection.immutable.Set[org.apache.spark.sql.catalyst.expressions.Expression]](((x$43: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; x$43.transform(({
      @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
        def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
          $anonfun.super.&lt;init&gt;();
          ()
        };
        final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x1: A1, default: A1 =&gt; B1): B1 = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
          case (expr @ (_: org.apache.spark.sql.catalyst.expressions.Expression)) if expr.semanticEquals(e) =&gt; a.toAttribute
          case (defaultCase$ @ _) =&gt; default.apply(x1)
        };
        final def isDefinedAt(x1: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
          case (expr @ (_: org.apache.spark.sql.catalyst.expressions.Expression)) if expr.semanticEquals(e) =&gt; true
          case (defaultCase$ @ _) =&gt; false
        }
      };
      new $anonfun()
    }: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))))(immutable.this.Set.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]));
    allConstraints = allConstraints.+(org.apache.spark.sql.catalyst.expressions.EqualNullSafe.apply(e, a.toAttribute))
  }
  case _ =&gt; ()
}))
        </td>
      </tr><tr>
        <td>
          270
        </td>
        <td>
          367
        </td>
        <td>
          10369
          -
          10382
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Alias.toAttribute
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          a.toAttribute
        </td>
      </tr><tr>
        <td>
          270
        </td>
        <td>
          369
        </td>
        <td>
          10337
          -
          10386
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SetLike.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          allConstraints.+(org.apache.spark.sql.catalyst.expressions.EqualNullSafe.apply(a.toAttribute, l))
        </td>
      </tr><tr>
        <td>
          270
        </td>
        <td>
          368
        </td>
        <td>
          10355
          -
          10386
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.EqualNullSafe.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.EqualNullSafe.apply(a.toAttribute, l)
        </td>
      </tr><tr>
        <td>
          273
        </td>
        <td>
          373
        </td>
        <td>
          10567
          -
          10683
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.transform
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$43.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x1: A1, default: A1 =&gt; B1): B1 = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (expr @ (_: org.apache.spark.sql.catalyst.expressions.Expression)) if expr.semanticEquals(e) =&gt; a.toAttribute
      case (defaultCase$ @ _) =&gt; default.apply(x1)
    };
    final def isDefinedAt(x1: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (expr @ (_: org.apache.spark.sql.catalyst.expressions.Expression)) if expr.semanticEquals(e) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          273
        </td>
        <td>
          376
        </td>
        <td>
          10529
          -
          10684
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SetLike.++
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          allConstraints.++(allConstraints.map[org.apache.spark.sql.catalyst.expressions.Expression, scala.collection.immutable.Set[org.apache.spark.sql.catalyst.expressions.Expression]](((x$43: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; x$43.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x1: A1, default: A1 =&gt; B1): B1 = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (expr @ (_: org.apache.spark.sql.catalyst.expressions.Expression)) if expr.semanticEquals(e) =&gt; a.toAttribute
      case (defaultCase$ @ _) =&gt; default.apply(x1)
    };
    final def isDefinedAt(x1: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (expr @ (_: org.apache.spark.sql.catalyst.expressions.Expression)) if expr.semanticEquals(e) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))))(immutable.this.Set.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          273
        </td>
        <td>
          372
        </td>
        <td>
          10579
          -
          10579
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.FakePlan.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          273
        </td>
        <td>
          375
        </td>
        <td>
          10548
          -
          10684
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.SetLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          allConstraints.map[org.apache.spark.sql.catalyst.expressions.Expression, scala.collection.immutable.Set[org.apache.spark.sql.catalyst.expressions.Expression]](((x$43: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; x$43.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x1: A1, default: A1 =&gt; B1): B1 = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (expr @ (_: org.apache.spark.sql.catalyst.expressions.Expression)) if expr.semanticEquals(e) =&gt; a.toAttribute
      case (defaultCase$ @ _) =&gt; default.apply(x1)
    };
    final def isDefinedAt(x1: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (expr @ (_: org.apache.spark.sql.catalyst.expressions.Expression)) if expr.semanticEquals(e) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))))(immutable.this.Set.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression])
        </td>
      </tr><tr>
        <td>
          273
        </td>
        <td>
          374
        </td>
        <td>
          10566
          -
          10566
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.Set.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          immutable.this.Set.canBuildFrom[org.apache.spark.sql.catalyst.expressions.Expression]
        </td>
      </tr><tr>
        <td>
          274
        </td>
        <td>
          370
        </td>
        <td>
          10618
          -
          10640
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.semanticEquals
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          expr.semanticEquals(e)
        </td>
      </tr><tr>
        <td>
          275
        </td>
        <td>
          371
        </td>
        <td>
          10658
          -
          10671
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Alias.toAttribute
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          a.toAttribute
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          379
        </td>
        <td>
          10695
          -
          10744
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SetLike.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          allConstraints.+(org.apache.spark.sql.catalyst.expressions.EqualNullSafe.apply(e, a.toAttribute))
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          378
        </td>
        <td>
          10713
          -
          10744
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.EqualNullSafe.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.EqualNullSafe.apply(e, a.toAttribute)
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          377
        </td>
        <td>
          10730
          -
          10743
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Alias.toAttribute
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          a.toAttribute
        </td>
      </tr><tr>
        <td>
          278
        </td>
        <td>
          380
        </td>
        <td>
          10760
          -
          10762
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          286
        </td>
        <td>
          382
        </td>
        <td>
          11002
          -
          11002
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.FakePlan.copy$default$1
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          FakePlan.this.copy$default$1
        </td>
      </tr><tr>
        <td>
          286
        </td>
        <td>
          383
        </td>
        <td>
          11002
          -
          11024
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.FakePlan.copy
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          FakePlan.this.copy(x$2, x$1)
        </td>
      </tr><tr>
        <td>
          295
        </td>
        <td>
          384
        </td>
        <td>
          11178
          -
          11198
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.execution.SparkSqlParser.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.sql.execution.SparkSqlParser()
        </td>
      </tr><tr>
        <td>
          306
        </td>
        <td>
          385
        </td>
        <td>
          11547
          -
          11609
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.FunctionRegistryBase.createOrReplaceTempFunction
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          funcReg.createOrReplaceTempFunction(name, builder, &quot;built-in&quot;)
        </td>
      </tr><tr>
        <td>
          309
        </td>
        <td>
          388
        </td>
        <td>
          11755
          -
          11774
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.ShowParams.vertical
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          showParams.vertical
        </td>
      </tr><tr>
        <td>
          309
        </td>
        <td>
          387
        </td>
        <td>
          11734
          -
          11753
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.ShowParams.truncate
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          showParams.truncate
        </td>
      </tr><tr>
        <td>
          309
        </td>
        <td>
          386
        </td>
        <td>
          11714
          -
          11732
        </td>
        <td>
          Select
        </td>
        <td>
          com.sparkutils.quality.impl.ShowParams.numRows
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          showParams.numRows
        </td>
      </tr><tr>
        <td>
          309
        </td>
        <td>
          389
        </td>
        <td>
          11693
          -
          11775
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.Dataset.showString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          dataFrame.showString(showParams.numRows, showParams.truncate, showParams.vertical)
        </td>
      </tr><tr>
        <td>
          318
        </td>
        <td>
          391
        </td>
        <td>
          12084
          -
          12114
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.ExpressionInfo.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.sql.catalyst.expressions.ExpressionInfo(name, name)
        </td>
      </tr><tr>
        <td>
          318
        </td>
        <td>
          390
        </td>
        <td>
          12058
          -
          12082
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.FunctionIdentifier.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.FunctionIdentifier.apply(name)
        </td>
      </tr><tr>
        <td>
          318
        </td>
        <td>
          393
        </td>
        <td>
          12030
          -
          12127
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.SparkSessionExtensions.injectFunction
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          extensions.injectFunction(scala.Tuple3.apply[org.apache.spark.sql.catalyst.FunctionIdentifier, org.apache.spark.sql.catalyst.expressions.ExpressionInfo, Seq[org.apache.spark.sql.catalyst.expressions.Expression] =&gt; org.apache.spark.sql.catalyst.expressions.Expression](org.apache.spark.sql.catalyst.FunctionIdentifier.apply(name), new org.apache.spark.sql.catalyst.expressions.ExpressionInfo(name, name), builder))
        </td>
      </tr><tr>
        <td>
          318
        </td>
        <td>
          392
        </td>
        <td>
          12057
          -
          12125
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple3.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple3.apply[org.apache.spark.sql.catalyst.FunctionIdentifier, org.apache.spark.sql.catalyst.expressions.ExpressionInfo, Seq[org.apache.spark.sql.catalyst.expressions.Expression] =&gt; org.apache.spark.sql.catalyst.expressions.Expression](org.apache.spark.sql.catalyst.FunctionIdentifier.apply(name), new org.apache.spark.sql.catalyst.expressions.ExpressionInfo(name, name), builder)
        </td>
      </tr><tr>
        <td>
          327
        </td>
        <td>
          394
        </td>
        <td>
          12425
          -
          12474
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.StringContext.apply(&quot;&quot;, &quot; extra info - &quot;, &quot;&quot;).s(errorSubClass, messageParameters)
        </td>
      </tr><tr>
        <td>
          327
        </td>
        <td>
          395
        </td>
        <td>
          12392
          -
          12475
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply(scala.StringContext.apply(&quot;&quot;, &quot; extra info - &quot;, &quot;&quot;).s(errorSubClass, messageParameters))
        </td>
      </tr><tr>
        <td>
          330
        </td>
        <td>
          396
        </td>
        <td>
          12568
          -
          12615
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.mkString
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          types.map[String, Seq[String]]({
  ((t: org.apache.spark.sql.types.AbstractDataType) =&gt; QualitySparkUtils.this.toSQLType(t))
})(collection.this.Seq.canBuildFrom[String]).mkString(&quot;(&quot;, &quot; or &quot;, &quot;)&quot;)
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          397
        </td>
        <td>
          12656
          -
          12662
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DataType.sql
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          dt.sql
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          398
        </td>
        <td>
          12641
          -
          12663
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.quoteByDefault
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          QualitySparkUtils.this.quoteByDefault(dt.sql)
        </td>
      </tr><tr>
        <td>
          332
        </td>
        <td>
          400
        </td>
        <td>
          12694
          -
          12734
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.toUpperCase
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          at.simpleString.toUpperCase(java.util.Locale.ROOT)
        </td>
      </tr><tr>
        <td>
          332
        </td>
        <td>
          399
        </td>
        <td>
          12722
          -
          12733
        </td>
        <td>
          Select
        </td>
        <td>
          java.util.Locale.ROOT
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          java.util.Locale.ROOT
        </td>
      </tr><tr>
        <td>
          332
        </td>
        <td>
          401
        </td>
        <td>
          12679
          -
          12735
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.quoteByDefault
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          QualitySparkUtils.this.quoteByDefault(at.simpleString.toUpperCase(java.util.Locale.ROOT))
        </td>
      </tr><tr>
        <td>
          335
        </td>
        <td>
          403
        </td>
        <td>
          12787
          -
          12817
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.quoteByDefault
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          QualitySparkUtils.this.quoteByDefault(QualitySparkUtils.this.toPrettySQL(e))
        </td>
      </tr><tr>
        <td>
          335
        </td>
        <td>
          402
        </td>
        <td>
          12802
          -
          12816
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.toPrettySQL
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          QualitySparkUtils.this.toPrettySQL(e)
        </td>
      </tr><tr>
        <td>
          338
        </td>
        <td>
          430
        </td>
        <td>
          12878
          -
          13568
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.trees.TreeNode.transform
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          e.transform(({
  @SerialVersionUID(value = 0) final &lt;synthetic&gt; class $anonfun extends scala.runtime.AbstractPartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression] with Serializable {
    def &lt;init&gt;(): &lt;$anon: org.apache.spark.sql.catalyst.expressions.Expression =&gt; org.apache.spark.sql.catalyst.expressions.Expression&gt; = {
      $anonfun.super.&lt;init&gt;();
      ()
    };
    final override def applyOrElse[A1 &lt;: org.apache.spark.sql.catalyst.expressions.Expression, B1 &gt;: org.apache.spark.sql.catalyst.expressions.Expression](x1: A1, default: A1 =&gt; B1): B1 = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (a @ (_: org.apache.spark.sql.catalyst.expressions.Attribute)) =&gt; new org.apache.spark.sql.catalyst.expressions.PrettyAttribute(a)
      case (value: Any, dataType: org.apache.spark.sql.types.DataType)org.apache.spark.sql.catalyst.expressions.Literal((s @ (_: org.apache.spark.unsafe.types.UTF8String)), org.apache.spark.sql.types.StringType) =&gt; org.apache.spark.sql.catalyst.expressions.PrettyAttribute.apply(s.toString(), org.apache.spark.sql.types.StringType)
      case (value: Any, dataType: org.apache.spark.sql.types.DataType)org.apache.spark.sql.catalyst.expressions.Literal((v @ _), (t @ (_: org.apache.spark.sql.types.NumericType))) if v.!=(null) =&gt; org.apache.spark.sql.catalyst.expressions.PrettyAttribute.apply(v.toString(), t)
      case (value: Any, dataType: org.apache.spark.sql.types.DataType)org.apache.spark.sql.catalyst.expressions.Literal(null, (dataType @ _)) =&gt; org.apache.spark.sql.catalyst.expressions.PrettyAttribute.apply(&quot;NULL&quot;, dataType)
      case (e @ (_: org.apache.spark.sql.catalyst.expressions.GetStructField)) =&gt; {
        val name: String = e.name.getOrElse[String](e.childSchema.apply(e.ordinal).name);
        org.apache.spark.sql.catalyst.expressions.PrettyAttribute.apply(QualitySparkUtils.this.usePrettyExpression(e.child).sql.+(&quot;.&quot;).+(name), e.dataType)
      }
      case (e @ (_: org.apache.spark.sql.catalyst.expressions.GetArrayStructFields)) =&gt; org.apache.spark.sql.catalyst.expressions.PrettyAttribute.apply(scala.Predef.any2stringadd[org.apache.spark.sql.catalyst.expressions.Expression](QualitySparkUtils.this.usePrettyExpression(e.child)).+(&quot;.&quot;).+(e.field.name), e.dataType)
      case (c @ (_: org.apache.spark.sql.catalyst.expressions.Cast)) =&gt; org.apache.spark.sql.catalyst.expressions.PrettyAttribute.apply(QualitySparkUtils.this.usePrettyExpression(c.child).sql, c.dataType)
      case (defaultCase$ @ _) =&gt; default.apply(x1)
    };
    final def isDefinedAt(x1: org.apache.spark.sql.catalyst.expressions.Expression): Boolean = ((x1.asInstanceOf[org.apache.spark.sql.catalyst.expressions.Expression]: org.apache.spark.sql.catalyst.expressions.Expression): org.apache.spark.sql.catalyst.expressions.Expression @unchecked) match {
      case (a @ (_: org.apache.spark.sql.catalyst.expressions.Attribute)) =&gt; true
      case (value: Any, dataType: org.apache.spark.sql.types.DataType)org.apache.spark.sql.catalyst.expressions.Literal((s @ (_: org.apache.spark.unsafe.types.UTF8String)), org.apache.spark.sql.types.StringType) =&gt; true
      case (value: Any, dataType: org.apache.spark.sql.types.DataType)org.apache.spark.sql.catalyst.expressions.Literal((v @ _), (t @ (_: org.apache.spark.sql.types.NumericType))) if v.!=(null) =&gt; true
      case (value: Any, dataType: org.apache.spark.sql.types.DataType)org.apache.spark.sql.catalyst.expressions.Literal(null, (dataType @ _)) =&gt; true
      case (e @ (_: org.apache.spark.sql.catalyst.expressions.GetStructField)) =&gt; true
      case (e @ (_: org.apache.spark.sql.catalyst.expressions.GetArrayStructFields)) =&gt; true
      case (c @ (_: org.apache.spark.sql.catalyst.expressions.Cast)) =&gt; true
      case (defaultCase$ @ _) =&gt; false
    }
  };
  new $anonfun()
}: PartialFunction[org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression]))
        </td>
      </tr><tr>
        <td>
          338
        </td>
        <td>
          429
        </td>
        <td>
          12890
          -
          12890
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.$anonfun.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          new $anonfun()
        </td>
      </tr><tr>
        <td>
          339
        </td>
        <td>
          404
        </td>
        <td>
          12917
          -
          12939
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.PrettyAttribute.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          new org.apache.spark.sql.catalyst.expressions.PrettyAttribute(a)
        </td>
      </tr><tr>
        <td>
          340
        </td>
        <td>
          406
        </td>
        <td>
          13015
          -
          13025
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StringType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StringType
        </td>
      </tr><tr>
        <td>
          340
        </td>
        <td>
          405
        </td>
        <td>
          13003
          -
          13013
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.unsafe.types.UTF8String.toString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          s.toString()
        </td>
      </tr><tr>
        <td>
          340
        </td>
        <td>
          407
        </td>
        <td>
          12987
          -
          13026
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.PrettyAttribute.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.PrettyAttribute.apply(s.toString(), org.apache.spark.sql.types.StringType)
        </td>
      </tr><tr>
        <td>
          341
        </td>
        <td>
          409
        </td>
        <td>
          13095
          -
          13105
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.toString
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          v.toString()
        </td>
      </tr><tr>
        <td>
          341
        </td>
        <td>
          408
        </td>
        <td>
          13066
          -
          13075
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.!=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          v.!=(null)
        </td>
      </tr><tr>
        <td>
          341
        </td>
        <td>
          410
        </td>
        <td>
          13079
          -
          13109
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.PrettyAttribute.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.PrettyAttribute.apply(v.toString(), t)
        </td>
      </tr><tr>
        <td>
          342
        </td>
        <td>
          411
        </td>
        <td>
          13146
          -
          13179
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.PrettyAttribute.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.PrettyAttribute.apply(&quot;NULL&quot;, dataType)
        </td>
      </tr><tr>
        <td>
          344
        </td>
        <td>
          412
        </td>
        <td>
          13258
          -
          13267
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.GetStructField.ordinal
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          e.ordinal
        </td>
      </tr><tr>
        <td>
          344
        </td>
        <td>
          414
        </td>
        <td>
          13227
          -
          13274
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Option.getOrElse
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          e.name.getOrElse[String](e.childSchema.apply(e.ordinal).name)
        </td>
      </tr><tr>
        <td>
          344
        </td>
        <td>
          413
        </td>
        <td>
          13244
          -
          13273
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.name
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          e.childSchema.apply(e.ordinal).name
        </td>
      </tr><tr>
        <td>
          345
        </td>
        <td>
          415
        </td>
        <td>
          13297
          -
          13342
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          QualitySparkUtils.this.usePrettyExpression(e.child).sql.+(&quot;.&quot;).+(name)
        </td>
      </tr><tr>
        <td>
          345
        </td>
        <td>
          417
        </td>
        <td>
          13281
          -
          13355
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.PrettyAttribute.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.PrettyAttribute.apply(QualitySparkUtils.this.usePrettyExpression(e.child).sql.+(&quot;.&quot;).+(name), e.dataType)
        </td>
      </tr><tr>
        <td>
          345
        </td>
        <td>
          416
        </td>
        <td>
          13344
          -
          13354
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.GetStructField.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          e.dataType
        </td>
      </tr><tr>
        <td>
          347
        </td>
        <td>
          424
        </td>
        <td>
          13398
          -
          13476
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.PrettyAttribute.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.PrettyAttribute.apply(scala.Predef.any2stringadd[org.apache.spark.sql.catalyst.expressions.Expression](QualitySparkUtils.this.usePrettyExpression(e.child)).+(&quot;.&quot;).+(e.field.name), e.dataType)
        </td>
      </tr><tr>
        <td>
          347
        </td>
        <td>
          418
        </td>
        <td>
          13434
          -
          13441
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.GetArrayStructFields.child
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          e.child
        </td>
      </tr><tr>
        <td>
          347
        </td>
        <td>
          421
        </td>
        <td>
          13451
          -
          13463
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.name
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          e.field.name
        </td>
      </tr><tr>
        <td>
          347
        </td>
        <td>
          420
        </td>
        <td>
          13445
          -
          13448
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.&quot;
        </td>
      </tr><tr>
        <td>
          347
        </td>
        <td>
          423
        </td>
        <td>
          13465
          -
          13475
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.GetArrayStructFields.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          e.dataType
        </td>
      </tr><tr>
        <td>
          347
        </td>
        <td>
          419
        </td>
        <td>
          13414
          -
          13442
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.usePrettyExpression
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          QualitySparkUtils.this.usePrettyExpression(e.child)
        </td>
      </tr><tr>
        <td>
          347
        </td>
        <td>
          422
        </td>
        <td>
          13414
          -
          13463
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.any2stringadd[org.apache.spark.sql.catalyst.expressions.Expression](QualitySparkUtils.this.usePrettyExpression(e.child)).+(&quot;.&quot;).+(e.field.name)
        </td>
      </tr><tr>
        <td>
          349
        </td>
        <td>
          427
        </td>
        <td>
          13553
          -
          13563
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Cast.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          c.dataType
        </td>
      </tr><tr>
        <td>
          349
        </td>
        <td>
          426
        </td>
        <td>
          13519
          -
          13551
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.sql
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          QualitySparkUtils.this.usePrettyExpression(c.child).sql
        </td>
      </tr><tr>
        <td>
          349
        </td>
        <td>
          428
        </td>
        <td>
          13503
          -
          13564
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.PrettyAttribute.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.PrettyAttribute.apply(QualitySparkUtils.this.usePrettyExpression(c.child).sql, c.dataType)
        </td>
      </tr><tr>
        <td>
          349
        </td>
        <td>
          425
        </td>
        <td>
          13539
          -
          13546
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Cast.child
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          c.child
        </td>
      </tr><tr>
        <td>
          352
        </td>
        <td>
          431
        </td>
        <td>
          13613
          -
          13639
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.sql
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          QualitySparkUtils.this.usePrettyExpression(e).sql
        </td>
      </tr><tr>
        <td>
          354
        </td>
        <td>
          432
        </td>
        <td>
          13753
          -
          13773
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Literal.create
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.expressions.Literal.create(v, t)
        </td>
      </tr><tr>
        <td>
          355
        </td>
        <td>
          433
        </td>
        <td>
          13811
          -
          13817
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;NULL&quot;
        </td>
      </tr><tr>
        <td>
          357
        </td>
        <td>
          436
        </td>
        <td>
          13878
          -
          13883
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;NaN&quot;
        </td>
      </tr><tr>
        <td>
          357
        </td>
        <td>
          435
        </td>
        <td>
          13878
          -
          13883
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;NaN&quot;
        </td>
      </tr><tr>
        <td>
          357
        </td>
        <td>
          434
        </td>
        <td>
          13869
          -
          13876
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Float.isNaN
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.float2Float(v).isNaN()
        </td>
      </tr><tr>
        <td>
          358
        </td>
        <td>
          439
        </td>
        <td>
          13916
          -
          13926
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;Infinity&quot;
        </td>
      </tr><tr>
        <td>
          358
        </td>
        <td>
          438
        </td>
        <td>
          13916
          -
          13926
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;Infinity&quot;
        </td>
      </tr><tr>
        <td>
          358
        </td>
        <td>
          437
        </td>
        <td>
          13899
          -
          13914
        </td>
        <td>
          Select
        </td>
        <td>
          scala.runtime.RichFloat.isPosInfinity
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.floatWrapper(v).isPosInfinity
        </td>
      </tr><tr>
        <td>
          358
        </td>
        <td>
          446
        </td>
        <td>
          13895
          -
          13992
        </td>
        <td>
          If
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          if (scala.Predef.floatWrapper(v).isPosInfinity)
  &quot;Infinity&quot;
else
  if (scala.Predef.floatWrapper(v).isNegInfinity)
    &quot;-Infinity&quot;
  else
    v.toString()
        </td>
      </tr><tr>
        <td>
          359
        </td>
        <td>
          442
        </td>
        <td>
          13959
          -
          13970
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;-Infinity&quot;
        </td>
      </tr><tr>
        <td>
          359
        </td>
        <td>
          445
        </td>
        <td>
          13938
          -
          13992
        </td>
        <td>
          If
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          if (scala.Predef.floatWrapper(v).isNegInfinity)
  &quot;-Infinity&quot;
else
  v.toString()
        </td>
      </tr><tr>
        <td>
          359
        </td>
        <td>
          441
        </td>
        <td>
          13959
          -
          13970
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;-Infinity&quot;
        </td>
      </tr><tr>
        <td>
          359
        </td>
        <td>
          440
        </td>
        <td>
          13942
          -
          13957
        </td>
        <td>
          Select
        </td>
        <td>
          scala.runtime.RichFloat.isNegInfinity
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.floatWrapper(v).isNegInfinity
        </td>
      </tr><tr>
        <td>
          360
        </td>
        <td>
          444
        </td>
        <td>
          13982
          -
          13992
        </td>
        <td>
          Block
        </td>
        <td>
          scala.Any.toString
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          v.toString()
        </td>
      </tr><tr>
        <td>
          360
        </td>
        <td>
          443
        </td>
        <td>
          13982
          -
          13992
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.toString
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          v.toString()
        </td>
      </tr><tr>
        <td>
          362
        </td>
        <td>
          448
        </td>
        <td>
          14059
          -
          14064
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;NaN&quot;
        </td>
      </tr><tr>
        <td>
          362
        </td>
        <td>
          447
        </td>
        <td>
          14050
          -
          14057
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Double.isNaN
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.double2Double(v).isNaN()
        </td>
      </tr><tr>
        <td>
          362
        </td>
        <td>
          449
        </td>
        <td>
          14059
          -
          14064
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;NaN&quot;
        </td>
      </tr><tr>
        <td>
          363
        </td>
        <td>
          451
        </td>
        <td>
          14097
          -
          14107
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;Infinity&quot;
        </td>
      </tr><tr>
        <td>
          363
        </td>
        <td>
          459
        </td>
        <td>
          14076
          -
          14168
        </td>
        <td>
          If
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          if (scala.Predef.doubleWrapper(v).isPosInfinity)
  &quot;Infinity&quot;
else
  if (scala.Predef.doubleWrapper(v).isNegInfinity)
    &quot;-Infinity&quot;
  else
    l.sql
        </td>
      </tr><tr>
        <td>
          363
        </td>
        <td>
          450
        </td>
        <td>
          14080
          -
          14095
        </td>
        <td>
          Select
        </td>
        <td>
          scala.runtime.RichDouble.isPosInfinity
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.doubleWrapper(v).isPosInfinity
        </td>
      </tr><tr>
        <td>
          363
        </td>
        <td>
          452
        </td>
        <td>
          14097
          -
          14107
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;Infinity&quot;
        </td>
      </tr><tr>
        <td>
          364
        </td>
        <td>
          454
        </td>
        <td>
          14140
          -
          14151
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;-Infinity&quot;
        </td>
      </tr><tr>
        <td>
          364
        </td>
        <td>
          453
        </td>
        <td>
          14123
          -
          14138
        </td>
        <td>
          Select
        </td>
        <td>
          scala.runtime.RichDouble.isNegInfinity
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.doubleWrapper(v).isNegInfinity
        </td>
      </tr><tr>
        <td>
          364
        </td>
        <td>
          455
        </td>
        <td>
          14140
          -
          14151
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;-Infinity&quot;
        </td>
      </tr><tr>
        <td>
          364
        </td>
        <td>
          458
        </td>
        <td>
          14119
          -
          14168
        </td>
        <td>
          If
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          if (scala.Predef.doubleWrapper(v).isNegInfinity)
  &quot;-Infinity&quot;
else
  l.sql
        </td>
      </tr><tr>
        <td>
          365
        </td>
        <td>
          457
        </td>
        <td>
          14163
          -
          14168
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Literal.sql
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          l.sql
        </td>
      </tr><tr>
        <td>
          365
        </td>
        <td>
          456
        </td>
        <td>
          14163
          -
          14168
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Literal.sql
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          l.sql
        </td>
      </tr><tr>
        <td>
          366
        </td>
        <td>
          460
        </td>
        <td>
          14183
          -
          14188
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Literal.sql
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          l.sql
        </td>
      </tr><tr>
        <td>
          370
        </td>
        <td>
          461
        </td>
        <td>
          14253
          -
          14271
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;\&quot;&quot;.+(elem).+(&quot;\&quot;&quot;)
        </td>
      </tr>
    </table>
          </div>
        </div>
      </body>
    </html>