<html>
      <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <title id="title">
          org/apache/spark/sql/qualityFunctions/Hash.scala.html
        </title>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/css/theme.default.min.css" type="text/css"/><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/js/jquery.tablesorter.min.js"></script><link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css" type="text/css"/><script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script><script type="text/javascript">
        $(document).ready(function() {$(".tablesorter").tablesorter();});
      </script>
        <style>
          table.codegrid { font-family: monospace; font-size: 12px; width: auto!important; }table.statementlist { width: auto!important; font-size: 13px; } table.codegrid td { padding: 0!important; border: 0!important } table td.linenumber { width: 40px!important; } 
        </style>
      </head>
      <body style="font-family: monospace;">
        <ul class="nav nav-tabs">
          <li>
            <a href="#codegrid" data-toggle="tab">Codegrid</a>
          </li>
          <li>
            <a href="#statementlist" data-toggle="tab">Statement List</a>
          </li>
        </ul>
        <div class="tab-content">
          <div class="tab-pane active" id="codegrid">
            <pre style='font-size: 12pt; font-family: courier, monospace;'>1 <span style=''>package org.apache.spark.sql.qualityFunctions
</span>2 <span style=''>
</span>3 <span style=''>import org.apache.spark.sql.QualitySparkUtils
</span>4 <span style=''>import org.apache.spark.sql.catalyst.InternalRow
</span>5 <span style=''>import org.apache.spark.sql.catalyst.analysis.TypeCheckResult
</span>6 <span style=''>import org.apache.spark.sql.catalyst.expressions.Expression
</span>7 <span style=''>import org.apache.spark.sql.catalyst.expressions.codegen.Block._
</span>8 <span style=''>import org.apache.spark.sql.catalyst.expressions.codegen._
</span>9 <span style=''>import org.apache.spark.sql.catalyst.util.{ArrayData, GenericArrayData, MapData}
</span>10 <span style=''>import org.apache.spark.sql.internal.SQLConf
</span>11 <span style=''>import org.apache.spark.sql.types._
</span>12 <span style=''>import org.apache.spark.unsafe.Platform
</span>13 <span style=''>import org.apache.spark.unsafe.types.{CalendarInterval, UTF8String}
</span>14 <span style=''>
</span>15 <span style=''>import scala.annotation.tailrec
</span>16 <span style=''>
</span>17 <span style=''>//CTw - this is copied 1:1 from the main dist replacing E and Long with Array[Long] for variable length hashes
</span>18 <span style=''>// seed gets replaced with a type that returns Array[Long] and includes generation / reset for each new digest with
</span>19 <span style=''>// a clear MessageDigest impl
</span>20 <span style=''>
</span>21 <span style=''>/**
</span>22 <span style=''> * Basic digest implementation for Array[Long] based hashes
</span>23 <span style=''> */
</span>24 <span style=''>trait Digest {
</span>25 <span style=''>  def hashInt(i: Int): Unit
</span>26 <span style=''>
</span>27 <span style=''>  def hashLong(l: Long): Unit
</span>28 <span style=''>
</span>29 <span style=''>  def hashBytes(base: Array[Byte], offset: Int, length: Int): Unit
</span>30 <span style=''>
</span>31 <span style=''>  def digest: Array[Long]
</span>32 <span style=''>}
</span>33 <span style=''>
</span>34 <span style=''>/**
</span>35 <span style=''> * Factory to get a new or reset digest for each row
</span>36 <span style=''> */
</span>37 <span style=''>trait DigestFactory extends Serializable {
</span>38 <span style=''>  def fresh: Digest
</span>39 <span style=''>  def length: Int
</span>40 <span style=''>}
</span>41 <span style=''>
</span>42 <span style=''>/**
</span>43 <span style=''> * A function that calculates hash value for a group of expressions.  Note that the `seed` argument
</span>44 <span style=''> * is not exposed to users and should only be set inside spark SQL.
</span>45 <span style=''> *
</span>46 <span style=''> * The hash value for an expression depends on its type and seed:
</span>47 <span style=''> *  - null:                    seed
</span>48 <span style=''> *  - boolean:                 turn boolean into int, 1 for true, 0 for false,
</span>49 <span style=''> *                             and then use murmur3 to hash this int with seed.
</span>50 <span style=''> *  - byte, short, int:        use murmur3 to hash the input as int with seed.
</span>51 <span style=''> *  - long:                    use murmur3 to hash the long input with seed.
</span>52 <span style=''> *  - float:                   turn it into int: java.lang.Float.floatToIntBits(input), and hash it.
</span>53 <span style=''> *  - double:                  turn it into long: java.lang.Double.doubleToLongBits(input),
</span>54 <span style=''> *                             and hash it.
</span>55 <span style=''> *  - decimal:                 if it's a small decimal, i.e. precision &lt;= 18, turn it into long
</span>56 <span style=''> *                             and hash it. Else, turn it into bytes and hash it.
</span>57 <span style=''> *  - calendar interval:       hash `microseconds` first, and use the result as seed
</span>58 <span style=''> *                             to hash `months`.
</span>59 <span style=''> *  - interval day to second:  it store long value of `microseconds`, use murmur3 to hash the long
</span>60 <span style=''> *                             input with seed.
</span>61 <span style=''> *  - interval year to month:  it store int value of `months`, use murmur3 to hash the int
</span>62 <span style=''> *                             input with seed.
</span>63 <span style=''> *  - binary:                  use murmur3 to hash the bytes with seed.
</span>64 <span style=''> *  - string:                  get the bytes of string and hash it.
</span>65 <span style=''> *  - array:                   The `result` starts with seed, then use `result` as seed, recursively
</span>66 <span style=''> *                             calculate hash value for each element, and assign the element hash
</span>67 <span style=''> *                             value to `result`.
</span>68 <span style=''> *  - struct:                  The `result` starts with seed, then use `result` as seed, recursively
</span>69 <span style=''> *                             calculate hash value for each field, and assign the field hash value
</span>70 <span style=''> *                             to `result`.
</span>71 <span style=''> *
</span>72 <span style=''> * Finally we aggregate the hash values for each expression by the same way of struct.
</span>73 <span style=''> */
</span>74 <span style=''>abstract class HashLongsExpression extends Expression with CodegenFallback {
</span>75 <span style=''>  val factory: DigestFactory
</span>76 <span style=''>
</span>77 <span style=''>  val asStruct: Boolean
</span>78 <span style=''>
</span>79 <span style=''>  override def dataType: DataType =
</span>80 <span style=''>    if (</span><span style='background: #AEF1AE'>asStruct</span><span style=''>)
</span>81 <span style=''>      </span><span style='background: #AEF1AE'>StructType(
</span>82 <span style=''></span><span style='background: #AEF1AE'>        (0 until factory.length).map(i =&gt; StructField(name = &quot;i&quot;+i, dataType = LongType))
</span>83 <span style=''></span><span style='background: #AEF1AE'>      )</span><span style=''>
</span>84 <span style=''>    else
</span>85 <span style=''>      </span><span style='background: #F0ADAD'>ArrayType(LongType)</span><span style=''>
</span>86 <span style=''>
</span>87 <span style=''>  override def foldable: Boolean = </span><span style='background: #AEF1AE'>children.forall(_.foldable)</span><span style=''>
</span>88 <span style=''>
</span>89 <span style=''>  override def nullable: Boolean = </span><span style='background: #AEF1AE'>false</span><span style=''>
</span>90 <span style=''>
</span>91 <span style=''>  private def hasMapType(dt: DataType): Boolean = {
</span>92 <span style=''>    </span><span style='background: #AEF1AE'>dt.existsRecursively(_.isInstanceOf[MapType])</span><span style=''>
</span>93 <span style=''>  }
</span>94 <span style=''>
</span>95 <span style=''>  override def checkInputDataTypes(): TypeCheckResult = {
</span>96 <span style=''>    if (</span><span style='background: #AEF1AE'>children.length &lt; 1</span><span style=''>) {
</span>97 <span style=''>      </span><span style='background: #F0ADAD'>TypeCheckResult.TypeCheckFailure(
</span>98 <span style=''></span><span style='background: #F0ADAD'>        s&quot;input to function $prettyName requires at least one argument&quot;)</span><span style=''>
</span>99 <span style=''>    } /*
</span>100 <span style=''>original code but we'll assume it can't be disabled
</span>101 <span style=''>    else if (children.exists(child =&gt; hasMapType(child.dataType)) &amp;&amp;
</span>102 <span style=''>      !SQLConf.get.getConf(SQLConf.LEGACY_ALLOW_HASH_ON_MAPTYPE)) {
</span>103 <span style=''>
</span>104 <span style=''>      TypeCheckResult.TypeCheckFailure(
</span>105 <span style=''>        s&quot;input to function $prettyName cannot contain elements of MapType. In Spark, same maps &quot; +
</span>106 <span style=''>          &quot;may have different hashcode, thus hash expressions are prohibited on MapType elements.&quot; +
</span>107 <span style=''>          s&quot; To restore previous behavior set ${SQLConf.LEGACY_ALLOW_HASH_ON_MAPTYPE.key} &quot; +
</span>108 <span style=''>          &quot;to true.&quot;)
</span>109 <span style=''>       */
</span>110 <span style=''>    else </span><span style='background: #AEF1AE'>if (children.exists(child =&gt; hasMapType(child.dataType))) {
</span>111 <span style=''></span><span style='background: #AEF1AE'>      </span><span style='background: #F0ADAD'>TypeCheckResult.TypeCheckFailure(
</span>112 <span style=''></span><span style='background: #F0ADAD'>        s&quot;input to function $prettyName cannot contain elements of MapType. In Spark, same maps &quot; +
</span>113 <span style=''></span><span style='background: #F0ADAD'>          &quot;may have different hashcode, thus hash expressions are prohibited on MapType elements.&quot;)</span><span style='background: #AEF1AE'>
</span>114 <span style=''></span><span style='background: #AEF1AE'>    } else {
</span>115 <span style=''></span><span style='background: #AEF1AE'>      TypeCheckResult.TypeCheckSuccess
</span>116 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>117 <span style=''>  }
</span>118 <span style=''>
</span>119 <span style=''>  override def eval(input: InternalRow = null): Any = {
</span>120 <span style=''>    val hash = </span><span style='background: #AEF1AE'>factory.fresh</span><span style=''>
</span>121 <span style=''>    var i = </span><span style='background: #AEF1AE'>0</span><span style=''>
</span>122 <span style=''>    val len = </span><span style='background: #AEF1AE'>children.length</span><span style=''>
</span>123 <span style=''>    while (</span><span style='background: #AEF1AE'>i &lt; len</span><span style=''>) </span><span style='background: #AEF1AE'>{
</span>124 <span style=''></span><span style='background: #AEF1AE'>      computeHash(children(i).eval(input), children(i).dataType, hash)
</span>125 <span style=''></span><span style='background: #AEF1AE'>      i += 1
</span>126 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>127 <span style=''>    if (</span><span style='background: #AEF1AE'>asStruct</span><span style=''>)
</span>128 <span style=''>      </span><span style='background: #AEF1AE'>InternalRow(hash.digest :_*)</span><span style=''> // make the array nested
</span>129 <span style=''>    else
</span>130 <span style=''>      </span><span style='background: #F0ADAD'>new GenericArrayData(hash.digest)</span><span style=''>
</span>131 <span style=''>  }
</span>132 <span style=''>
</span>133 <span style=''>  protected def computeHash(value: Any, dataType: DataType, hash: Digest): Unit
</span>134 <span style=''>/*
</span>135 <span style=''>  override def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {
</span>136 <span style=''>    ev.isNull = FalseLiteral
</span>137 <span style=''>
</span>138 <span style=''>    val childrenHash = children.map { child =&gt;
</span>139 <span style=''>      val childGen = child.genCode(ctx)
</span>140 <span style=''>      childGen.code + ctx.nullSafeExec(child.nullable, childGen.isNull) {
</span>141 <span style=''>        computeHash(childGen.value, child.dataType, ev.value, ctx)
</span>142 <span style=''>      }
</span>143 <span style=''>    }
</span>144 <span style=''>
</span>145 <span style=''>    val hashResultType = &quot;Long[]&quot;
</span>146 <span style=''>    val typedSeed = s&quot;org.apache.spark.sql.qualityFunctions.Digest&quot;
</span>147 <span style=''>    val codes = ctx.splitExpressionsWithCurrentInputs(
</span>148 <span style=''>      expressions = childrenHash,
</span>149 <span style=''>      funcName = &quot;computeHash&quot;,
</span>150 <span style=''>      extraArguments = Seq(), // hashResultType -&gt; ev.value , not needed
</span>151 <span style=''>      returnType = &quot;void&quot;,
</span>152 <span style=''>      makeSplitFunction = body =&gt;
</span>153 <span style=''>        s&quot;&quot;&quot;
</span>154 <span style=''>           |$body
</span>155 <span style=''>         &quot;&quot;&quot;.stripMargin,
</span>156 <span style=''>      foldFunctions = _.map(funcCall =&gt; s&quot;${ev.value} = $funcCall;&quot;).mkString(&quot;\n&quot;))
</span>157 <span style=''>
</span>158 <span style=''>    ev.copy(code =
</span>159 <span style=''>      code&quot;&quot;&quot;
</span>160 <span style=''>            |$hashResultType ${ev.value} = $typedSeed;
</span>161 <span style=''>            |$codes
</span>162 <span style=''>       &quot;&quot;&quot;.stripMargin)
</span>163 <span style=''>  }
</span>164 <span style=''>*/
</span>165 <span style=''>  protected def nullSafeElementHash(
</span>166 <span style=''>                                     input: String,
</span>167 <span style=''>                                     index: String,
</span>168 <span style=''>                                     nullable: Boolean,
</span>169 <span style=''>                                     elementType: DataType,
</span>170 <span style=''>                                     result: String,
</span>171 <span style=''>                                     ctx: CodegenContext): String = {
</span>172 <span style=''>    val element = </span><span style='background: #F0ADAD'>ctx.freshName(&quot;element&quot;)</span><span style=''>
</span>173 <span style=''>
</span>174 <span style=''>    val jt = </span><span style='background: #F0ADAD'>CodeGenerator.javaType(elementType)</span><span style=''>
</span>175 <span style=''>    </span><span style='background: #F0ADAD'>ctx.nullSafeExec(nullable, s&quot;$input.isNullAt($index)&quot;) {
</span>176 <span style=''></span><span style='background: #F0ADAD'>      s&quot;&quot;&quot;
</span>177 <span style=''></span><span style='background: #F0ADAD'>        final $jt $element = ${CodeGenerator.getValue(input, elementType, index)};
</span>178 <span style=''></span><span style='background: #F0ADAD'>        ${computeHash(element, elementType, result, ctx)}
</span>179 <span style=''></span><span style='background: #F0ADAD'>      &quot;&quot;&quot;
</span>180 <span style=''></span><span style='background: #F0ADAD'>    }</span><span style=''>
</span>181 <span style=''>  }
</span>182 <span style=''>
</span>183 <span style=''>  protected def genHashInt(i: String, result: String): String =
</span>184 <span style=''>    </span><span style='background: #F0ADAD'>s&quot;$result = $hasherClassName.hashInt($i, $result);&quot;</span><span style=''>
</span>185 <span style=''>
</span>186 <span style=''>  protected def genHashLong(l: String, result: String): String =
</span>187 <span style=''>    </span><span style='background: #F0ADAD'>s&quot;$result = $hasherClassName.hashLong($l, $result);&quot;</span><span style=''>
</span>188 <span style=''>
</span>189 <span style=''>  protected def genHashBytes(b: String, result: String): String = {
</span>190 <span style=''>    val offset = </span><span style='background: #F0ADAD'>&quot;Platform.BYTE_ARRAY_OFFSET&quot;</span><span style=''>
</span>191 <span style=''>    </span><span style='background: #F0ADAD'>s&quot;$result = $hasherClassName.hashUnsafeBytes($b, $offset, $b.length, $result);&quot;</span><span style=''>
</span>192 <span style=''>  }
</span>193 <span style=''>
</span>194 <span style=''>  protected def genHashBoolean(input: String, result: String): String =
</span>195 <span style=''>    </span><span style='background: #F0ADAD'>genHashInt(s&quot;$input ? 1 : 0&quot;, result)</span><span style=''>
</span>196 <span style=''>
</span>197 <span style=''>  protected def genHashFloat(input: String, result: String): String = {
</span>198 <span style=''>    </span><span style='background: #F0ADAD'>s&quot;&quot;&quot;
</span>199 <span style=''></span><span style='background: #F0ADAD'>       |if($input == -0.0f) {
</span>200 <span style=''></span><span style='background: #F0ADAD'>       |  ${genHashInt(&quot;0&quot;, result)}
</span>201 <span style=''></span><span style='background: #F0ADAD'>       |} else {
</span>202 <span style=''></span><span style='background: #F0ADAD'>       |  ${genHashInt(s&quot;Float.floatToIntBits($input)&quot;, result)}
</span>203 <span style=''></span><span style='background: #F0ADAD'>       |}
</span>204 <span style=''></span><span style='background: #F0ADAD'>     &quot;&quot;&quot;.stripMargin</span><span style=''>
</span>205 <span style=''>  }
</span>206 <span style=''>
</span>207 <span style=''>  protected def genHashDouble(input: String, result: String): String = {
</span>208 <span style=''>    </span><span style='background: #F0ADAD'>s&quot;&quot;&quot;
</span>209 <span style=''></span><span style='background: #F0ADAD'>       |if($input == -0.0d) {
</span>210 <span style=''></span><span style='background: #F0ADAD'>       |  ${genHashLong(&quot;0L&quot;, result)}
</span>211 <span style=''></span><span style='background: #F0ADAD'>       |} else {
</span>212 <span style=''></span><span style='background: #F0ADAD'>       |  ${genHashLong(s&quot;Double.doubleToLongBits($input)&quot;, result)}
</span>213 <span style=''></span><span style='background: #F0ADAD'>       |}
</span>214 <span style=''></span><span style='background: #F0ADAD'>     &quot;&quot;&quot;.stripMargin</span><span style=''>
</span>215 <span style=''>  }
</span>216 <span style=''>
</span>217 <span style=''>  protected def genHashDecimal(
</span>218 <span style=''>                                ctx: CodegenContext,
</span>219 <span style=''>                                d: DecimalType,
</span>220 <span style=''>                                input: String,
</span>221 <span style=''>                                result: String): String = {
</span>222 <span style=''>    if (</span><span style='background: #F0ADAD'>d.precision &lt;= Decimal.MAX_LONG_DIGITS</span><span style=''>) {
</span>223 <span style=''>      </span><span style='background: #F0ADAD'>genHashLong(s&quot;$input.toUnscaledLong()&quot;, result)</span><span style=''>
</span>224 <span style=''>    } else </span><span style='background: #F0ADAD'>{
</span>225 <span style=''></span><span style='background: #F0ADAD'>      val bytes = ctx.freshName(&quot;bytes&quot;)
</span>226 <span style=''></span><span style='background: #F0ADAD'>      s&quot;&quot;&quot;
</span>227 <span style=''></span><span style='background: #F0ADAD'>         |final byte[] $bytes = $input.toJavaBigDecimal().unscaledValue().toByteArray();
</span>228 <span style=''></span><span style='background: #F0ADAD'>         |${genHashBytes(bytes, result)}
</span>229 <span style=''></span><span style='background: #F0ADAD'>       &quot;&quot;&quot;.stripMargin
</span>230 <span style=''></span><span style='background: #F0ADAD'>    }</span><span style=''>
</span>231 <span style=''>  }
</span>232 <span style=''>
</span>233 <span style=''>  protected def genHashTimestamp(t: String, result: String): String = </span><span style='background: #F0ADAD'>genHashLong(t, result)</span><span style=''>
</span>234 <span style=''>
</span>235 <span style=''>  protected def genHashCalendarInterval(input: String, result: String): String = {
</span>236 <span style=''>    val microsecondsHash = </span><span style='background: #F0ADAD'>s&quot;$hasherClassName.hashLong($input.microseconds, $result)&quot;</span><span style=''>
</span>237 <span style=''>    </span><span style='background: #F0ADAD'>s&quot;$result = $hasherClassName.hashInt($input.months, $microsecondsHash);&quot;</span><span style=''>
</span>238 <span style=''>  }
</span>239 <span style=''>
</span>240 <span style=''>  protected def genHashString(input: String, result: String): String = {
</span>241 <span style=''>    val baseObject = </span><span style='background: #F0ADAD'>s&quot;$input.getBaseObject()&quot;</span><span style=''>
</span>242 <span style=''>    val baseOffset = </span><span style='background: #F0ADAD'>s&quot;$input.getBaseOffset()&quot;</span><span style=''>
</span>243 <span style=''>    val numBytes = </span><span style='background: #F0ADAD'>s&quot;$input.numBytes()&quot;</span><span style=''>
</span>244 <span style=''>    </span><span style='background: #F0ADAD'>s&quot;$result = $hasherClassName.hashUnsafeBytes($baseObject, $baseOffset, $numBytes, $result);&quot;</span><span style=''>
</span>245 <span style=''>  }
</span>246 <span style=''>
</span>247 <span style=''>  protected def genHashForMap(
</span>248 <span style=''>                               ctx: CodegenContext,
</span>249 <span style=''>                               input: String,
</span>250 <span style=''>                               result: String,
</span>251 <span style=''>                               keyType: DataType,
</span>252 <span style=''>                               valueType: DataType,
</span>253 <span style=''>                               valueContainsNull: Boolean): String = {
</span>254 <span style=''>    val index = </span><span style='background: #F0ADAD'>ctx.freshName(&quot;index&quot;)</span><span style=''>
</span>255 <span style=''>    val keys = </span><span style='background: #F0ADAD'>ctx.freshName(&quot;keys&quot;)</span><span style=''>
</span>256 <span style=''>    val values = </span><span style='background: #F0ADAD'>ctx.freshName(&quot;values&quot;)</span><span style=''>
</span>257 <span style=''>    </span><span style='background: #F0ADAD'>s&quot;&quot;&quot;
</span>258 <span style=''></span><span style='background: #F0ADAD'>        final ArrayData $keys = $input.keyArray();
</span>259 <span style=''></span><span style='background: #F0ADAD'>        final ArrayData $values = $input.valueArray();
</span>260 <span style=''></span><span style='background: #F0ADAD'>        for (int $index = 0; $index &lt; $input.numElements(); $index++) {
</span>261 <span style=''></span><span style='background: #F0ADAD'>          ${nullSafeElementHash(keys, index, false, keyType, result, ctx)}
</span>262 <span style=''></span><span style='background: #F0ADAD'>          ${nullSafeElementHash(values, index, valueContainsNull, valueType, result, ctx)}
</span>263 <span style=''></span><span style='background: #F0ADAD'>        }
</span>264 <span style=''></span><span style='background: #F0ADAD'>      &quot;&quot;&quot;</span><span style=''>
</span>265 <span style=''>  }
</span>266 <span style=''>
</span>267 <span style=''>  protected def genHashForArray(
</span>268 <span style=''>                                 ctx: CodegenContext,
</span>269 <span style=''>                                 input: String,
</span>270 <span style=''>                                 result: String,
</span>271 <span style=''>                                 elementType: DataType,
</span>272 <span style=''>                                 containsNull: Boolean): String = {
</span>273 <span style=''>    val index = </span><span style='background: #F0ADAD'>ctx.freshName(&quot;index&quot;)</span><span style=''>
</span>274 <span style=''>    </span><span style='background: #F0ADAD'>s&quot;&quot;&quot;
</span>275 <span style=''></span><span style='background: #F0ADAD'>        for (int $index = 0; $index &lt; $input.numElements(); $index++) {
</span>276 <span style=''></span><span style='background: #F0ADAD'>          ${nullSafeElementHash(input, index, containsNull, elementType, result, ctx)}
</span>277 <span style=''></span><span style='background: #F0ADAD'>        }
</span>278 <span style=''></span><span style='background: #F0ADAD'>      &quot;&quot;&quot;</span><span style=''>
</span>279 <span style=''>  }
</span>280 <span style=''>
</span>281 <span style=''>  protected def genHashForStruct(
</span>282 <span style=''>                                  ctx: CodegenContext,
</span>283 <span style=''>                                  input: String,
</span>284 <span style=''>                                  result: String,
</span>285 <span style=''>                                  fields: Array[StructField]): String = {
</span>286 <span style=''>    val tmpInput = </span><span style='background: #F0ADAD'>ctx.freshName(&quot;input&quot;)</span><span style=''>
</span>287 <span style=''>    val fieldsHash = </span><span style='background: #F0ADAD'>fields.zipWithIndex.map { case (field, index) =&gt;
</span>288 <span style=''></span><span style='background: #F0ADAD'>      nullSafeElementHash(tmpInput, index.toString, field.nullable, field.dataType, result, ctx)
</span>289 <span style=''></span><span style='background: #F0ADAD'>    }</span><span style=''>
</span>290 <span style=''>    val hashResultType = </span><span style='background: #F0ADAD'>CodeGenerator.javaType(dataType)</span><span style=''>
</span>291 <span style=''>    val code = </span><span style='background: #F0ADAD'>ctx.splitExpressions(
</span>292 <span style=''></span><span style='background: #F0ADAD'>      expressions = fieldsHash,
</span>293 <span style=''></span><span style='background: #F0ADAD'>      funcName = &quot;computeHashForStruct&quot;,
</span>294 <span style=''></span><span style='background: #F0ADAD'>      arguments = Seq(&quot;InternalRow&quot; -&gt; tmpInput, hashResultType -&gt; result),
</span>295 <span style=''></span><span style='background: #F0ADAD'>      returnType = hashResultType,
</span>296 <span style=''></span><span style='background: #F0ADAD'>      makeSplitFunction = body =&gt;
</span>297 <span style=''></span><span style='background: #F0ADAD'>        s&quot;&quot;&quot;
</span>298 <span style=''></span><span style='background: #F0ADAD'>           |$body
</span>299 <span style=''></span><span style='background: #F0ADAD'>           |return $result;
</span>300 <span style=''></span><span style='background: #F0ADAD'>         &quot;&quot;&quot;.stripMargin,
</span>301 <span style=''></span><span style='background: #F0ADAD'>      foldFunctions = _.map(funcCall =&gt; s&quot;$result = $funcCall;&quot;).mkString(&quot;\n&quot;))</span><span style=''>
</span>302 <span style=''>    </span><span style='background: #F0ADAD'>s&quot;&quot;&quot;
</span>303 <span style=''></span><span style='background: #F0ADAD'>       |final InternalRow $tmpInput = $input;
</span>304 <span style=''></span><span style='background: #F0ADAD'>       |$code
</span>305 <span style=''></span><span style='background: #F0ADAD'>     &quot;&quot;&quot;.stripMargin</span><span style=''>
</span>306 <span style=''>  }
</span>307 <span style=''>
</span>308 <span style=''>  @tailrec
</span>309 <span style=''>  private def computeHashWithTailRec(
</span>310 <span style=''>                                      input: String,
</span>311 <span style=''>                                      dataType: DataType,
</span>312 <span style=''>                                      result: String,
</span>313 <span style=''>                                      ctx: CodegenContext): String = dataType match {
</span>314 <span style=''>    case NullType =&gt; </span><span style='background: #F0ADAD'>&quot;&quot;</span><span style=''>
</span>315 <span style=''>    case BooleanType =&gt; </span><span style='background: #F0ADAD'>genHashBoolean(input, result)</span><span style=''>
</span>316 <span style=''>    case ByteType | ShortType | IntegerType | DateType =&gt; </span><span style='background: #F0ADAD'>genHashInt(input, result)</span><span style=''>
</span>317 <span style=''>    case LongType =&gt; </span><span style='background: #F0ADAD'>genHashLong(input, result)</span><span style=''>
</span>318 <span style=''>    case TimestampType =&gt; </span><span style='background: #F0ADAD'>genHashTimestamp(input, result)</span><span style=''>
</span>319 <span style=''>    case FloatType =&gt; </span><span style='background: #F0ADAD'>genHashFloat(input, result)</span><span style=''>
</span>320 <span style=''>    case DoubleType =&gt; </span><span style='background: #F0ADAD'>genHashDouble(input, result)</span><span style=''>
</span>321 <span style=''>    case d: DecimalType =&gt; </span><span style='background: #F0ADAD'>genHashDecimal(ctx, d, input, result)</span><span style=''>
</span>322 <span style=''>    case CalendarIntervalType =&gt; </span><span style='background: #F0ADAD'>genHashCalendarInterval(input, result)</span><span style=''>
</span>323 <span style=''>      // TODO figure these out
</span>324 <span style=''>//    case _: DayTimeIntervalType =&gt; genHashLong(input, result)
</span>325 <span style=''>//    case YearMonthIntervalType =&gt; genHashInt(input, result)
</span>326 <span style=''>    case BinaryType =&gt; </span><span style='background: #F0ADAD'>genHashBytes(input, result)</span><span style=''>
</span>327 <span style=''>    case StringType =&gt; </span><span style='background: #F0ADAD'>genHashString(input, result)</span><span style=''>
</span>328 <span style=''>    case ArrayType(et, containsNull) =&gt; </span><span style='background: #F0ADAD'>genHashForArray(ctx, input, result, et, containsNull)</span><span style=''>
</span>329 <span style=''>    case MapType(kt, vt, valueContainsNull) =&gt;
</span>330 <span style=''>      </span><span style='background: #F0ADAD'>genHashForMap(ctx, input, result, kt, vt, valueContainsNull)</span><span style=''>
</span>331 <span style=''>    case StructType(fields) =&gt; </span><span style='background: #F0ADAD'>genHashForStruct(ctx, input, result, fields)</span><span style=''>
</span>332 <span style=''>    case udt: UserDefinedType[_] =&gt; </span><span style='background: #F0ADAD'>computeHashWithTailRec(input, udt.sqlType, result, ctx)</span><span style=''>
</span>333 <span style=''>  }
</span>334 <span style=''>
</span>335 <span style=''>  protected def computeHash(
</span>336 <span style=''>                             input: String,
</span>337 <span style=''>                             dataType: DataType,
</span>338 <span style=''>                             result: String,
</span>339 <span style=''>                             ctx: CodegenContext): String = </span><span style='background: #AEF1AE'>computeHashWithTailRec(input, dataType, result, ctx)</span><span style=''>
</span>340 <span style=''>
</span>341 <span style=''>  protected def hasherClassName: String
</span>342 <span style=''>}
</span>343 <span style=''>
</span>344 <span style=''>object SafeUTF8 {
</span>345 <span style=''>  /**
</span>346 <span style=''>   * Returns the actual byte array if it's a byte array, otherwise gets the bytes serialisation of it
</span>347 <span style=''>   * @param s
</span>348 <span style=''>   * @return
</span>349 <span style=''>   */
</span>350 <span style=''>  def safeUT8ByteArray(s: UTF8String): (Array[Byte], Int, Int) = {
</span>351 <span style=''>    if (</span><span style='background: #AEF1AE'>s.getBaseObject.isInstanceOf[Array[Byte]] &amp;&amp; s.getBaseOffset &gt;= Platform.BYTE_ARRAY_OFFSET.toLong</span><span style=''>) </span><span style='background: #AEF1AE'>{
</span>352 <span style=''></span><span style='background: #AEF1AE'>      val bytes = s.getBaseObject.asInstanceOf[Array[Byte]].asInstanceOf[Array[Byte]]
</span>353 <span style=''></span><span style='background: #AEF1AE'>      val arrayOffset = s.getBaseOffset - Platform.BYTE_ARRAY_OFFSET.toLong
</span>354 <span style=''></span><span style='background: #AEF1AE'>      if (bytes.length.toLong &lt; arrayOffset + s.numBytes.toLong) </span><span style='background: #F0ADAD'>throw new ArrayIndexOutOfBoundsException</span><span style='background: #AEF1AE'>
</span>355 <span style=''></span><span style='background: #AEF1AE'>      else
</span>356 <span style=''></span><span style='background: #AEF1AE'>        (bytes, arrayOffset.toInt, s.numBytes)
</span>357 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>358 <span style=''>    else {
</span>359 <span style=''>      </span><span style='background: #F0ADAD'>(s.getBytes, 0, s.numBytes)</span><span style=''>
</span>360 <span style=''>    }
</span>361 <span style=''>  }
</span>362 <span style=''>}
</span>363 <span style=''>
</span>364 <span style=''>/**
</span>365 <span style=''> * Base class for interpreted hash functions.
</span>366 <span style=''> */
</span>367 <span style=''>abstract class InterpretedHashLongsFunction {
</span>368 <span style=''>  def hashInt(i: Int, digest: Digest): Digest
</span>369 <span style=''>
</span>370 <span style=''>  def hashLong(l: Long, digest: Digest): Digest
</span>371 <span style=''>
</span>372 <span style=''>  def hashBytes(base: Array[Byte], offset: Int, length: Int, digest: Digest): Digest
</span>373 <span style=''>
</span>374 <span style=''>  /**
</span>375 <span style=''>   * Computes hash of a given `value` of type `dataType`. The caller needs to check the validity
</span>376 <span style=''>   * of input `value`.
</span>377 <span style=''>   */
</span>378 <span style=''>  def hash(value: Any, dataType: DataType, digest: Digest): Digest = {
</span>379 <span style=''>    value match {
</span>380 <span style=''>      case null =&gt; digest
</span>381 <span style=''>      case b: Boolean =&gt; </span><span style='background: #F0ADAD'>hashInt(if (b) 1 else 0, digest)</span><span style=''>
</span>382 <span style=''>      case b: Byte =&gt; </span><span style='background: #F0ADAD'>hashInt(b, digest)</span><span style=''>
</span>383 <span style=''>      case s: Short =&gt; </span><span style='background: #F0ADAD'>hashInt(s, digest)</span><span style=''>
</span>384 <span style=''>      case i: Int =&gt; </span><span style='background: #F0ADAD'>hashInt(i, digest)</span><span style=''>
</span>385 <span style=''>      case l: Long =&gt; </span><span style='background: #F0ADAD'>hashLong(l, digest)</span><span style=''>
</span>386 <span style=''>      case f: Float if (</span><span style='background: #F0ADAD'>f == -0.0f</span><span style=''>) =&gt; </span><span style='background: #F0ADAD'>hashInt(0, digest)</span><span style=''>
</span>387 <span style=''>      case f: Float =&gt; </span><span style='background: #F0ADAD'>hashInt(java.lang.Float.floatToIntBits(f), digest)</span><span style=''>
</span>388 <span style=''>      case d: Double if (</span><span style='background: #F0ADAD'>d == -0.0d</span><span style=''>) =&gt; </span><span style='background: #F0ADAD'>hashLong(0L, digest)</span><span style=''>
</span>389 <span style=''>      case d: Double =&gt; </span><span style='background: #F0ADAD'>hashLong(java.lang.Double.doubleToLongBits(d), digest)</span><span style=''>
</span>390 <span style=''>      case d: Decimal =&gt;
</span>391 <span style=''>        val precision = </span><span style='background: #F0ADAD'>dataType.asInstanceOf[DecimalType].precision</span><span style=''>
</span>392 <span style=''>        if (</span><span style='background: #F0ADAD'>precision &lt;= Decimal.MAX_LONG_DIGITS</span><span style=''>) {
</span>393 <span style=''>          </span><span style='background: #F0ADAD'>hashLong(d.toUnscaledLong, digest)</span><span style=''>
</span>394 <span style=''>        } else </span><span style='background: #F0ADAD'>{
</span>395 <span style=''></span><span style='background: #F0ADAD'>          val bytes = d.toJavaBigDecimal.unscaledValue().toByteArray
</span>396 <span style=''></span><span style='background: #F0ADAD'>          hashBytes(bytes, Platform.BYTE_ARRAY_OFFSET, bytes.length, digest)
</span>397 <span style=''></span><span style='background: #F0ADAD'>        }</span><span style=''>
</span>398 <span style=''>      case c: CalendarInterval =&gt;
</span>399 <span style=''>        </span><span style='background: #F0ADAD'>QualitySparkUtils.hashCalendarInterval(c, this, digest)</span><span style=''>
</span>400 <span style=''>      case a: Array[Byte] =&gt;
</span>401 <span style=''>        </span><span style='background: #AEF1AE'>hashBytes(a, </span><span style='background: #F0ADAD'>Platform.BYTE_ARRAY_OFFSET</span><span style='background: #AEF1AE'>, </span><span style='background: #F0ADAD'>a.length</span><span style='background: #AEF1AE'>, digest)</span><span style=''>
</span>402 <span style=''>      case s: UTF8String =&gt; {
</span>403 <span style=''>        /* */
</span>404 <span style=''>        val (bytes, offset, numbytes) = SafeUTF8.safeUT8ByteArray(s)
</span>405 <span style=''>
</span>406 <span style=''>        </span><span style='background: #AEF1AE'>hashBytes(bytes, offset, numbytes, digest)</span><span style=''>
</span>407 <span style=''>
</span>408 <span style=''>      }
</span>409 <span style=''>
</span>410 <span style=''>      case array: ArrayData =&gt;
</span>411 <span style=''>        val elementType = dataType match {
</span>412 <span style=''>          case udt: UserDefinedType[_] =&gt; </span><span style='background: #F0ADAD'>udt.sqlType.asInstanceOf[ArrayType].elementType</span><span style=''>
</span>413 <span style=''>          case ArrayType(et, _) =&gt; et
</span>414 <span style=''>        }
</span>415 <span style=''>        var result = digest
</span>416 <span style=''>        var i = </span><span style='background: #F0ADAD'>0</span><span style=''>
</span>417 <span style=''>        while (</span><span style='background: #F0ADAD'>i &lt; array.numElements()</span><span style=''>) </span><span style='background: #F0ADAD'>{
</span>418 <span style=''></span><span style='background: #F0ADAD'>          result = hash(array.get(i, elementType), elementType, result)
</span>419 <span style=''></span><span style='background: #F0ADAD'>          i += 1
</span>420 <span style=''></span><span style='background: #F0ADAD'>        }</span><span style=''>
</span>421 <span style=''>        result
</span>422 <span style=''>
</span>423 <span style=''>      case map: MapData =&gt;
</span>424 <span style=''>        val (kt, vt) = dataType match {
</span>425 <span style=''>          case udt: UserDefinedType[_] =&gt;
</span>426 <span style=''>            val mapType = udt.sqlType.asInstanceOf[MapType]
</span>427 <span style=''>            mapType.keyType -&gt; mapType.valueType
</span>428 <span style=''>          case MapType(kt, vt, _) =&gt; kt -&gt; vt
</span>429 <span style=''>        }
</span>430 <span style=''>        val keys = </span><span style='background: #F0ADAD'>map.keyArray()</span><span style=''>
</span>431 <span style=''>        val values = </span><span style='background: #F0ADAD'>map.valueArray()</span><span style=''>
</span>432 <span style=''>        var result = digest
</span>433 <span style=''>        var i = </span><span style='background: #F0ADAD'>0</span><span style=''>
</span>434 <span style=''>        while (</span><span style='background: #F0ADAD'>i &lt; map.numElements()</span><span style=''>) </span><span style='background: #F0ADAD'>{
</span>435 <span style=''></span><span style='background: #F0ADAD'>          result = hash(keys.get(i, kt), kt, result)
</span>436 <span style=''></span><span style='background: #F0ADAD'>          result = hash(values.get(i, vt), vt, result)
</span>437 <span style=''></span><span style='background: #F0ADAD'>          i += 1
</span>438 <span style=''></span><span style='background: #F0ADAD'>        }</span><span style=''>
</span>439 <span style=''>        result
</span>440 <span style=''>
</span>441 <span style=''>      case struct: InternalRow =&gt;
</span>442 <span style=''>        val types: Array[DataType] = dataType match {
</span>443 <span style=''>          case udt: UserDefinedType[_] =&gt;
</span>444 <span style=''>            </span><span style='background: #F0ADAD'>udt.sqlType.asInstanceOf[StructType].map(_.dataType).toArray</span><span style=''>
</span>445 <span style=''>          case StructType(fields) =&gt; </span><span style='background: #F0ADAD'>fields.map(_.dataType)</span><span style=''>
</span>446 <span style=''>        }
</span>447 <span style=''>        var result = digest
</span>448 <span style=''>        var i = </span><span style='background: #F0ADAD'>0</span><span style=''>
</span>449 <span style=''>        val len = </span><span style='background: #F0ADAD'>struct.numFields</span><span style=''>
</span>450 <span style=''>        while (</span><span style='background: #F0ADAD'>i &lt; len</span><span style=''>) </span><span style='background: #F0ADAD'>{
</span>451 <span style=''></span><span style='background: #F0ADAD'>          result = hash(struct.get(i, types(i)), types(i), result)
</span>452 <span style=''></span><span style='background: #F0ADAD'>          i += 1
</span>453 <span style=''></span><span style='background: #F0ADAD'>        }</span><span style=''>
</span>454 <span style=''>        result
</span>455 <span style=''>    }
</span>456 <span style=''>  }
</span>457 <span style=''>}
</span></pre>
          </div>
          <div class="tab-pane" id="statementlist">
            <table cellspacing="0" cellpadding="0" class="table statementlist">
      <tr>
        <th>Line</th>
        <th>Stmt Id</th>
        <th>Pos</th>
        <th>Tree</th>
        <th>Symbol</th>
        <th>Tests</th>
        <th>Code</th>
      </tr><tr>
        <td>
          80
        </td>
        <td>
          7024
        </td>
        <td>
          3666
          -
          3674
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.asStruct
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          HashLongsExpression.this.asStruct
        </td>
      </tr><tr>
        <td>
          81
        </td>
        <td>
          7035
        </td>
        <td>
          3682
          -
          3791
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.types.StructType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructType.apply(scala.Predef.intWrapper(0).until(HashLongsExpression.this.factory.length).map[org.apache.spark.sql.types.StructField, scala.collection.immutable.IndexedSeq[org.apache.spark.sql.types.StructField]](((i: Int) =&gt; org.apache.spark.sql.types.StructField.apply(&quot;i&quot;.+(i), org.apache.spark.sql.types.LongType, org.apache.spark.sql.types.StructField.apply$default$3, org.apache.spark.sql.types.StructField.apply$default$4)))(immutable.this.IndexedSeq.canBuildFrom[org.apache.spark.sql.types.StructField]))
        </td>
      </tr><tr>
        <td>
          81
        </td>
        <td>
          7034
        </td>
        <td>
          3682
          -
          3791
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.StructType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructType.apply(scala.Predef.intWrapper(0).until(HashLongsExpression.this.factory.length).map[org.apache.spark.sql.types.StructField, scala.collection.immutable.IndexedSeq[org.apache.spark.sql.types.StructField]](((i: Int) =&gt; org.apache.spark.sql.types.StructField.apply(&quot;i&quot;.+(i), org.apache.spark.sql.types.LongType, org.apache.spark.sql.types.StructField.apply$default$3, org.apache.spark.sql.types.StructField.apply$default$4)))(immutable.this.IndexedSeq.canBuildFrom[org.apache.spark.sql.types.StructField]))
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          7030
        </td>
        <td>
          3736
          -
          3736
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.apply$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructField.apply$default$4
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          7033
        </td>
        <td>
          3703
          -
          3783
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Predef.intWrapper(0).until(HashLongsExpression.this.factory.length).map[org.apache.spark.sql.types.StructField, scala.collection.immutable.IndexedSeq[org.apache.spark.sql.types.StructField]](((i: Int) =&gt; org.apache.spark.sql.types.StructField.apply(&quot;i&quot;.+(i), org.apache.spark.sql.types.LongType, org.apache.spark.sql.types.StructField.apply$default$3, org.apache.spark.sql.types.StructField.apply$default$4)))(immutable.this.IndexedSeq.canBuildFrom[org.apache.spark.sql.types.StructField])
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          7027
        </td>
        <td>
          3755
          -
          3760
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          &quot;i&quot;.+(i)
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          7029
        </td>
        <td>
          3736
          -
          3736
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructField.apply$default$3
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          7032
        </td>
        <td>
          3730
          -
          3730
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.IndexedSeq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          immutable.this.IndexedSeq.canBuildFrom[org.apache.spark.sql.types.StructField]
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          7026
        </td>
        <td>
          3711
          -
          3725
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.DigestFactory.length
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          HashLongsExpression.this.factory.length
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          7025
        </td>
        <td>
          3703
          -
          3704
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          7028
        </td>
        <td>
          3773
          -
          3781
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          7031
        </td>
        <td>
          3736
          -
          3782
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.StructField.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructField.apply(&quot;i&quot;.+(i), org.apache.spark.sql.types.LongType, org.apache.spark.sql.types.StructField.apply$default$3, org.apache.spark.sql.types.StructField.apply$default$4)
        </td>
      </tr><tr>
        <td>
          85
        </td>
        <td>
          7036
        </td>
        <td>
          3817
          -
          3825
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          85
        </td>
        <td>
          7038
        </td>
        <td>
          3807
          -
          3826
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.types.ArrayType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.ArrayType.apply(org.apache.spark.sql.types.LongType)
        </td>
      </tr><tr>
        <td>
          85
        </td>
        <td>
          7037
        </td>
        <td>
          3807
          -
          3826
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.ArrayType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.ArrayType.apply(org.apache.spark.sql.types.LongType)
        </td>
      </tr><tr>
        <td>
          87
        </td>
        <td>
          7039
        </td>
        <td>
          3879
          -
          3889
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.foldable
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$1.foldable
        </td>
      </tr><tr>
        <td>
          87
        </td>
        <td>
          7040
        </td>
        <td>
          3863
          -
          3890
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.IterableLike.forall
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          HashLongsExpression.this.children.forall(((x$1: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; x$1.foldable))
        </td>
      </tr><tr>
        <td>
          89
        </td>
        <td>
          7041
        </td>
        <td>
          3927
          -
          3932
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          false
        </td>
      </tr><tr>
        <td>
          92
        </td>
        <td>
          7042
        </td>
        <td>
          4011
          -
          4034
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.isInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$2.isInstanceOf[org.apache.spark.sql.types.MapType]
        </td>
      </tr><tr>
        <td>
          92
        </td>
        <td>
          7043
        </td>
        <td>
          3990
          -
          4035
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.DataType.existsRecursively
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          dt.existsRecursively(((x$2: org.apache.spark.sql.types.DataType) =&gt; x$2.isInstanceOf[org.apache.spark.sql.types.MapType]))
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          7044
        </td>
        <td>
          4107
          -
          4126
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          HashLongsExpression.this.children.length.&lt;(1)
        </td>
      </tr><tr>
        <td>
          97
        </td>
        <td>
          7050
        </td>
        <td>
          4136
          -
          4242
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply(scala.StringContext.apply(&quot;input to function &quot;, &quot; requires at least one argument&quot;).s(HashLongsExpression.this.prettyName))
        </td>
      </tr><tr>
        <td>
          97
        </td>
        <td>
          7049
        </td>
        <td>
          4136
          -
          4242
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply(scala.StringContext.apply(&quot;input to function &quot;, &quot; requires at least one argument&quot;).s(HashLongsExpression.this.prettyName))
        </td>
      </tr><tr>
        <td>
          98
        </td>
        <td>
          7048
        </td>
        <td>
          4178
          -
          4241
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;input to function &quot;, &quot; requires at least one argument&quot;).s(HashLongsExpression.this.prettyName)
        </td>
      </tr><tr>
        <td>
          98
        </td>
        <td>
          7045
        </td>
        <td>
          4180
          -
          4199
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;input to function &quot;
        </td>
      </tr><tr>
        <td>
          98
        </td>
        <td>
          7047
        </td>
        <td>
          4199
          -
          4209
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.prettyName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.prettyName
        </td>
      </tr><tr>
        <td>
          98
        </td>
        <td>
          7046
        </td>
        <td>
          4209
          -
          4241
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; requires at least one argument&quot;
        </td>
      </tr><tr>
        <td>
          110
        </td>
        <td>
          7051
        </td>
        <td>
          4858
          -
          4872
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          child.dataType
        </td>
      </tr><tr>
        <td>
          110
        </td>
        <td>
          7053
        </td>
        <td>
          4822
          -
          4874
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.IterableLike.exists
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          HashLongsExpression.this.children.exists(((child: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; HashLongsExpression.this.hasMapType(child.dataType)))
        </td>
      </tr><tr>
        <td>
          110
        </td>
        <td>
          7059
        </td>
        <td>
          4818
          -
          5175
        </td>
        <td>
          If
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          if (HashLongsExpression.this.children.exists(((child: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; HashLongsExpression.this.hasMapType(child.dataType))))
  org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply(scala.StringContext.apply(&quot;input to function &quot;, &quot; cannot contain elements of MapType. In Spark, same maps &quot;).s(HashLongsExpression.this.prettyName).+(&quot;may have different hashcode, thus hash expressions are prohibited on MapType elements.&quot;))
else
  org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckSuccess
        </td>
      </tr><tr>
        <td>
          110
        </td>
        <td>
          7052
        </td>
        <td>
          4847
          -
          4873
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.hasMapType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          HashLongsExpression.this.hasMapType(child.dataType)
        </td>
      </tr><tr>
        <td>
          111
        </td>
        <td>
          7056
        </td>
        <td>
          4884
          -
          5117
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply(scala.StringContext.apply(&quot;input to function &quot;, &quot; cannot contain elements of MapType. In Spark, same maps &quot;).s(HashLongsExpression.this.prettyName).+(&quot;may have different hashcode, thus hash expressions are prohibited on MapType elements.&quot;))
        </td>
      </tr><tr>
        <td>
          111
        </td>
        <td>
          7055
        </td>
        <td>
          4884
          -
          5117
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply(scala.StringContext.apply(&quot;input to function &quot;, &quot; cannot contain elements of MapType. In Spark, same maps &quot;).s(HashLongsExpression.this.prettyName).+(&quot;may have different hashcode, thus hash expressions are prohibited on MapType elements.&quot;))
        </td>
      </tr><tr>
        <td>
          112
        </td>
        <td>
          7054
        </td>
        <td>
          4926
          -
          5116
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;input to function &quot;, &quot; cannot contain elements of MapType. In Spark, same maps &quot;).s(HashLongsExpression.this.prettyName).+(&quot;may have different hashcode, thus hash expressions are prohibited on MapType elements.&quot;)
        </td>
      </tr><tr>
        <td>
          115
        </td>
        <td>
          7057
        </td>
        <td>
          5137
          -
          5169
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckSuccess
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckSuccess
        </td>
      </tr><tr>
        <td>
          115
        </td>
        <td>
          7058
        </td>
        <td>
          5137
          -
          5169
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckSuccess
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckSuccess
        </td>
      </tr><tr>
        <td>
          120
        </td>
        <td>
          7060
        </td>
        <td>
          5252
          -
          5265
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.DigestFactory.fresh
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          HashLongsExpression.this.factory.fresh
        </td>
      </tr><tr>
        <td>
          121
        </td>
        <td>
          7061
        </td>
        <td>
          5278
          -
          5279
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          122
        </td>
        <td>
          7062
        </td>
        <td>
          5294
          -
          5309
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.length
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          HashLongsExpression.this.children.length
        </td>
      </tr><tr>
        <td>
          123
        </td>
        <td>
          7069
        </td>
        <td>
          5330
          -
          5421
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  {
    HashLongsExpression.this.computeHash(HashLongsExpression.this.children.apply(i).eval(input), HashLongsExpression.this.children.apply(i).dataType, hash);
    i = i.+(1)
  };
  while$1()
}
        </td>
      </tr><tr>
        <td>
          123
        </td>
        <td>
          7063
        </td>
        <td>
          5321
          -
          5328
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.&lt;(len)
        </td>
      </tr><tr>
        <td>
          123
        </td>
        <td>
          7071
        </td>
        <td>
          5314
          -
          5314
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          123
        </td>
        <td>
          7068
        </td>
        <td>
          5330
          -
          5330
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.while$1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          while$1()
        </td>
      </tr><tr>
        <td>
          123
        </td>
        <td>
          7070
        </td>
        <td>
          5314
          -
          5314
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          124
        </td>
        <td>
          7066
        </td>
        <td>
          5338
          -
          5402
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.computeHash
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          HashLongsExpression.this.computeHash(HashLongsExpression.this.children.apply(i).eval(input), HashLongsExpression.this.children.apply(i).dataType, hash)
        </td>
      </tr><tr>
        <td>
          124
        </td>
        <td>
          7065
        </td>
        <td>
          5375
          -
          5395
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          HashLongsExpression.this.children.apply(i).dataType
        </td>
      </tr><tr>
        <td>
          124
        </td>
        <td>
          7064
        </td>
        <td>
          5350
          -
          5373
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.eval
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          HashLongsExpression.this.children.apply(i).eval(input)
        </td>
      </tr><tr>
        <td>
          125
        </td>
        <td>
          7067
        </td>
        <td>
          5409
          -
          5415
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          i.+(1)
        </td>
      </tr><tr>
        <td>
          127
        </td>
        <td>
          7072
        </td>
        <td>
          5430
          -
          5438
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.asStruct
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          HashLongsExpression.this.asStruct
        </td>
      </tr><tr>
        <td>
          128
        </td>
        <td>
          7075
        </td>
        <td>
          5446
          -
          5474
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.catalyst.InternalRow.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.InternalRow.apply((hash.digest: _*))
        </td>
      </tr><tr>
        <td>
          128
        </td>
        <td>
          7074
        </td>
        <td>
          5446
          -
          5474
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.InternalRow.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.catalyst.InternalRow.apply((hash.digest: _*))
        </td>
      </tr><tr>
        <td>
          128
        </td>
        <td>
          7073
        </td>
        <td>
          5458
          -
          5469
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.Digest.digest
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          hash.digest
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          7078
        </td>
        <td>
          5515
          -
          5548
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.catalyst.util.GenericArrayData.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          new org.apache.spark.sql.catalyst.util.GenericArrayData(hash.digest)
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          7077
        </td>
        <td>
          5515
          -
          5548
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.util.GenericArrayData.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          new org.apache.spark.sql.catalyst.util.GenericArrayData(hash.digest)
        </td>
      </tr><tr>
        <td>
          130
        </td>
        <td>
          7076
        </td>
        <td>
          5536
          -
          5547
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.Digest.digest
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          hash.digest
        </td>
      </tr><tr>
        <td>
          172
        </td>
        <td>
          7079
        </td>
        <td>
          7021
          -
          7045
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.freshName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ctx.freshName(&quot;element&quot;)
        </td>
      </tr><tr>
        <td>
          174
        </td>
        <td>
          7080
        </td>
        <td>
          7060
          -
          7095
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.javaType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.javaType(elementType)
        </td>
      </tr><tr>
        <td>
          175
        </td>
        <td>
          7090
        </td>
        <td>
          7100
          -
          7324
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.nullSafeExec
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ctx.nullSafeExec(nullable, scala.StringContext.apply(&quot;&quot;, &quot;.isNullAt(&quot;, &quot;)&quot;).s(input, index))(scala.StringContext.apply(&quot;\n        final &quot;, &quot; &quot;, &quot; = &quot;, &quot;;\n        &quot;, &quot;\n      &quot;).s(jt, element, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.getValue(input, elementType, index), HashLongsExpression.this.computeHash(element, elementType, result, ctx)))
        </td>
      </tr><tr>
        <td>
          175
        </td>
        <td>
          7081
        </td>
        <td>
          7127
          -
          7153
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot;.isNullAt(&quot;, &quot;)&quot;).s(input, index)
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          7089
        </td>
        <td>
          7163
          -
          7318
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;\n        final &quot;, &quot; &quot;, &quot; = &quot;, &quot;;\n        &quot;, &quot;\n      &quot;).s(jt, element, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.getValue(input, elementType, index), HashLongsExpression.this.computeHash(element, elementType, result, ctx))
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          7082
        </td>
        <td>
          7167
          -
          7183
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n        final &quot;
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          7084
        </td>
        <td>
          7194
          -
          7198
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = &quot;
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          7087
        </td>
        <td>
          7199
          -
          7248
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.getValue
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.getValue(input, elementType, index)
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          7083
        </td>
        <td>
          7185
          -
          7187
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; &quot;
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          7085
        </td>
        <td>
          7249
          -
          7260
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;;\n        &quot;
        </td>
      </tr><tr>
        <td>
          178
        </td>
        <td>
          7086
        </td>
        <td>
          7308
          -
          7318
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n      &quot;
        </td>
      </tr><tr>
        <td>
          178
        </td>
        <td>
          7088
        </td>
        <td>
          7261
          -
          7307
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.computeHash
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.computeHash(element, elementType, result, ctx)
        </td>
      </tr><tr>
        <td>
          184
        </td>
        <td>
          7093
        </td>
        <td>
          7426
          -
          7436
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.hashInt(&quot;
        </td>
      </tr><tr>
        <td>
          184
        </td>
        <td>
          7096
        </td>
        <td>
          7411
          -
          7426
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.hasherClassName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.hasherClassName
        </td>
      </tr><tr>
        <td>
          184
        </td>
        <td>
          7092
        </td>
        <td>
          7407
          -
          7411
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = &quot;
        </td>
      </tr><tr>
        <td>
          184
        </td>
        <td>
          7095
        </td>
        <td>
          7446
          -
          7449
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;);&quot;
        </td>
      </tr><tr>
        <td>
          184
        </td>
        <td>
          7094
        </td>
        <td>
          7437
          -
          7440
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;, &quot;
        </td>
      </tr><tr>
        <td>
          184
        </td>
        <td>
          7097
        </td>
        <td>
          7398
          -
          7449
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot; = &quot;, &quot;.hashInt(&quot;, &quot;, &quot;, &quot;);&quot;).s(result, HashLongsExpression.this.hasherClassName, i, result)
        </td>
      </tr><tr>
        <td>
          184
        </td>
        <td>
          7091
        </td>
        <td>
          7400
          -
          7401
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          187
        </td>
        <td>
          7099
        </td>
        <td>
          7529
          -
          7533
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = &quot;
        </td>
      </tr><tr>
        <td>
          187
        </td>
        <td>
          7102
        </td>
        <td>
          7569
          -
          7572
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;);&quot;
        </td>
      </tr><tr>
        <td>
          187
        </td>
        <td>
          7098
        </td>
        <td>
          7522
          -
          7523
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          187
        </td>
        <td>
          7101
        </td>
        <td>
          7560
          -
          7563
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;, &quot;
        </td>
      </tr><tr>
        <td>
          187
        </td>
        <td>
          7104
        </td>
        <td>
          7520
          -
          7572
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot; = &quot;, &quot;.hashLong(&quot;, &quot;, &quot;, &quot;);&quot;).s(result, HashLongsExpression.this.hasherClassName, l, result)
        </td>
      </tr><tr>
        <td>
          187
        </td>
        <td>
          7103
        </td>
        <td>
          7533
          -
          7548
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.hasherClassName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.hasherClassName
        </td>
      </tr><tr>
        <td>
          187
        </td>
        <td>
          7100
        </td>
        <td>
          7548
          -
          7559
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.hashLong(&quot;
        </td>
      </tr><tr>
        <td>
          190
        </td>
        <td>
          7105
        </td>
        <td>
          7659
          -
          7687
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;Platform.BYTE_ARRAY_OFFSET&quot;
        </td>
      </tr><tr>
        <td>
          191
        </td>
        <td>
          7108
        </td>
        <td>
          7720
          -
          7738
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.hashUnsafeBytes(&quot;
        </td>
      </tr><tr>
        <td>
          191
        </td>
        <td>
          7111
        </td>
        <td>
          7752
          -
          7762
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.length, &quot;
        </td>
      </tr><tr>
        <td>
          191
        </td>
        <td>
          7114
        </td>
        <td>
          7692
          -
          7771
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot; = &quot;, &quot;.hashUnsafeBytes(&quot;, &quot;, &quot;, &quot;, &quot;, &quot;.length, &quot;, &quot;);&quot;).s(result, HashLongsExpression.this.hasherClassName, b, offset, b, result)
        </td>
      </tr><tr>
        <td>
          191
        </td>
        <td>
          7113
        </td>
        <td>
          7705
          -
          7720
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.hasherClassName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.hasherClassName
        </td>
      </tr><tr>
        <td>
          191
        </td>
        <td>
          7107
        </td>
        <td>
          7701
          -
          7705
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = &quot;
        </td>
      </tr><tr>
        <td>
          191
        </td>
        <td>
          7110
        </td>
        <td>
          7748
          -
          7751
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;, &quot;
        </td>
      </tr><tr>
        <td>
          191
        </td>
        <td>
          7112
        </td>
        <td>
          7768
          -
          7771
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;);&quot;
        </td>
      </tr><tr>
        <td>
          191
        </td>
        <td>
          7106
        </td>
        <td>
          7694
          -
          7695
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          191
        </td>
        <td>
          7109
        </td>
        <td>
          7739
          -
          7742
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;, &quot;
        </td>
      </tr><tr>
        <td>
          195
        </td>
        <td>
          7116
        </td>
        <td>
          7853
          -
          7890
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashInt(scala.StringContext.apply(&quot;&quot;, &quot; ? 1 : 0&quot;).s(input), result)
        </td>
      </tr><tr>
        <td>
          195
        </td>
        <td>
          7115
        </td>
        <td>
          7864
          -
          7881
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot; ? 1 : 0&quot;).s(input)
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          7117
        </td>
        <td>
          7972
          -
          7985
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n       |if(&quot;
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          7124
        </td>
        <td>
          7968
          -
          8140
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;\n       |if(&quot;, &quot; == -0.0f) {\n       |  &quot;, &quot;\n       |} else {\n       |  &quot;, &quot;\n       |}\n     &quot;).s(input, HashLongsExpression.this.genHashInt(&quot;0&quot;, result), HashLongsExpression.this.genHashInt(scala.StringContext.apply(&quot;Float.floatToIntBits(&quot;, &quot;)&quot;).s(input), result))
        </td>
      </tr><tr>
        <td>
          199
        </td>
        <td>
          7118
        </td>
        <td>
          7990
          -
          8014
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; == -0.0f) {\n       |  &quot;
        </td>
      </tr><tr>
        <td>
          200
        </td>
        <td>
          7119
        </td>
        <td>
          8039
          -
          8068
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n       |} else {\n       |  &quot;
        </td>
      </tr><tr>
        <td>
          200
        </td>
        <td>
          7121
        </td>
        <td>
          8015
          -
          8038
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashInt(&quot;0&quot;, result)
        </td>
      </tr><tr>
        <td>
          202
        </td>
        <td>
          7120
        </td>
        <td>
          8121
          -
          8140
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n       |}\n     &quot;
        </td>
      </tr><tr>
        <td>
          202
        </td>
        <td>
          7123
        </td>
        <td>
          8069
          -
          8120
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashInt(scala.StringContext.apply(&quot;Float.floatToIntBits(&quot;, &quot;)&quot;).s(input), result)
        </td>
      </tr><tr>
        <td>
          202
        </td>
        <td>
          7122
        </td>
        <td>
          8080
          -
          8111
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;Float.floatToIntBits(&quot;, &quot;)&quot;).s(input)
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          7125
        </td>
        <td>
          7968
          -
          8152
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.StringLike.stripMargin
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.augmentString(scala.StringContext.apply(&quot;\n       |if(&quot;, &quot; == -0.0f) {\n       |  &quot;, &quot;\n       |} else {\n       |  &quot;, &quot;\n       |}\n     &quot;).s(input, HashLongsExpression.this.genHashInt(&quot;0&quot;, result), HashLongsExpression.this.genHashInt(scala.StringContext.apply(&quot;Float.floatToIntBits(&quot;, &quot;)&quot;).s(input), result))).stripMargin
        </td>
      </tr><tr>
        <td>
          208
        </td>
        <td>
          7126
        </td>
        <td>
          8239
          -
          8252
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n       |if(&quot;
        </td>
      </tr><tr>
        <td>
          208
        </td>
        <td>
          7133
        </td>
        <td>
          8235
          -
          8413
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;\n       |if(&quot;, &quot; == -0.0d) {\n       |  &quot;, &quot;\n       |} else {\n       |  &quot;, &quot;\n       |}\n     &quot;).s(input, HashLongsExpression.this.genHashLong(&quot;0L&quot;, result), HashLongsExpression.this.genHashLong(scala.StringContext.apply(&quot;Double.doubleToLongBits(&quot;, &quot;)&quot;).s(input), result))
        </td>
      </tr><tr>
        <td>
          209
        </td>
        <td>
          7127
        </td>
        <td>
          8257
          -
          8281
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; == -0.0d) {\n       |  &quot;
        </td>
      </tr><tr>
        <td>
          210
        </td>
        <td>
          7128
        </td>
        <td>
          8308
          -
          8337
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n       |} else {\n       |  &quot;
        </td>
      </tr><tr>
        <td>
          210
        </td>
        <td>
          7130
        </td>
        <td>
          8282
          -
          8307
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashLong(&quot;0L&quot;, result)
        </td>
      </tr><tr>
        <td>
          212
        </td>
        <td>
          7129
        </td>
        <td>
          8394
          -
          8413
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n       |}\n     &quot;
        </td>
      </tr><tr>
        <td>
          212
        </td>
        <td>
          7132
        </td>
        <td>
          8338
          -
          8393
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashLong(scala.StringContext.apply(&quot;Double.doubleToLongBits(&quot;, &quot;)&quot;).s(input), result)
        </td>
      </tr><tr>
        <td>
          212
        </td>
        <td>
          7131
        </td>
        <td>
          8350
          -
          8384
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;Double.doubleToLongBits(&quot;, &quot;)&quot;).s(input)
        </td>
      </tr><tr>
        <td>
          214
        </td>
        <td>
          7134
        </td>
        <td>
          8235
          -
          8425
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.StringLike.stripMargin
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.augmentString(scala.StringContext.apply(&quot;\n       |if(&quot;, &quot; == -0.0d) {\n       |  &quot;, &quot;\n       |} else {\n       |  &quot;, &quot;\n       |}\n     &quot;).s(input, HashLongsExpression.this.genHashLong(&quot;0L&quot;, result), HashLongsExpression.this.genHashLong(scala.StringContext.apply(&quot;Double.doubleToLongBits(&quot;, &quot;)&quot;).s(input), result))).stripMargin
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          7135
        </td>
        <td>
          8694
          -
          8717
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.Decimal.MAX_LONG_DIGITS
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.Decimal.MAX_LONG_DIGITS
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          7136
        </td>
        <td>
          8679
          -
          8717
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;=
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          d.precision.&lt;=(org.apache.spark.sql.types.Decimal.MAX_LONG_DIGITS)
        </td>
      </tr><tr>
        <td>
          223
        </td>
        <td>
          7138
        </td>
        <td>
          8727
          -
          8774
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashLong(scala.StringContext.apply(&quot;&quot;, &quot;.toUnscaledLong()&quot;).s(input), result)
        </td>
      </tr><tr>
        <td>
          223
        </td>
        <td>
          7137
        </td>
        <td>
          8739
          -
          8765
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot;.toUnscaledLong()&quot;).s(input)
        </td>
      </tr><tr>
        <td>
          223
        </td>
        <td>
          7139
        </td>
        <td>
          8727
          -
          8774
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashLong(scala.StringContext.apply(&quot;&quot;, &quot;.toUnscaledLong()&quot;).s(input), result)
        </td>
      </tr><tr>
        <td>
          224
        </td>
        <td>
          7148
        </td>
        <td>
          8786
          -
          8998
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          {
  val bytes: String = ctx.freshName(&quot;bytes&quot;);
  scala.Predef.augmentString(scala.StringContext.apply(&quot;\n         |final byte[] &quot;, &quot; = &quot;, &quot;.toJavaBigDecimal().unscaledValue().toByteArray();\n         |&quot;, &quot;\n       &quot;).s(bytes, input, HashLongsExpression.this.genHashBytes(bytes, result))).stripMargin
}
        </td>
      </tr><tr>
        <td>
          225
        </td>
        <td>
          7140
        </td>
        <td>
          8806
          -
          8828
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.freshName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ctx.freshName(&quot;bytes&quot;)
        </td>
      </tr><tr>
        <td>
          226
        </td>
        <td>
          7141
        </td>
        <td>
          8839
          -
          8864
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n         |final byte[] &quot;
        </td>
      </tr><tr>
        <td>
          226
        </td>
        <td>
          7146
        </td>
        <td>
          8835
          -
          8980
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;\n         |final byte[] &quot;, &quot; = &quot;, &quot;.toJavaBigDecimal().unscaledValue().toByteArray();\n         |&quot;, &quot;\n       &quot;).s(bytes, input, HashLongsExpression.this.genHashBytes(bytes, result))
        </td>
      </tr><tr>
        <td>
          227
        </td>
        <td>
          7143
        </td>
        <td>
          8878
          -
          8940
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.toJavaBigDecimal().unscaledValue().toByteArray();\n         |&quot;
        </td>
      </tr><tr>
        <td>
          227
        </td>
        <td>
          7142
        </td>
        <td>
          8869
          -
          8873
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = &quot;
        </td>
      </tr><tr>
        <td>
          228
        </td>
        <td>
          7144
        </td>
        <td>
          8969
          -
          8980
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n       &quot;
        </td>
      </tr><tr>
        <td>
          228
        </td>
        <td>
          7145
        </td>
        <td>
          8941
          -
          8968
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashBytes
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashBytes(bytes, result)
        </td>
      </tr><tr>
        <td>
          229
        </td>
        <td>
          7147
        </td>
        <td>
          8835
          -
          8992
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.StringLike.stripMargin
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.augmentString(scala.StringContext.apply(&quot;\n         |final byte[] &quot;, &quot; = &quot;, &quot;.toJavaBigDecimal().unscaledValue().toByteArray();\n         |&quot;, &quot;\n       &quot;).s(bytes, input, HashLongsExpression.this.genHashBytes(bytes, result))).stripMargin
        </td>
      </tr><tr>
        <td>
          233
        </td>
        <td>
          7149
        </td>
        <td>
          9074
          -
          9096
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashLong(t, result)
        </td>
      </tr><tr>
        <td>
          236
        </td>
        <td>
          7153
        </td>
        <td>
          9264
          -
          9266
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;)&quot;
        </td>
      </tr><tr>
        <td>
          236
        </td>
        <td>
          7152
        </td>
        <td>
          9242
          -
          9258
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.microseconds, &quot;
        </td>
      </tr><tr>
        <td>
          236
        </td>
        <td>
          7155
        </td>
        <td>
          9208
          -
          9266
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot;.hashLong(&quot;, &quot;.microseconds, &quot;, &quot;)&quot;).s(HashLongsExpression.this.hasherClassName, input, result)
        </td>
      </tr><tr>
        <td>
          236
        </td>
        <td>
          7154
        </td>
        <td>
          9211
          -
          9226
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.hasherClassName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.hasherClassName
        </td>
      </tr><tr>
        <td>
          236
        </td>
        <td>
          7151
        </td>
        <td>
          9226
          -
          9237
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.hashLong(&quot;
        </td>
      </tr><tr>
        <td>
          236
        </td>
        <td>
          7150
        </td>
        <td>
          9210
          -
          9211
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          237
        </td>
        <td>
          7162
        </td>
        <td>
          9271
          -
          9343
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot; = &quot;, &quot;.hashInt(&quot;, &quot;.months, &quot;, &quot;);&quot;).s(result, HashLongsExpression.this.hasherClassName, input, microsecondsHash)
        </td>
      </tr><tr>
        <td>
          237
        </td>
        <td>
          7156
        </td>
        <td>
          9273
          -
          9274
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          237
        </td>
        <td>
          7158
        </td>
        <td>
          9299
          -
          9309
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.hashInt(&quot;
        </td>
      </tr><tr>
        <td>
          237
        </td>
        <td>
          7161
        </td>
        <td>
          9284
          -
          9299
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.hasherClassName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.hasherClassName
        </td>
      </tr><tr>
        <td>
          237
        </td>
        <td>
          7157
        </td>
        <td>
          9280
          -
          9284
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = &quot;
        </td>
      </tr><tr>
        <td>
          237
        </td>
        <td>
          7160
        </td>
        <td>
          9340
          -
          9343
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;);&quot;
        </td>
      </tr><tr>
        <td>
          237
        </td>
        <td>
          7159
        </td>
        <td>
          9314
          -
          9324
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.months, &quot;
        </td>
      </tr><tr>
        <td>
          241
        </td>
        <td>
          7163
        </td>
        <td>
          9443
          -
          9468
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot;.getBaseObject()&quot;).s(input)
        </td>
      </tr><tr>
        <td>
          242
        </td>
        <td>
          7164
        </td>
        <td>
          9490
          -
          9515
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot;.getBaseOffset()&quot;).s(input)
        </td>
      </tr><tr>
        <td>
          243
        </td>
        <td>
          7165
        </td>
        <td>
          9535
          -
          9555
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot;.numBytes()&quot;).s(input)
        </td>
      </tr><tr>
        <td>
          244
        </td>
        <td>
          7171
        </td>
        <td>
          9640
          -
          9643
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;, &quot;
        </td>
      </tr><tr>
        <td>
          244
        </td>
        <td>
          7174
        </td>
        <td>
          9560
          -
          9652
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot; = &quot;, &quot;.hashUnsafeBytes(&quot;, &quot;, &quot;, &quot;, &quot;, &quot;, &quot;, &quot;);&quot;).s(result, HashLongsExpression.this.hasherClassName, baseObject, baseOffset, numBytes, result)
        </td>
      </tr><tr>
        <td>
          244
        </td>
        <td>
          7173
        </td>
        <td>
          9573
          -
          9588
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.hasherClassName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.hasherClassName
        </td>
      </tr><tr>
        <td>
          244
        </td>
        <td>
          7167
        </td>
        <td>
          9569
          -
          9573
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = &quot;
        </td>
      </tr><tr>
        <td>
          244
        </td>
        <td>
          7170
        </td>
        <td>
          9629
          -
          9632
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;, &quot;
        </td>
      </tr><tr>
        <td>
          244
        </td>
        <td>
          7172
        </td>
        <td>
          9649
          -
          9652
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;);&quot;
        </td>
      </tr><tr>
        <td>
          244
        </td>
        <td>
          7166
        </td>
        <td>
          9562
          -
          9563
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          244
        </td>
        <td>
          7169
        </td>
        <td>
          9616
          -
          9619
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;, &quot;
        </td>
      </tr><tr>
        <td>
          244
        </td>
        <td>
          7168
        </td>
        <td>
          9588
          -
          9606
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.hashUnsafeBytes(&quot;
        </td>
      </tr><tr>
        <td>
          254
        </td>
        <td>
          7175
        </td>
        <td>
          10023
          -
          10045
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.freshName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ctx.freshName(&quot;index&quot;)
        </td>
      </tr><tr>
        <td>
          255
        </td>
        <td>
          7176
        </td>
        <td>
          10061
          -
          10082
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.freshName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ctx.freshName(&quot;keys&quot;)
        </td>
      </tr><tr>
        <td>
          256
        </td>
        <td>
          7177
        </td>
        <td>
          10100
          -
          10123
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.freshName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ctx.freshName(&quot;values&quot;)
        </td>
      </tr><tr>
        <td>
          257
        </td>
        <td>
          7191
        </td>
        <td>
          10128
          -
          10496
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;\n        final ArrayData &quot;, &quot; = &quot;, &quot;.keyArray();\n        final ArrayData &quot;, &quot; = &quot;, &quot;.valueArray();\n        for (int &quot;, &quot; = 0; &quot;, &quot; &lt; &quot;, &quot;.numElements(); &quot;, &quot;++) {\n          &quot;, &quot;\n          &quot;, &quot;\n        }\n      &quot;).s(keys, input, values, input, index, index, input, index, HashLongsExpression.this.nullSafeElementHash(keys, index, false, keyType, result, ctx), HashLongsExpression.this.nullSafeElementHash(values, index, valueContainsNull, valueType, result, ctx))
        </td>
      </tr><tr>
        <td>
          257
        </td>
        <td>
          7178
        </td>
        <td>
          10132
          -
          10158
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n        final ArrayData &quot;
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          7180
        </td>
        <td>
          10171
          -
          10209
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.keyArray();\n        final ArrayData &quot;
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          7179
        </td>
        <td>
          10162
          -
          10166
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = &quot;
        </td>
      </tr><tr>
        <td>
          259
        </td>
        <td>
          7182
        </td>
        <td>
          10224
          -
          10257
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.valueArray();\n        for (int &quot;
        </td>
      </tr><tr>
        <td>
          259
        </td>
        <td>
          7181
        </td>
        <td>
          10215
          -
          10219
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = &quot;
        </td>
      </tr><tr>
        <td>
          260
        </td>
        <td>
          7183
        </td>
        <td>
          10262
          -
          10269
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = 0; &quot;
        </td>
      </tr><tr>
        <td>
          260
        </td>
        <td>
          7185
        </td>
        <td>
          10283
          -
          10300
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.numElements(); &quot;
        </td>
      </tr><tr>
        <td>
          260
        </td>
        <td>
          7184
        </td>
        <td>
          10274
          -
          10278
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; &lt; &quot;
        </td>
      </tr><tr>
        <td>
          260
        </td>
        <td>
          7186
        </td>
        <td>
          10305
          -
          10322
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;++) {\n          &quot;
        </td>
      </tr><tr>
        <td>
          261
        </td>
        <td>
          7189
        </td>
        <td>
          10323
          -
          10384
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.nullSafeElementHash
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.nullSafeElementHash(keys, index, false, keyType, result, ctx)
        </td>
      </tr><tr>
        <td>
          261
        </td>
        <td>
          7187
        </td>
        <td>
          10385
          -
          10397
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n          &quot;
        </td>
      </tr><tr>
        <td>
          262
        </td>
        <td>
          7188
        </td>
        <td>
          10476
          -
          10496
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n        }\n      &quot;
        </td>
      </tr><tr>
        <td>
          262
        </td>
        <td>
          7190
        </td>
        <td>
          10398
          -
          10475
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.nullSafeElementHash
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.nullSafeElementHash(values, index, valueContainsNull, valueType, result, ctx)
        </td>
      </tr><tr>
        <td>
          273
        </td>
        <td>
          7192
        </td>
        <td>
          10826
          -
          10848
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.freshName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ctx.freshName(&quot;index&quot;)
        </td>
      </tr><tr>
        <td>
          274
        </td>
        <td>
          7200
        </td>
        <td>
          10853
          -
          11036
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;\n        for (int &quot;, &quot; = 0; &quot;, &quot; &lt; &quot;, &quot;.numElements(); &quot;, &quot;++) {\n          &quot;, &quot;\n        }\n      &quot;).s(index, index, input, index, HashLongsExpression.this.nullSafeElementHash(input, index, containsNull, elementType, result, ctx))
        </td>
      </tr><tr>
        <td>
          274
        </td>
        <td>
          7193
        </td>
        <td>
          10857
          -
          10876
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n        for (int &quot;
        </td>
      </tr><tr>
        <td>
          275
        </td>
        <td>
          7194
        </td>
        <td>
          10881
          -
          10888
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = 0; &quot;
        </td>
      </tr><tr>
        <td>
          275
        </td>
        <td>
          7197
        </td>
        <td>
          10924
          -
          10941
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;++) {\n          &quot;
        </td>
      </tr><tr>
        <td>
          275
        </td>
        <td>
          7196
        </td>
        <td>
          10902
          -
          10919
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.numElements(); &quot;
        </td>
      </tr><tr>
        <td>
          275
        </td>
        <td>
          7195
        </td>
        <td>
          10893
          -
          10897
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; &lt; &quot;
        </td>
      </tr><tr>
        <td>
          276
        </td>
        <td>
          7198
        </td>
        <td>
          11016
          -
          11036
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n        }\n      &quot;
        </td>
      </tr><tr>
        <td>
          276
        </td>
        <td>
          7199
        </td>
        <td>
          10942
          -
          11015
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.nullSafeElementHash
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.nullSafeElementHash(input, index, containsNull, elementType, result, ctx)
        </td>
      </tr><tr>
        <td>
          286
        </td>
        <td>
          7201
        </td>
        <td>
          11323
          -
          11345
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.freshName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ctx.freshName(&quot;input&quot;)
        </td>
      </tr><tr>
        <td>
          287
        </td>
        <td>
          7209
        </td>
        <td>
          11367
          -
          11518
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.refArrayOps[(org.apache.spark.sql.types.StructField, Int)](scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](fields).zipWithIndex[org.apache.spark.sql.types.StructField, Array[(org.apache.spark.sql.types.StructField, Int)]](scala.this.Array.canBuildFrom[(org.apache.spark.sql.types.StructField, Int)]((ClassTag.apply[(org.apache.spark.sql.types.StructField, Int)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(org.apache.spark.sql.types.StructField, Int)])))).map[String, Array[String]](((x0$1: (org.apache.spark.sql.types.StructField, Int)) =&gt; x0$1 match {
  case (_1: org.apache.spark.sql.types.StructField, _2: Int)(org.apache.spark.sql.types.StructField, Int)((field @ _), (index @ _)) =&gt; HashLongsExpression.this.nullSafeElementHash(tmpInput, index.toString(), field.nullable, field.dataType, result, ctx)
}))(scala.this.Array.canBuildFrom[String]((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String])))
        </td>
      </tr><tr>
        <td>
          287
        </td>
        <td>
          7203
        </td>
        <td>
          11367
          -
          11386
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.IndexedSeqOptimized.zipWithIndex
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](fields).zipWithIndex[org.apache.spark.sql.types.StructField, Array[(org.apache.spark.sql.types.StructField, Int)]](scala.this.Array.canBuildFrom[(org.apache.spark.sql.types.StructField, Int)]((ClassTag.apply[(org.apache.spark.sql.types.StructField, Int)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(org.apache.spark.sql.types.StructField, Int)])))
        </td>
      </tr><tr>
        <td>
          287
        </td>
        <td>
          7208
        </td>
        <td>
          11391
          -
          11391
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.this.Array.canBuildFrom[String]((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))
        </td>
      </tr><tr>
        <td>
          287
        </td>
        <td>
          7202
        </td>
        <td>
          11374
          -
          11374
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.this.Array.canBuildFrom[(org.apache.spark.sql.types.StructField, Int)]((ClassTag.apply[(org.apache.spark.sql.types.StructField, Int)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(org.apache.spark.sql.types.StructField, Int)]))
        </td>
      </tr><tr>
        <td>
          288
        </td>
        <td>
          7207
        </td>
        <td>
          11422
          -
          11512
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.nullSafeElementHash
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.nullSafeElementHash(tmpInput, index.toString(), field.nullable, field.dataType, result, ctx)
        </td>
      </tr><tr>
        <td>
          288
        </td>
        <td>
          7206
        </td>
        <td>
          11484
          -
          11498
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          field.dataType
        </td>
      </tr><tr>
        <td>
          288
        </td>
        <td>
          7205
        </td>
        <td>
          11468
          -
          11482
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.nullable
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          field.nullable
        </td>
      </tr><tr>
        <td>
          288
        </td>
        <td>
          7204
        </td>
        <td>
          11452
          -
          11466
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.toString
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          index.toString()
        </td>
      </tr><tr>
        <td>
          290
        </td>
        <td>
          7211
        </td>
        <td>
          11544
          -
          11576
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.javaType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.javaType(HashLongsExpression.this.dataType)
        </td>
      </tr><tr>
        <td>
          290
        </td>
        <td>
          7210
        </td>
        <td>
          11567
          -
          11575
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.dataType
        </td>
      </tr><tr>
        <td>
          291
        </td>
        <td>
          7220
        </td>
        <td>
          11592
          -
          11997
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.splitExpressions
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ctx.splitExpressions(scala.Predef.wrapRefArray[String](fieldsHash), &quot;computeHashForStruct&quot;, scala.collection.Seq.apply[(String, String)](scala.Predef.ArrowAssoc[String](&quot;InternalRow&quot;).-&gt;[String](tmpInput), scala.Predef.ArrowAssoc[String](hashResultType).-&gt;[String](result)), hashResultType, ((body: String) =&gt; scala.Predef.augmentString(scala.StringContext.apply(&quot;\n           |&quot;, &quot;\n           |return &quot;, &quot;;\n         &quot;).s(body, result)).stripMargin), ((x$3: Seq[String]) =&gt; x$3.map[String, Seq[String]](((funcCall: String) =&gt; scala.StringContext.apply(&quot;&quot;, &quot; = &quot;, &quot;;&quot;).s(result, funcCall)))(collection.this.Seq.canBuildFrom[String]).mkString(&quot;\n&quot;)))
        </td>
      </tr><tr>
        <td>
          292
        </td>
        <td>
          7212
        </td>
        <td>
          11634
          -
          11644
        </td>
        <td>
          ApplyImplicitView
        </td>
        <td>
          scala.LowPriorityImplicits.wrapRefArray
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.wrapRefArray[String](fieldsHash)
        </td>
      </tr><tr>
        <td>
          293
        </td>
        <td>
          7213
        </td>
        <td>
          11663
          -
          11685
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;computeHashForStruct&quot;
        </td>
      </tr><tr>
        <td>
          294
        </td>
        <td>
          7216
        </td>
        <td>
          11705
          -
          11761
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.collection.Seq.apply[(String, String)](scala.Predef.ArrowAssoc[String](&quot;InternalRow&quot;).-&gt;[String](tmpInput), scala.Predef.ArrowAssoc[String](hashResultType).-&gt;[String](result))
        </td>
      </tr><tr>
        <td>
          294
        </td>
        <td>
          7215
        </td>
        <td>
          11736
          -
          11760
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Predef.ArrowAssoc.-&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.ArrowAssoc[String](hashResultType).-&gt;[String](result)
        </td>
      </tr><tr>
        <td>
          294
        </td>
        <td>
          7214
        </td>
        <td>
          11709
          -
          11734
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Predef.ArrowAssoc.-&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.ArrowAssoc[String](&quot;InternalRow&quot;).-&gt;[String](tmpInput)
        </td>
      </tr><tr>
        <td>
          297
        </td>
        <td>
          7217
        </td>
        <td>
          11840
          -
          11903
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;\n           |&quot;, &quot;\n           |return &quot;, &quot;;\n         &quot;).s(body, result)
        </td>
      </tr><tr>
        <td>
          300
        </td>
        <td>
          7218
        </td>
        <td>
          11840
          -
          11915
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.StringLike.stripMargin
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.augmentString(scala.StringContext.apply(&quot;\n           |&quot;, &quot;\n           |return &quot;, &quot;;\n         &quot;).s(body, result)).stripMargin
        </td>
      </tr><tr>
        <td>
          301
        </td>
        <td>
          7219
        </td>
        <td>
          11939
          -
          11996
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.mkString
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$3.map[String, Seq[String]](((funcCall: String) =&gt; scala.StringContext.apply(&quot;&quot;, &quot; = &quot;, &quot;;&quot;).s(result, funcCall)))(collection.this.Seq.canBuildFrom[String]).mkString(&quot;\n&quot;)
        </td>
      </tr><tr>
        <td>
          302
        </td>
        <td>
          7221
        </td>
        <td>
          12002
          -
          12075
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;\n       |final InternalRow &quot;, &quot; = &quot;, &quot;;\n       |&quot;, &quot;\n     &quot;).s(tmpInput, input, code)
        </td>
      </tr><tr>
        <td>
          305
        </td>
        <td>
          7222
        </td>
        <td>
          12002
          -
          12087
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.StringLike.stripMargin
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.augmentString(scala.StringContext.apply(&quot;\n       |final InternalRow &quot;, &quot; = &quot;, &quot;;\n       |&quot;, &quot;\n     &quot;).s(tmpInput, input, code)).stripMargin
        </td>
      </tr><tr>
        <td>
          314
        </td>
        <td>
          7223
        </td>
        <td>
          12414
          -
          12416
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          315
        </td>
        <td>
          7224
        </td>
        <td>
          12441
          -
          12470
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashBoolean
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashBoolean(input, result)
        </td>
      </tr><tr>
        <td>
          316
        </td>
        <td>
          7225
        </td>
        <td>
          12529
          -
          12554
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashInt(input, result)
        </td>
      </tr><tr>
        <td>
          317
        </td>
        <td>
          7226
        </td>
        <td>
          12576
          -
          12602
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashLong(input, result)
        </td>
      </tr><tr>
        <td>
          318
        </td>
        <td>
          7227
        </td>
        <td>
          12629
          -
          12660
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashTimestamp
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashTimestamp(input, result)
        </td>
      </tr><tr>
        <td>
          319
        </td>
        <td>
          7228
        </td>
        <td>
          12683
          -
          12710
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashFloat
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashFloat(input, result)
        </td>
      </tr><tr>
        <td>
          320
        </td>
        <td>
          7229
        </td>
        <td>
          12734
          -
          12762
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashDouble
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashDouble(input, result)
        </td>
      </tr><tr>
        <td>
          321
        </td>
        <td>
          7230
        </td>
        <td>
          12790
          -
          12827
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashDecimal
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashDecimal(ctx, d, input, result)
        </td>
      </tr><tr>
        <td>
          322
        </td>
        <td>
          7231
        </td>
        <td>
          12861
          -
          12899
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashCalendarInterval
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashCalendarInterval(input, result)
        </td>
      </tr><tr>
        <td>
          326
        </td>
        <td>
          7232
        </td>
        <td>
          13080
          -
          13107
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashBytes
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashBytes(input, result)
        </td>
      </tr><tr>
        <td>
          327
        </td>
        <td>
          7233
        </td>
        <td>
          13131
          -
          13159
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashString
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashString(input, result)
        </td>
      </tr><tr>
        <td>
          328
        </td>
        <td>
          7234
        </td>
        <td>
          13200
          -
          13253
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashForArray
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashForArray(ctx, input, result, et, containsNull)
        </td>
      </tr><tr>
        <td>
          330
        </td>
        <td>
          7235
        </td>
        <td>
          13307
          -
          13367
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashForMap
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashForMap(ctx, input, result, kt, vt, valueContainsNull)
        </td>
      </tr><tr>
        <td>
          331
        </td>
        <td>
          7236
        </td>
        <td>
          13399
          -
          13443
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.genHashForStruct
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashForStruct(ctx, input, result, fields)
        </td>
      </tr><tr>
        <td>
          332
        </td>
        <td>
          7238
        </td>
        <td>
          13480
          -
          13535
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.computeHashWithTailRec
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.computeHashWithTailRec(input, udt.sqlType, result, ctx)
        </td>
      </tr><tr>
        <td>
          332
        </td>
        <td>
          7237
        </td>
        <td>
          13510
          -
          13521
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.UserDefinedType.sqlType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          udt.sqlType
        </td>
      </tr><tr>
        <td>
          339
        </td>
        <td>
          7239
        </td>
        <td>
          13768
          -
          13820
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.HashLongsExpression.computeHashWithTailRec
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          HashLongsExpression.this.computeHashWithTailRec(input, dataType, result, ctx)
        </td>
      </tr><tr>
        <td>
          351
        </td>
        <td>
          7242
        </td>
        <td>
          14099
          -
          14196
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          s.getBaseObject().isInstanceOf[Array[Byte]].&amp;&amp;(s.getBaseOffset().&gt;=(org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET.toLong))
        </td>
      </tr><tr>
        <td>
          351
        </td>
        <td>
          7241
        </td>
        <td>
          14144
          -
          14196
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Long.&gt;=
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          s.getBaseOffset().&gt;=(org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET.toLong)
        </td>
      </tr><tr>
        <td>
          351
        </td>
        <td>
          7255
        </td>
        <td>
          14198
          -
          14531
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          {
  val bytes: Array[Byte] = s.getBaseObject().asInstanceOf[Array[Byte]].asInstanceOf[Array[Byte]];
  val arrayOffset: Long = s.getBaseOffset().-(org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET.toLong);
  if (bytes.length.toLong.&lt;(arrayOffset.+(s.numBytes().toLong)))
    throw new scala.`package`.ArrayIndexOutOfBoundsException()
  else
    scala.Tuple3.apply[Array[Byte], Int, Int](bytes, arrayOffset.toInt, s.numBytes())
}
        </td>
      </tr><tr>
        <td>
          351
        </td>
        <td>
          7240
        </td>
        <td>
          14163
          -
          14196
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Int.toLong
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET.toLong
        </td>
      </tr><tr>
        <td>
          352
        </td>
        <td>
          7243
        </td>
        <td>
          14218
          -
          14285
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.asInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          s.getBaseObject().asInstanceOf[Array[Byte]].asInstanceOf[Array[Byte]]
        </td>
      </tr><tr>
        <td>
          353
        </td>
        <td>
          7245
        </td>
        <td>
          14310
          -
          14361
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Long.-
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          s.getBaseOffset().-(org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET.toLong)
        </td>
      </tr><tr>
        <td>
          353
        </td>
        <td>
          7244
        </td>
        <td>
          14328
          -
          14361
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Int.toLong
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET.toLong
        </td>
      </tr><tr>
        <td>
          354
        </td>
        <td>
          7248
        </td>
        <td>
          14372
          -
          14425
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Long.&lt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          bytes.length.toLong.&lt;(arrayOffset.+(s.numBytes().toLong))
        </td>
      </tr><tr>
        <td>
          354
        </td>
        <td>
          7247
        </td>
        <td>
          14394
          -
          14425
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Long.+
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          arrayOffset.+(s.numBytes().toLong)
        </td>
      </tr><tr>
        <td>
          354
        </td>
        <td>
          7250
        </td>
        <td>
          14427
          -
          14467
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          throw new scala.`package`.ArrayIndexOutOfBoundsException()
        </td>
      </tr><tr>
        <td>
          354
        </td>
        <td>
          7246
        </td>
        <td>
          14408
          -
          14425
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Int.toLong
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          s.numBytes().toLong
        </td>
      </tr><tr>
        <td>
          354
        </td>
        <td>
          7249
        </td>
        <td>
          14427
          -
          14467
        </td>
        <td>
          Throw
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          throw new scala.`package`.ArrayIndexOutOfBoundsException()
        </td>
      </tr><tr>
        <td>
          356
        </td>
        <td>
          7252
        </td>
        <td>
          14514
          -
          14524
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.unsafe.types.UTF8String.numBytes
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          s.numBytes()
        </td>
      </tr><tr>
        <td>
          356
        </td>
        <td>
          7251
        </td>
        <td>
          14495
          -
          14512
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Long.toInt
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          arrayOffset.toInt
        </td>
      </tr><tr>
        <td>
          356
        </td>
        <td>
          7254
        </td>
        <td>
          14487
          -
          14525
        </td>
        <td>
          Block
        </td>
        <td>
          scala.Tuple3.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple3.apply[Array[Byte], Int, Int](bytes, arrayOffset.toInt, s.numBytes())
        </td>
      </tr><tr>
        <td>
          356
        </td>
        <td>
          7253
        </td>
        <td>
          14487
          -
          14525
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple3.apply
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple3.apply[Array[Byte], Int, Int](bytes, arrayOffset.toInt, s.numBytes())
        </td>
      </tr><tr>
        <td>
          359
        </td>
        <td>
          7260
        </td>
        <td>
          14549
          -
          14576
        </td>
        <td>
          Block
        </td>
        <td>
          scala.Tuple3.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Tuple3.apply[Array[Byte], Int, Int](s.getBytes(), 0, s.numBytes())
        </td>
      </tr><tr>
        <td>
          359
        </td>
        <td>
          7257
        </td>
        <td>
          14562
          -
          14563
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          0
        </td>
      </tr><tr>
        <td>
          359
        </td>
        <td>
          7256
        </td>
        <td>
          14550
          -
          14560
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.unsafe.types.UTF8String.getBytes
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          s.getBytes()
        </td>
      </tr><tr>
        <td>
          359
        </td>
        <td>
          7259
        </td>
        <td>
          14549
          -
          14576
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple3.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Tuple3.apply[Array[Byte], Int, Int](s.getBytes(), 0, s.numBytes())
        </td>
      </tr><tr>
        <td>
          359
        </td>
        <td>
          7258
        </td>
        <td>
          14565
          -
          14575
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.unsafe.types.UTF8String.numBytes
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          s.numBytes()
        </td>
      </tr><tr>
        <td>
          381
        </td>
        <td>
          7261
        </td>
        <td>
          15159
          -
          15160
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          1
        </td>
      </tr><tr>
        <td>
          381
        </td>
        <td>
          7263
        </td>
        <td>
          15166
          -
          15167
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          0
        </td>
      </tr><tr>
        <td>
          381
        </td>
        <td>
          7265
        </td>
        <td>
          15144
          -
          15176
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashInt(if (b)
  1
else
  0, digest)
        </td>
      </tr><tr>
        <td>
          381
        </td>
        <td>
          7262
        </td>
        <td>
          15159
          -
          15160
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          1
        </td>
      </tr><tr>
        <td>
          381
        </td>
        <td>
          7264
        </td>
        <td>
          15166
          -
          15167
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          0
        </td>
      </tr><tr>
        <td>
          382
        </td>
        <td>
          7266
        </td>
        <td>
          15207
          -
          15208
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Byte.toInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          b.toInt
        </td>
      </tr><tr>
        <td>
          382
        </td>
        <td>
          7267
        </td>
        <td>
          15199
          -
          15217
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashInt(b.toInt, digest)
        </td>
      </tr><tr>
        <td>
          383
        </td>
        <td>
          7269
        </td>
        <td>
          15241
          -
          15259
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashInt(s.toInt, digest)
        </td>
      </tr><tr>
        <td>
          383
        </td>
        <td>
          7268
        </td>
        <td>
          15249
          -
          15250
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Short.toInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          s.toInt
        </td>
      </tr><tr>
        <td>
          384
        </td>
        <td>
          7270
        </td>
        <td>
          15281
          -
          15299
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashInt(i, digest)
        </td>
      </tr><tr>
        <td>
          385
        </td>
        <td>
          7271
        </td>
        <td>
          15322
          -
          15341
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashLong(l, digest)
        </td>
      </tr><tr>
        <td>
          386
        </td>
        <td>
          7272
        </td>
        <td>
          15366
          -
          15376
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Float.==
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          f.==(-0.0)
        </td>
      </tr><tr>
        <td>
          386
        </td>
        <td>
          7273
        </td>
        <td>
          15381
          -
          15399
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashInt(0, digest)
        </td>
      </tr><tr>
        <td>
          387
        </td>
        <td>
          7275
        </td>
        <td>
          15423
          -
          15473
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashInt(java.lang.Float.floatToIntBits(f), digest)
        </td>
      </tr><tr>
        <td>
          387
        </td>
        <td>
          7274
        </td>
        <td>
          15431
          -
          15464
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Float.floatToIntBits
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          java.lang.Float.floatToIntBits(f)
        </td>
      </tr><tr>
        <td>
          388
        </td>
        <td>
          7277
        </td>
        <td>
          15514
          -
          15534
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashLong(0L, digest)
        </td>
      </tr><tr>
        <td>
          388
        </td>
        <td>
          7276
        </td>
        <td>
          15499
          -
          15509
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Double.==
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          d.==(-0.0)
        </td>
      </tr><tr>
        <td>
          389
        </td>
        <td>
          7278
        </td>
        <td>
          15568
          -
          15604
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Double.doubleToLongBits
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          java.lang.Double.doubleToLongBits(d)
        </td>
      </tr><tr>
        <td>
          389
        </td>
        <td>
          7279
        </td>
        <td>
          15559
          -
          15613
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashLong(java.lang.Double.doubleToLongBits(d), digest)
        </td>
      </tr><tr>
        <td>
          391
        </td>
        <td>
          7280
        </td>
        <td>
          15663
          -
          15707
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.precision
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          dataType.asInstanceOf[org.apache.spark.sql.types.DecimalType].precision
        </td>
      </tr><tr>
        <td>
          392
        </td>
        <td>
          7281
        </td>
        <td>
          15733
          -
          15756
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.Decimal.MAX_LONG_DIGITS
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.Decimal.MAX_LONG_DIGITS
        </td>
      </tr><tr>
        <td>
          392
        </td>
        <td>
          7282
        </td>
        <td>
          15720
          -
          15756
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;=
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          precision.&lt;=(org.apache.spark.sql.types.Decimal.MAX_LONG_DIGITS)
        </td>
      </tr><tr>
        <td>
          393
        </td>
        <td>
          7284
        </td>
        <td>
          15770
          -
          15804
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashLong(d.toUnscaledLong, digest)
        </td>
      </tr><tr>
        <td>
          393
        </td>
        <td>
          7283
        </td>
        <td>
          15779
          -
          15795
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.Decimal.toUnscaledLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          d.toUnscaledLong
        </td>
      </tr><tr>
        <td>
          393
        </td>
        <td>
          7285
        </td>
        <td>
          15770
          -
          15804
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashLong(d.toUnscaledLong, digest)
        </td>
      </tr><tr>
        <td>
          394
        </td>
        <td>
          7290
        </td>
        <td>
          15820
          -
          15977
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          {
  val bytes: Array[Byte] = d.toJavaBigDecimal.unscaledValue().toByteArray();
  InterpretedHashLongsFunction.this.hashBytes(bytes, org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET, bytes.length, digest)
}
        </td>
      </tr><tr>
        <td>
          395
        </td>
        <td>
          7286
        </td>
        <td>
          15844
          -
          15890
        </td>
        <td>
          Apply
        </td>
        <td>
          java.math.BigInteger.toByteArray
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          d.toJavaBigDecimal.unscaledValue().toByteArray()
        </td>
      </tr><tr>
        <td>
          396
        </td>
        <td>
          7287
        </td>
        <td>
          15918
          -
          15944
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET
        </td>
      </tr><tr>
        <td>
          396
        </td>
        <td>
          7289
        </td>
        <td>
          15901
          -
          15967
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hashBytes
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashBytes(bytes, org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET, bytes.length, digest)
        </td>
      </tr><tr>
        <td>
          396
        </td>
        <td>
          7288
        </td>
        <td>
          15946
          -
          15958
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Array.length
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          bytes.length
        </td>
      </tr><tr>
        <td>
          399
        </td>
        <td>
          7291
        </td>
        <td>
          16020
          -
          16075
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.QualitySparkUtils.hashCalendarInterval
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.QualitySparkUtils.hashCalendarInterval(c, this, digest)
        </td>
      </tr><tr>
        <td>
          401
        </td>
        <td>
          7293
        </td>
        <td>
          16154
          -
          16162
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Array.length
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          a.length
        </td>
      </tr><tr>
        <td>
          401
        </td>
        <td>
          7292
        </td>
        <td>
          16126
          -
          16152
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET
        </td>
      </tr><tr>
        <td>
          401
        </td>
        <td>
          7294
        </td>
        <td>
          16113
          -
          16171
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hashBytes
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          InterpretedHashLongsFunction.this.hashBytes(a, org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET, a.length, digest)
        </td>
      </tr><tr>
        <td>
          404
        </td>
        <td>
          7296
        </td>
        <td>
          16236
          -
          16236
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._2
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$4._2
        </td>
      </tr><tr>
        <td>
          404
        </td>
        <td>
          7295
        </td>
        <td>
          16229
          -
          16229
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._1
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$4._1
        </td>
      </tr><tr>
        <td>
          404
        </td>
        <td>
          7297
        </td>
        <td>
          16244
          -
          16244
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._3
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          x$4._3
        </td>
      </tr><tr>
        <td>
          406
        </td>
        <td>
          7298
        </td>
        <td>
          16294
          -
          16336
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hashBytes
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          InterpretedHashLongsFunction.this.hashBytes(bytes, offset, numbytes, digest)
        </td>
      </tr><tr>
        <td>
          412
        </td>
        <td>
          7299
        </td>
        <td>
          16463
          -
          16510
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.ArrayType.elementType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          udt.sqlType.asInstanceOf[org.apache.spark.sql.types.ArrayType].elementType
        </td>
      </tr><tr>
        <td>
          416
        </td>
        <td>
          7300
        </td>
        <td>
          16603
          -
          16604
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          0
        </td>
      </tr><tr>
        <td>
          417
        </td>
        <td>
          7302
        </td>
        <td>
          16620
          -
          16643
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          i.&lt;(array.numElements())
        </td>
      </tr><tr>
        <td>
          417
        </td>
        <td>
          7308
        </td>
        <td>
          16613
          -
          16613
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          417
        </td>
        <td>
          7307
        </td>
        <td>
          16645
          -
          16745
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          {
  {
    result = InterpretedHashLongsFunction.this.hash(array.get(i, elementType), elementType, result);
    i = i.+(1)
  };
  while$2()
}
        </td>
      </tr><tr>
        <td>
          417
        </td>
        <td>
          7301
        </td>
        <td>
          16624
          -
          16643
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.util.ArrayData.numElements
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          array.numElements()
        </td>
      </tr><tr>
        <td>
          417
        </td>
        <td>
          7306
        </td>
        <td>
          16645
          -
          16645
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.while$2
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          while$2()
        </td>
      </tr><tr>
        <td>
          417
        </td>
        <td>
          7309
        </td>
        <td>
          16613
          -
          16613
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          418
        </td>
        <td>
          7304
        </td>
        <td>
          16666
          -
          16718
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hash
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hash(array.get(i, elementType), elementType, result)
        </td>
      </tr><tr>
        <td>
          418
        </td>
        <td>
          7303
        </td>
        <td>
          16671
          -
          16696
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.SpecializedGetters.get
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          array.get(i, elementType)
        </td>
      </tr><tr>
        <td>
          419
        </td>
        <td>
          7305
        </td>
        <td>
          16729
          -
          16735
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          i.+(1)
        </td>
      </tr><tr>
        <td>
          424
        </td>
        <td>
          7311
        </td>
        <td>
          16806
          -
          16806
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$5._2
        </td>
      </tr><tr>
        <td>
          424
        </td>
        <td>
          7310
        </td>
        <td>
          16802
          -
          16802
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$5._1
        </td>
      </tr><tr>
        <td>
          430
        </td>
        <td>
          7312
        </td>
        <td>
          17055
          -
          17069
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.util.MapData.keyArray
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          map.keyArray()
        </td>
      </tr><tr>
        <td>
          431
        </td>
        <td>
          7313
        </td>
        <td>
          17091
          -
          17107
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.util.MapData.valueArray
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          map.valueArray()
        </td>
      </tr><tr>
        <td>
          433
        </td>
        <td>
          7314
        </td>
        <td>
          17152
          -
          17153
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          0
        </td>
      </tr><tr>
        <td>
          434
        </td>
        <td>
          7323
        </td>
        <td>
          17192
          -
          17328
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          {
  {
    result = InterpretedHashLongsFunction.this.hash(keys.get(i, kt), kt, result);
    result = InterpretedHashLongsFunction.this.hash(values.get(i, vt), vt, result);
    i = i.+(1)
  };
  while$3()
}
        </td>
      </tr><tr>
        <td>
          434
        </td>
        <td>
          7316
        </td>
        <td>
          17169
          -
          17190
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          i.&lt;(map.numElements())
        </td>
      </tr><tr>
        <td>
          434
        </td>
        <td>
          7325
        </td>
        <td>
          17162
          -
          17162
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          434
        </td>
        <td>
          7322
        </td>
        <td>
          17192
          -
          17192
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.while$3
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          while$3()
        </td>
      </tr><tr>
        <td>
          434
        </td>
        <td>
          7315
        </td>
        <td>
          17173
          -
          17190
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.util.MapData.numElements
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          map.numElements()
        </td>
      </tr><tr>
        <td>
          434
        </td>
        <td>
          7324
        </td>
        <td>
          17162
          -
          17162
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          435
        </td>
        <td>
          7317
        </td>
        <td>
          17218
          -
          17233
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.SpecializedGetters.get
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          keys.get(i, kt)
        </td>
      </tr><tr>
        <td>
          435
        </td>
        <td>
          7318
        </td>
        <td>
          17213
          -
          17246
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hash
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hash(keys.get(i, kt), kt, result)
        </td>
      </tr><tr>
        <td>
          436
        </td>
        <td>
          7320
        </td>
        <td>
          17266
          -
          17301
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hash
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hash(values.get(i, vt), vt, result)
        </td>
      </tr><tr>
        <td>
          436
        </td>
        <td>
          7319
        </td>
        <td>
          17271
          -
          17288
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.SpecializedGetters.get
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          values.get(i, vt)
        </td>
      </tr><tr>
        <td>
          437
        </td>
        <td>
          7321
        </td>
        <td>
          17312
          -
          17318
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          i.+(1)
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          7326
        </td>
        <td>
          17528
          -
          17538
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$6.dataType
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          7328
        </td>
        <td>
          17487
          -
          17547
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableOnce.toArray
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          udt.sqlType.asInstanceOf[org.apache.spark.sql.types.StructType].map[org.apache.spark.sql.types.DataType, Seq[org.apache.spark.sql.types.DataType]](((x$6: org.apache.spark.sql.types.StructField) =&gt; x$6.dataType))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.types.DataType]).toArray[org.apache.spark.sql.types.DataType]((ClassTag.apply[org.apache.spark.sql.types.DataType](classOf[org.apache.spark.sql.types.DataType]): scala.reflect.ClassTag[org.apache.spark.sql.types.DataType]))
        </td>
      </tr><tr>
        <td>
          444
        </td>
        <td>
          7327
        </td>
        <td>
          17527
          -
          17527
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.types.DataType]
        </td>
      </tr><tr>
        <td>
          445
        </td>
        <td>
          7329
        </td>
        <td>
          17596
          -
          17606
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$7.dataType
        </td>
      </tr><tr>
        <td>
          445
        </td>
        <td>
          7331
        </td>
        <td>
          17585
          -
          17607
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](fields).map[org.apache.spark.sql.types.DataType, Array[org.apache.spark.sql.types.DataType]](((x$7: org.apache.spark.sql.types.StructField) =&gt; x$7.dataType))(scala.this.Array.canBuildFrom[org.apache.spark.sql.types.DataType]((ClassTag.apply[org.apache.spark.sql.types.DataType](classOf[org.apache.spark.sql.types.DataType]): scala.reflect.ClassTag[org.apache.spark.sql.types.DataType])))
        </td>
      </tr><tr>
        <td>
          445
        </td>
        <td>
          7330
        </td>
        <td>
          17595
          -
          17595
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.this.Array.canBuildFrom[org.apache.spark.sql.types.DataType]((ClassTag.apply[org.apache.spark.sql.types.DataType](classOf[org.apache.spark.sql.types.DataType]): scala.reflect.ClassTag[org.apache.spark.sql.types.DataType]))
        </td>
      </tr><tr>
        <td>
          448
        </td>
        <td>
          7332
        </td>
        <td>
          17662
          -
          17663
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          0
        </td>
      </tr><tr>
        <td>
          449
        </td>
        <td>
          7333
        </td>
        <td>
          17682
          -
          17698
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.InternalRow.numFields
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          struct.numFields
        </td>
      </tr><tr>
        <td>
          450
        </td>
        <td>
          7341
        </td>
        <td>
          17723
          -
          17818
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          {
  {
    result = InterpretedHashLongsFunction.this.hash(struct.get(i, types.apply(i)), types.apply(i), result);
    i = i.+(1)
  };
  while$4()
}
        </td>
      </tr><tr>
        <td>
          450
        </td>
        <td>
          7343
        </td>
        <td>
          17707
          -
          17707
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          450
        </td>
        <td>
          7334
        </td>
        <td>
          17714
          -
          17721
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          i.&lt;(len)
        </td>
      </tr><tr>
        <td>
          450
        </td>
        <td>
          7340
        </td>
        <td>
          17723
          -
          17723
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.while$4
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          while$4()
        </td>
      </tr><tr>
        <td>
          450
        </td>
        <td>
          7342
        </td>
        <td>
          17707
          -
          17707
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          451
        </td>
        <td>
          7338
        </td>
        <td>
          17744
          -
          17791
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.qualityFunctions.InterpretedHashLongsFunction.hash
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hash(struct.get(i, types.apply(i)), types.apply(i), result)
        </td>
      </tr><tr>
        <td>
          451
        </td>
        <td>
          7335
        </td>
        <td>
          17763
          -
          17771
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Array.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          types.apply(i)
        </td>
      </tr><tr>
        <td>
          451
        </td>
        <td>
          7337
        </td>
        <td>
          17774
          -
          17782
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Array.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          types.apply(i)
        </td>
      </tr><tr>
        <td>
          451
        </td>
        <td>
          7336
        </td>
        <td>
          17749
          -
          17772
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.SpecializedGetters.get
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          struct.get(i, types.apply(i))
        </td>
      </tr><tr>
        <td>
          452
        </td>
        <td>
          7339
        </td>
        <td>
          17802
          -
          17808
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          i.+(1)
        </td>
      </tr>
    </table>
          </div>
        </div>
      </body>
    </html>